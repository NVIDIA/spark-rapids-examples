{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac9fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "from operator import itemgetter\n",
    "from time import time\n",
    "\n",
    "from pyspark import broadcast\n",
    "from pyspark.sql import Row, SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3879c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = 0\n",
    "INT_COLS = list(range(1, 14))\n",
    "CAT_COLS = list(range(14, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6799eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_counts_with_frequency_limit(df, frequency_limit = None):\n",
    "    cols = ['_c%d' % i for i in CAT_COLS]\n",
    "    df = (df\n",
    "        .select(posexplode(array(*cols)))\n",
    "        .withColumnRenamed('pos', 'column_id')\n",
    "        .withColumnRenamed('col', 'data')\n",
    "        .filter('data is not null')\n",
    "        .groupBy('column_id', 'data')\n",
    "        .count())\n",
    "\n",
    "    if frequency_limit:\n",
    "        frequency_limit = frequency_limit.split(\",\")\n",
    "        exclude = []\n",
    "        default_limit = None\n",
    "        for fl in frequency_limit:\n",
    "            frequency_pair = fl.split(\":\")\n",
    "            if len(frequency_pair) == 1:\n",
    "                default_limit = int(frequency_pair[0])\n",
    "            elif len(frequency_pair) == 2:\n",
    "                df = df.filter((col('column_id') != int(frequency_pair[0]) - CAT_COLS[0]) | (col('count') >= int(frequency_pair[1])))\n",
    "                exclude.append(int(frequency_pair[0]))\n",
    "        if default_limit:\n",
    "            remain = [x - CAT_COLS[0] for x in CAT_COLS if x not in exclude]\n",
    "            df = df.filter((~col('column_id').isin(remain)) | (col('count') >= default_limit))\n",
    "            # for comparing isin and separate filter\n",
    "            # for i in remain:\n",
    "            #     df = df.filter((col('column_id') != i - CAT_COLS[0]) | (col('count') >= default_limit))\n",
    "    return df\n",
    "\n",
    "\n",
    "def assign_id_with_window(df):\n",
    "    windowed = Window.partitionBy('column_id').orderBy(desc('count'))\n",
    "    return (df\n",
    "            .withColumn('id', row_number().over(windowed))\n",
    "            .withColumnRenamed('count', 'model_count'))\n",
    "\n",
    "\n",
    "def assign_low_mem_partial_ids(df):\n",
    "    # To avoid some scaling issues with a simple window operation, we use a more complex method\n",
    "    # to compute the same thing, but in a more distributed spark specific way\n",
    "    df = df.orderBy(asc('column_id'), desc('count'))\n",
    "    # The monotonically_increasing_id is the partition id in the top 31 bits and the rest\n",
    "    # is an increasing count of the rows within that partition.  So we split it into two parts,\n",
    "    # the partion id part_id and the count mono_id\n",
    "    df = df.withColumn('part_id', spark_partition_id())\n",
    "    return df.withColumn('mono_id', monotonically_increasing_id() - shiftLeft(col('part_id'), 33))\n",
    "\n",
    "\n",
    "def assign_low_mem_final_ids(df):\n",
    "    # Now we can find the minimum and maximum mono_ids within a given column/partition pair\n",
    "    sub_model = df.groupBy('column_id', 'part_id').agg(max('mono_id').alias('top'), min('mono_id').alias('bottom'))\n",
    "    sub_model = sub_model.withColumn('diff', col('top') - col('bottom') + 1)\n",
    "    sub_model = sub_model.drop('top')\n",
    "    # This window function is over aggregated column/partition pair table. It will do a running sum of the rows\n",
    "    # within that column\n",
    "    windowed = Window.partitionBy('column_id').orderBy('part_id').rowsBetween(Window.unboundedPreceding, -1)\n",
    "    sub_model = sub_model.withColumn('running_sum', sum('diff').over(windowed)).na.fill(0, [\"running_sum\"])\n",
    "\n",
    "    joined = df.withColumnRenamed('column_id', 'i_column_id')\n",
    "    joined = joined.withColumnRenamed('part_id', 'i_part_id')\n",
    "    joined = joined.withColumnRenamed('count', 'model_count')\n",
    "\n",
    "    # Then we can join the original input with the pair it is a part of\n",
    "    joined = joined.join(sub_model, (col('i_column_id') == col('column_id')) & (col('part_id') == col('i_part_id')))\n",
    "\n",
    "    # So with all that we can subtract bottom from mono_id makeing it start at 0 for each partition\n",
    "    # and then add in the running_sum so the id is contiguous and unique for the entire column. + 1 to make it match the 1 based indexing\n",
    "    # for row_number\n",
    "    ret = joined.select(col('column_id'),\n",
    "                        col('data'),\n",
    "                        (col('mono_id') - col('bottom') + col('running_sum') + 1).cast(IntegerType()).alias('id'),\n",
    "                        col('model_count'))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_column_models(combined_model):\n",
    "    for i in CAT_COLS:\n",
    "        model = (combined_model\n",
    "            .filter('column_id == %d' % (i - CAT_COLS[0]))\n",
    "            .drop('column_id'))\n",
    "        yield i, model\n",
    "\n",
    "\n",
    "def col_of_rand_long():\n",
    "    return (rand() * (1 << 52)).cast(LongType())\n",
    "\n",
    "def skewed_join(df, model, col_name, cutoff):\n",
    "    # Most versions of spark don't have a good way\n",
    "    # to deal with a skewed join out of the box.\n",
    "    # Some do and if you want to replace this with\n",
    "    # one of those that would be great.\n",
    "    \n",
    "    # Because we have statistics about the skewedness\n",
    "    # that we can used we divide the model up into two parts\n",
    "    # one part is the highly skewed part and we do a\n",
    "    # broadcast join for that part, but keep the result in\n",
    "    # a separate column\n",
    "    b_model = broadcast(model.filter(col('model_count') >= cutoff)\n",
    "            .withColumnRenamed('data', col_name)\n",
    "            .drop('model_count'))\n",
    "    \n",
    "    df = (df\n",
    "            .join(b_model, col_name, how='left')\n",
    "            .withColumnRenamed('id', 'id_tmp'))\n",
    "    \n",
    "    # We also need to spread the skewed data that matched\n",
    "    # evenly.  We will use a source of randomness for this\n",
    "    # but use a -1 for anything that still needs to be matched\n",
    "    if 'ordinal' in df.columns:\n",
    "        rand_column = col('ordinal')\n",
    "    else:\n",
    "        rand_column = col_of_rand_long()\n",
    "\n",
    "    df = df.withColumn('join_rand',\n",
    "            # null values are not in the model, they are filtered out\n",
    "            # but can be a source of skewedness so include them in\n",
    "            # the even distribution\n",
    "            when(col('id_tmp').isNotNull() | col(col_name).isNull(), rand_column)\n",
    "            .otherwise(lit(-1)))\n",
    "    \n",
    "    # Null out the string data that already matched to save memory\n",
    "    df = df.withColumn(col_name,\n",
    "            when(col('id_tmp').isNotNull(), None)\n",
    "            .otherwise(col(col_name)))\n",
    "    \n",
    "    # Now do the second join, which will be a non broadcast join.\n",
    "    # Sadly spark is too smart for its own good and will optimize out\n",
    "    # joining on a column it knows will always be a constant value.\n",
    "    # So we have to make a convoluted version of assigning a -1 to the\n",
    "    # randomness column for the model itself to work around that.\n",
    "    nb_model = (model\n",
    "            .withColumn('join_rand', when(col('model_count') < cutoff, lit(-1)).otherwise(lit(-2)))\n",
    "            .filter(col('model_count') < cutoff)\n",
    "            .withColumnRenamed('data', col_name)\n",
    "            .drop('model_count'))\n",
    "    \n",
    "    df = (df\n",
    "            .join(nb_model, ['join_rand', col_name], how='left')\n",
    "            .drop(col_name, 'join_rand')\n",
    "            # Pick either join result as an answer\n",
    "            .withColumn(col_name, coalesce(col('id'), col('id_tmp')))\n",
    "            .drop('id', 'id_tmp'))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_models(df, models, broadcast_model = False, skew_broadcast_pct = 1.0):\n",
    "    # sort the models so broadcast joins come first. This is\n",
    "    # so we reduce the amount of shuffle data sooner than later\n",
    "    # If we parsed the string hex values to ints early on this would\n",
    "    # not make a difference.\n",
    "    models = sorted(models, key=itemgetter(3), reverse=True)\n",
    "    for i, model, original_rows, would_broadcast in models:\n",
    "        col_name = '_c%d' % i\n",
    "        if not (would_broadcast or broadcast_model):\n",
    "            # The data is highly skewed so we need to offset that\n",
    "            cutoff = int(original_rows * skew_broadcast_pct/100.0)\n",
    "            df = skewed_join(df, model, col_name, cutoff)\n",
    "        else:\n",
    "            # broadcast joins can handle skewed data so no need to\n",
    "            # do anything special\n",
    "            model = (model.drop('model_count')\n",
    "                          .withColumnRenamed('data', col_name))\n",
    "            model = broadcast(model) if broadcast_model else model\n",
    "            df = (df\n",
    "                .join(model, col_name, how='left')\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed('id', col_name))\n",
    "    return df.fillna(0, ['_c%d' % i for i in CAT_COLS])\n",
    "\n",
    "\n",
    "def transform_log(df, transform_log = False):\n",
    "    cols = ['_c%d' % i for i in INT_COLS]\n",
    "    if transform_log:\n",
    "        for col_name in cols:\n",
    "            df = df.withColumn(col_name, log(df[col_name] + 3))\n",
    "    return df.fillna(0, cols)\n",
    "\n",
    "\n",
    "def would_broadcast(spark, str_path):\n",
    "    sc = spark.sparkContext\n",
    "    config = sc._jsc.hadoopConfiguration()\n",
    "    path = sc._jvm.org.apache.hadoop.fs.Path(str_path)\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(config)\n",
    "    stat = fs.listFiles(path, True)\n",
    "    sum = 0\n",
    "    while stat.hasNext():\n",
    "       sum = sum + stat.next().getLen()\n",
    "    sql_conf = sc._jvm.org.apache.spark.sql.internal.SQLConf()\n",
    "    cutoff = sql_conf.autoBroadcastJoinThreshold() * sql_conf.fileCompressionFactor()\n",
    "    return sum <= cutoff\n",
    "\n",
    "def delete_data_source(spark, path):\n",
    "    sc = spark.sparkContext\n",
    "    config = sc._jsc.hadoopConfiguration()\n",
    "    path = sc._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    sc._jvm.org.apache.hadoop.fs.FileSystem.get(config).delete(path, True)\n",
    "\n",
    "\n",
    "def load_raw(spark, folder, day_range):\n",
    "    label_fields = [StructField('_c%d' % LABEL_COL, IntegerType())]\n",
    "    int_fields = [StructField('_c%d' % i, IntegerType()) for i in INT_COLS]\n",
    "    str_fields = [StructField('_c%d' % i, StringType()) for i in CAT_COLS]\n",
    "\n",
    "    schema = StructType(label_fields + int_fields + str_fields)\n",
    "    paths = [os.path.join(folder, 'day_%d' % i) for i in day_range]\n",
    "    return (spark\n",
    "        .read\n",
    "        .schema(schema)\n",
    "        .option('sep', '\\t')\n",
    "        .csv(paths))\n",
    "\n",
    "def rand_ordinal(df):\n",
    "    # create a random long from the double precision float.  \n",
    "    # The fraction part of a double is 52 bits, so we try to capture as much\n",
    "    # of that as possible\n",
    "    return df.withColumn('ordinal', col_of_rand_long())\n",
    "\n",
    "def day_from_ordinal(df, num_days):\n",
    "    return df.withColumn('day', (col('ordinal') % num_days).cast(IntegerType()))\n",
    "\n",
    "def day_from_input_file(df):\n",
    "    return df.withColumn('day', substring_index(input_file_name(), '_', -1).cast(IntegerType()))\n",
    "\n",
    "def psudo_sort_by_day_plus(spark, df, num_days):\n",
    "    # Sort is very expensive because it needs to calculate the partitions\n",
    "    # which in our case may involve rereading all of the data.  In some cases\n",
    "    # we can avoid this by repartitioning the data and sorting within a single partition\n",
    "    shuffle_parts = int(spark.conf.get('spark.sql.shuffle.partitions'))\n",
    "    extra_parts = int(shuffle_parts/num_days)\n",
    "    if extra_parts <= 0:\n",
    "        df = df.repartition('day')\n",
    "    else:\n",
    "        #We want to spread out the computation to about the same amount as shuffle_parts\n",
    "        divided = (col('ordinal') / num_days).cast(LongType())\n",
    "        extra_ident = divided % extra_parts\n",
    "        df = df.repartition(col('day'), extra_ident)\n",
    "    return df.sortWithinPartitions('day', 'ordinal')\n",
    "\n",
    "\n",
    "def load_combined_model(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'combined.parquet')\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "\n",
    "def save_combined_model(df, model_folder, mode=None):\n",
    "    path = os.path.join(model_folder, 'combined.parquet')\n",
    "    df.write.parquet(path, mode=mode)\n",
    "\n",
    "\n",
    "def delete_combined_model(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'combined.parquet')\n",
    "    delete_data_source(spark, path)\n",
    "\n",
    "\n",
    "def load_low_mem_partial_ids(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'partial_ids.parquet')\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "\n",
    "def save_low_mem_partial_ids(df, model_folder, mode=None):\n",
    "    path = os.path.join(model_folder, 'partial_ids.parquet')\n",
    "    df.write.parquet(path, mode=mode)\n",
    "\n",
    "\n",
    "def delete_low_mem_partial_ids(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'partial_ids.parquet')\n",
    "    delete_data_source(spark, path)\n",
    "\n",
    "\n",
    "def load_column_models(spark, model_folder, count_required):\n",
    "    for i in CAT_COLS:\n",
    "        path = os.path.join(model_folder, '%d.parquet' % i)\n",
    "        df = spark.read.parquet(path)\n",
    "        if count_required:\n",
    "            values = df.agg(sum('model_count').alias('sum'), count('*').alias('size')).collect()\n",
    "        else:\n",
    "            values = df.agg(sum('model_count').alias('sum')).collect()\n",
    "        yield i, df, values[0], would_broadcast(spark, path)\n",
    "\n",
    "def save_column_models(column_models, model_folder, mode=None):\n",
    "    for i, model in column_models:\n",
    "        path = os.path.join(model_folder, '%d.parquet' % i)\n",
    "        model.write.parquet(path, mode=mode)\n",
    "\n",
    "\n",
    "def save_model_size(model_size, path, write_mode):\n",
    "    if os.path.exists(path) and write_mode == 'errorifexists':\n",
    "        print('Error: model size file %s exists' % path)\n",
    "        sys.exit(1)\n",
    "\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
    "    with open(path, 'w') as fp:\n",
    "        json.dump(model_size, fp, indent=4)\n",
    "\n",
    "\n",
    "_benchmark = {}\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def _timed(step):\n",
    "    start = time()\n",
    "    yield\n",
    "    end = time()\n",
    "    _benchmark[step] = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b555d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args(input_args):\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--mode',\n",
    "        required=True,\n",
    "        choices=['generate_models', 'transform'])\n",
    "\n",
    "    parser.add_argument('--days', required=True)\n",
    "    parser.add_argument('--input_folder', required=True)\n",
    "    parser.add_argument('--output_folder')\n",
    "    parser.add_argument('--model_size_file')\n",
    "    parser.add_argument('--model_folder', required=True)\n",
    "    parser.add_argument(\n",
    "        '--write_mode',\n",
    "        choices=['overwrite', 'errorifexists'],\n",
    "        default='errorifexists')\n",
    "\n",
    "    parser.add_argument('--frequency_limit')\n",
    "    parser.add_argument('--no_numeric_log_col', action='store_true')\n",
    "    #Support for running in a lower memory environment\n",
    "    parser.add_argument('--low_mem', action='store_true')\n",
    "    parser.add_argument(\n",
    "        '--output_ordering',\n",
    "        choices=['total_random', 'day_random', 'any', 'input'],\n",
    "        default='total_random')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--output_partitioning',\n",
    "        choices=['day', 'none'],\n",
    "        default='none')\n",
    "\n",
    "    parser.add_argument('--dict_build_shuffle_parallel_per_day', type=int, default=2)\n",
    "    parser.add_argument('--apply_shuffle_parallel_per_day', type=int, default=25)\n",
    "    parser.add_argument('--skew_broadcast_pct', type=float, default=1.0)\n",
    "\n",
    "    parser.add_argument('--debug_mode', action='store_true')\n",
    "    \n",
    "    # Parameters are defined here\n",
    "    args = parser.parse_args(args=input_args)\n",
    "\n",
    "    start, end = args.days.split('-')\n",
    "    args.day_range = list(range(int(start), int(end) + 1))\n",
    "    args.days = len(args.day_range)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f55fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main(input_args):\n",
    "    args = _parse_args(input_args)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    df = load_raw(spark, args.input_folder, args.day_range)\n",
    "\n",
    "    if args.mode == 'generate_models':\n",
    "        spark.conf.set('spark.sql.shuffle.partitions', args.days * args.dict_build_shuffle_parallel_per_day)\n",
    "        with _timed('generate models'):\n",
    "            col_counts = get_column_counts_with_frequency_limit(df, args.frequency_limit)\n",
    "            if args.low_mem:\n",
    "                # in low memory mode we have to save an intermediate result\n",
    "                # because if we try to do it in one query spark ends up assigning the\n",
    "                # partial ids in two different locations that are not guaranteed to line up\n",
    "                # this prevents that from happening by assigning the partial ids\n",
    "                # and then writeing them out.\n",
    "                save_low_mem_partial_ids(\n",
    "                        assign_low_mem_partial_ids(col_counts),\n",
    "                        args.model_folder,\n",
    "                        args.write_mode)\n",
    "                save_combined_model(\n",
    "                        assign_low_mem_final_ids(load_low_mem_partial_ids(spark, args.model_folder)),\n",
    "                        args.model_folder,\n",
    "                        args.write_mode)\n",
    "                if not args.debug_mode:\n",
    "                    delete_low_mem_partial_ids(spark, args.model_folder)\n",
    "\n",
    "            else:\n",
    "                save_combined_model(\n",
    "                        assign_id_with_window(col_counts),\n",
    "                        args.model_folder,\n",
    "                        args.write_mode)\n",
    "            save_column_models(\n",
    "                get_column_models(load_combined_model(spark, args.model_folder)),\n",
    "                args.model_folder,\n",
    "                args.write_mode)\n",
    "            if not args.debug_mode:\n",
    "                delete_combined_model(spark, args.model_folder)\n",
    "\n",
    "    if args.mode == 'transform':\n",
    "        spark.conf.set('spark.sql.shuffle.partitions', args.days * args.apply_shuffle_parallel_per_day)\n",
    "        with _timed('transform'):\n",
    "            if args.output_ordering == 'total_random':\n",
    "                df = rand_ordinal(df)\n",
    "                if args.output_partitioning == 'day':\n",
    "                    df = day_from_ordinal(df, args.days)\n",
    "            elif args.output_ordering == 'day_random':\n",
    "                df = rand_ordinal(df)\n",
    "                df = day_from_input_file(df)\n",
    "            elif args.output_ordering == 'input':\n",
    "                df = df.withColumn('ordinal', monotonically_increasing_id())\n",
    "                if args.output_partitioning == 'day':\n",
    "                    df = day_from_input_file(df)\n",
    "            else: # any ordering\n",
    "                if args.output_partitioning == 'day':\n",
    "                    df = day_from_input_file(df)\n",
    "\n",
    "            models = list(load_column_models(spark, args.model_folder, bool(args.model_size_file)))\n",
    "            if args.model_size_file:\n",
    "                save_model_size(\n",
    "                    OrderedDict(('_c%d' % i, agg.size) for i, _, agg, _ in models),\n",
    "                    args.model_size_file,\n",
    "                    args.write_mode)\n",
    "            models = [(i, df, agg.sum, flag) for i, df, agg, flag in models]\n",
    "\n",
    "            df = apply_models(\n",
    "                df,\n",
    "                models,\n",
    "                not args.low_mem,\n",
    "                args.skew_broadcast_pct)\n",
    "            df = transform_log(df, not args.no_numeric_log_col)\n",
    "\n",
    "\n",
    "            if args.output_partitioning == 'day':\n",
    "                partitionBy = 'day'\n",
    "            else:\n",
    "                partitionBy = None\n",
    "\n",
    "            if args.output_ordering == 'total_random':\n",
    "                if args.output_partitioning == 'day':\n",
    "                    df = psudo_sort_by_day_plus(spark, df, args.days)\n",
    "                else: # none\n",
    "                    # Don't do a full sort it is expensive. Order is random so\n",
    "                    # just make it random\n",
    "                    df = df.repartition('ordinal').sortWithinPartitions('ordinal')\n",
    "\n",
    "                df = df.drop('ordinal')\n",
    "            elif args.output_ordering == 'day_random':\n",
    "                df = psudo_sort_by_day_plus(spark, df, args.days)\n",
    "                df = df.drop('ordinal')\n",
    "                if args.output_partitioning != 'day':\n",
    "                    df = df.drop('day')\n",
    "            elif args.output_ordering == 'input':\n",
    "                if args.low_mem:\n",
    "                    # This is the slowest option. We totally messed up the order so we have to put\n",
    "                    # it back in the correct order\n",
    "                    df = df.orderBy('ordinal')\n",
    "                else:\n",
    "                    # Applying the dictionary happened within a single task so we are already really\n",
    "                    # close to the correct order, just need to sort within the partition\n",
    "                    df = df.sortWithinPartitions('ordinal')\n",
    "                df = df.drop('ordinal')\n",
    "                if args.output_partitioning != 'day':\n",
    "                    df = df.drop('day')\n",
    "            # else: any ordering so do nothing the ordering does not matter\n",
    "\n",
    "            df.write.parquet(\n",
    "                args.output_folder,\n",
    "                mode=args.write_mode,\n",
    "                partitionBy=partitionBy)\n",
    "\n",
    "    print('=' * 100)\n",
    "    print(_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6df5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 12:48:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "21/12/22 12:50:23 WARN GpuOverrides:                                            \n",
      "              !Exec <WindowExec> cannot run on GPU because not all expressions can be replaced\n",
      "                @Expression <Alias> sum(diff#143L) windowspecdefinition(column_id#120, part_id#123 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), -1)) AS running_sum#154L could run on GPU\n",
      "                  !Expression <WindowExpression> sum(diff#143L) windowspecdefinition(column_id#120, part_id#123 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), -1)) cannot run on GPU because upper-bounds behind the current row is not supported. Found -1\n",
      "                    @Expression <AggregateExpression> sum(diff#143L) could run on GPU\n",
      "                      @Expression <Sum> sum(diff#143L) could run on GPU\n",
      "                        @Expression <AttributeReference> diff#143L could run on GPU\n",
      "                    @Expression <WindowSpecDefinition> windowspecdefinition(column_id#120, part_id#123 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), -1)) could run on GPU\n",
      "                      @Expression <AttributeReference> column_id#120 could run on GPU\n",
      "                      @Expression <SortOrder> part_id#123 ASC NULLS FIRST could run on GPU\n",
      "                        @Expression <AttributeReference> part_id#123 could run on GPU\n",
      "                      @Expression <SpecifiedWindowFrame> specifiedwindowframe(RowFrame, unboundedpreceding$(), -1) could run on GPU\n",
      "                        @Expression <UnboundedPreceding$> unboundedpreceding$() could run on GPU\n",
      "                        @Expression <Literal> -1 could run on GPU\n",
      "                @Expression <AttributeReference> column_id#120 could run on GPU\n",
      "                @Expression <SortOrder> part_id#123 ASC NULLS FIRST could run on GPU\n",
      "                  @Expression <AttributeReference> part_id#123 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "{'generate models': 94.8656804561615}\n"
     ]
    }
   ],
   "source": [
    "_main(['--mode', 'generate_models',\n",
    "        '--frequency_limit', '15',\n",
    "        '--input_folder', '/data',\n",
    "        '--days', '4-4',\n",
    "        '--model_folder', '/models',\n",
    "        '--write_mode', 'overwrite',\n",
    "        '--low_mem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8419030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 12:50:34 WARN GpuOverrides: \n",
      "                                                                                                                !Exec <ProjectExec> cannot run on GPU because not all expressions can be replaced\n",
      "                                                                                                                  @Expression <AttributeReference> _c0#390 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c1#391 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c2#392 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c3#393 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c4#394 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c5#395 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c6#396 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c7#397 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c8#398 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c9#399 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c10#400 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c11#401 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c12#402 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c13#403 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c14#404 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c15#405 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c16#406 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c17#407 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c18#408 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c19#409 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c20#410 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c21#411 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c22#412 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c23#413 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c24#414 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c25#415 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c26#416 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c27#417 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c28#418 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c29#419 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c30#420 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c31#421 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c32#422 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c33#423 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c34#424 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c35#425 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c36#426 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c37#427 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c38#428 could run on GPU\n",
      "                                                                                                                  @Expression <AttributeReference> _c39#429 could run on GPU\n",
      "                                                                                                                  @Expression <Alias> cast((rand(3128137626833025233) * 4.503599627370496E15) as bigint) AS ordinal#470L could run on GPU\n",
      "                                                                                                                    !Expression <Cast> cast((rand(3128137626833025233) * 4.503599627370496E15) as bigint) cannot run on GPU because Casting from floating point types to integral types on the GPU supports a slightly different range of values when using Spark 3.1.0 or later. Refer to the CAST documentation for more details.. To enable this operation on the GPU, set spark.rapids.sql.castFloatToIntegralTypes.enabled to true.\n",
      "                                                                                                                      @Expression <Multiply> (rand(3128137626833025233) * 4.503599627370496E15) could run on GPU\n",
      "                                                                                                                        @Expression <Rand> rand(3128137626833025233) could run on GPU\n",
      "                                                                                                                          @Expression <Literal> 3128137626833025233 could run on GPU\n",
      "                                                                                                                        @Expression <Literal> 4.503599627370496E15 could run on GPU\n",
      "                                                                                                                  !Exec <FileSourceScanExec> cannot run on GPU because CSV reading is not 100% compatible when reading integers. To enable it please set spark.rapids.sql.csv.read.integer.enabled to true.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "{'generate models': 94.8656804561615, 'transform': 276.87314462661743}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_main(['--mode', 'transform',\n",
    "        '--input_folder', '/data',\n",
    "        '--days', '4-4',\n",
    "       '--output_folder', '/data/train',\n",
    "       '--model_size_file', '/data/model_size.json',\n",
    "        '--model_folder', '/models',\n",
    "        '--write_mode', 'overwrite',\n",
    "        '--low_mem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5353bcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     _c14   _c15   _c16  _c17   _c18  _c19  _c20  _c21  _c22    _c23  ...  \\\n",
      "0  327597  21882  14396  7063  19090     3  6484  1261    49  296349  ...   \n",
      "\n",
      "   _c30  _c31  _c32    _c33    _c34    _c35   _c36  _c37  _c38  _c39  \n",
      "0     3   938    14  327656  217544  319293  85130  9545    72    33  \n",
      "\n",
      "[1 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open('/data/model_size.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    rename_df = pd.DataFrame(data, index=[0])\n",
    "    print(rename_df)\n",
    "    rename_df.to_csv(\"/data/dimensions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668647d",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675da0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import pprint\n",
    "import sys\n",
    "# This needs to happen first to avoid pyarrow serialization errors.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Make sure pyarrow is referenced before anything else to avoid segfault due to conflict\n",
    "# with TensorFlow libraries.  Use `pa` package reference to ensure it's loaded before\n",
    "# functions like `deserialize_model` which are implemented at the top level.\n",
    "# See https://jira.apache.org/jira/browse/ARROW-3346\n",
    "import pyarrow as pa\n",
    "\n",
    "import horovod\n",
    "import horovod.tensorflow.keras as hvd\n",
    "import tensorflow as tf\n",
    "from horovod.spark.common.backend import SparkBackend\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Embedding, Concatenate, Dense, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031f6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "PETASTORM_DATALOADER = 'petastorm'\n",
    "NVTABULAR_DATALOADER = 'nvtabular'\n",
    "\n",
    "CONTINUOUS_COLUMNS = [f'_c{i}' for i in range(1,14)]\n",
    "CATEGORICAL_COLUMNS = [f'_c{c}' for c in range(14,40)]\n",
    "ALL_COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "LABEL_COLUMNS = ['_c0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eea799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_dimensions(spark, data_dir):\n",
    "    df = spark.read.csv(f'{data_dir}/dimensions.csv', header=True).toPandas()\n",
    "    dimensions = df.to_dict('records')[0]\n",
    "    pprint.pprint(dimensions)\n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28c2de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dimensions, args):\n",
    "    \n",
    "    inputs = {\n",
    "        **{i: Input(shape=(1,), name=i, dtype=tf.float32) for i in CONTINUOUS_COLUMNS},\n",
    "        **{c: Input(shape=(1,), name=c, dtype=tf.int32) for c in CATEGORICAL_COLUMNS}\n",
    "    }\n",
    "\n",
    "    one_hots = []\n",
    "    embeddings = []\n",
    "    for c in CATEGORICAL_COLUMNS:\n",
    "        dimension = int(dimensions[c]) + 1\n",
    "        # dimension <= 128, smaller size for demo\n",
    "        if dimension <= 4:\n",
    "            one_hots.append(CategoryEncoding(num_tokens=dimension, name=f'one_hot_{c}')(inputs[c]))\n",
    "        else:\n",
    "            # embedding_size = int(math.floor(0.6 * dimension ** 0.25)), smaller model size for demo\n",
    "            embedding_size = 2\n",
    "            embeddings.append(Embedding(input_dim=dimension,\n",
    "                                        output_dim=embedding_size,\n",
    "                                        input_length=1,\n",
    "                                        name=f'embedding_{c}')(inputs[c]))\n",
    "\n",
    "    x = Concatenate(name='embeddings_concat')(embeddings)\n",
    "    x = Flatten(name='embeddings_flatten')(x)\n",
    "    x = Concatenate(name='inputs_concat')([x] + one_hots + [inputs[i] for i in CONTINUOUS_COLUMNS])\n",
    "    x = BatchNormalization()(x)\n",
    "#     x = Flatten(input_shape=(39,)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    model = tf.keras.Model(inputs=[inputs[c] for c in ALL_COLUMNS], outputs=output)\n",
    "    if hvd.rank() == 0:\n",
    "        model.summary()\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)\n",
    "    opt = hvd.DistributedOptimizer(opt)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf82e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(dimensions, train_rows, val_rows, args):\n",
    "    # Make sure pyarrow is referenced before anything else to avoid segfault due to conflict\n",
    "    # with TensorFlow libraries.  Use `pa` package reference to ensure it's loaded before\n",
    "    # functions like `deserialize_model` which are implemented at the top level.\n",
    "    # See https://jira.apache.org/jira/browse/ARROW-3346\n",
    "    pa\n",
    "\n",
    "    import atexit\n",
    "    import horovod.tensorflow.keras as hvd\n",
    "    from horovod.spark.task import get_available_devices\n",
    "    import os\n",
    "    import tempfile\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras.backend as K\n",
    "    import shutil\n",
    "\n",
    "    gpus = get_available_devices()\n",
    "    if gpus:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = gpus[0]\n",
    "    if args.dataloader == NVTABULAR_DATALOADER:\n",
    "        os.environ['TF_MEMORY_ALLOCATION'] = '0.55'\n",
    "        from nvtabular.loader.tensorflow import KerasSequenceLoader\n",
    "\n",
    "    # Horovod: initialize Horovod inside the trainer.\n",
    "    hvd.init()\n",
    "\n",
    "    # Horovod: restore from checkpoint, use hvd.load_model under the hood.\n",
    "    model = build_model(dimensions, args)\n",
    "\n",
    "    # Horovod: adjust learning rate based on number of processes.\n",
    "    scaled_lr = K.get_value(model.optimizer.lr) * hvd.size()\n",
    "    K.set_value(model.optimizer.lr, scaled_lr)\n",
    "\n",
    "    # Horovod: print summary logs on the first worker.\n",
    "    verbose = 1 if hvd.rank() == 0 else 0\n",
    "\n",
    "    callbacks = [\n",
    "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "        # This is necessary to ensure consistent initialization of all workers when\n",
    "        # training is started with random weights or restored from a checkpoint.\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(root_rank=0),\n",
    "\n",
    "        # Horovod: average metrics among workers at the end of every epoch.\n",
    "        #\n",
    "        # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "        # TensorBoard, or other metrics-based callbacks.\n",
    "        hvd.callbacks.MetricAverageCallback(),\n",
    "\n",
    "        # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "        # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "        # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "        hvd.callbacks.LearningRateWarmupCallback(initial_lr=scaled_lr, warmup_epochs=5, verbose=verbose),\n",
    "\n",
    "        # Reduce LR if the metric is not improved for 10 epochs, and stop training\n",
    "        # if it has not improved for 20 epochs.\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', patience=10, verbose=verbose),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='min', patience=20, verbose=verbose),\n",
    "        tf.keras.callbacks.TerminateOnNaN(),\n",
    "\n",
    "        # Log Tensorboard events.\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=args.logs_dir, write_steps_per_second=True, update_freq=10)\n",
    "    ]\n",
    "\n",
    "    # Horovod: save checkpoints only on the first worker to prevent other workers from corrupting them.\n",
    "    if hvd.rank() == 0:\n",
    "        ckpt_dir = tempfile.mkdtemp()\n",
    "        ckpt_file = os.path.join(ckpt_dir, 'checkpoint.h5')\n",
    "        atexit.register(lambda: shutil.rmtree(ckpt_dir))\n",
    "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n",
    "            ckpt_file, monitor='val_auc', mode='min', save_best_only=True))\n",
    "\n",
    "    if args.dataloader == PETASTORM_DATALOADER:\n",
    "        from petastorm import make_batch_reader\n",
    "        from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "        # Make Petastorm readers.\n",
    "        with make_batch_reader(f'{args.data_dir}/train',\n",
    "                               num_epochs=None,\n",
    "                               cur_shard=hvd.rank(),\n",
    "                               shard_count=hvd.size(),\n",
    "                               hdfs_driver='libhdfs') as train_reader:\n",
    "            with make_batch_reader(f'{args.data_dir}/val',\n",
    "                                   num_epochs=None,\n",
    "                                   cur_shard=hvd.rank(),\n",
    "                                   shard_count=hvd.size(),\n",
    "                                   hdfs_driver='libhdfs') as val_reader:\n",
    "                # Convert readers to tf.data.Dataset.\n",
    "                train_ds = make_petastorm_dataset(train_reader) \\\n",
    "                    .unbatch() \\\n",
    "                    .shuffle(10 * args.batch_size) \\\n",
    "                    .batch(args.batch_size) \\\n",
    "                    .map(lambda x: (tuple(getattr(x, c) for c in ALL_COLUMNS), x.clicked))\n",
    "\n",
    "                val_ds = make_petastorm_dataset(val_reader) \\\n",
    "                    .unbatch() \\\n",
    "                    .batch(args.batch_size) \\\n",
    "                    .map(lambda x: (tuple(getattr(x, c) for c in ALL_COLUMNS), x.clicked))\n",
    "\n",
    "                history = model.fit(train_ds,\n",
    "                                    validation_data=val_ds,\n",
    "                                    steps_per_epoch=int(train_rows / args.batch_size / hvd.size()),\n",
    "                                    validation_steps=int(val_rows / args.batch_size / hvd.size()),\n",
    "                                    callbacks=callbacks,\n",
    "                                    verbose=verbose,\n",
    "                                    epochs=args.epochs)\n",
    "\n",
    "    else:\n",
    "        import cupy\n",
    "\n",
    "        def seed_fn():\n",
    "            \"\"\"\n",
    "            Generate consistent dataloader shuffle seeds across workers\n",
    "            Reseeds each worker's dataloader each epoch to get fresh a shuffle\n",
    "            that's consistent across workers.\n",
    "            \"\"\"\n",
    "            min_int, max_int = tf.int32.limits\n",
    "            max_rand = max_int // hvd.size()\n",
    "            # Generate a seed fragment on each worker\n",
    "            seed_fragment = cupy.random.randint(0, max_rand).get()\n",
    "            # Aggregate seed fragments from all Horovod workers\n",
    "            seed_tensor = tf.constant(seed_fragment)\n",
    "            reduced_seed = hvd.allreduce(seed_tensor, name=\"shuffle_seed\", op=hvd.Sum)\n",
    "            return reduced_seed % max_rand\n",
    "\n",
    "        train_ds = KerasSequenceLoader(\n",
    "            f'{args.data_dir}/train',\n",
    "            batch_size=args.batch_size,\n",
    "            label_names=LABEL_COLUMNS,\n",
    "            cat_names=CATEGORICAL_COLUMNS,\n",
    "            cont_names=CONTINUOUS_COLUMNS,\n",
    "            engine=\"parquet\",\n",
    "            shuffle=True,\n",
    "            buffer_size=0.06,  # how many batches to load at once\n",
    "            parts_per_chunk=1,\n",
    "            global_size=hvd.size(),\n",
    "            global_rank=hvd.rank(),\n",
    "            seed_fn=seed_fn)\n",
    "\n",
    "        val_ds = KerasSequenceLoader(\n",
    "            f'{args.data_dir}/val',\n",
    "            batch_size=args.batch_size,\n",
    "            label_names=LABEL_COLUMNS,\n",
    "            cat_names=CATEGORICAL_COLUMNS,\n",
    "            cont_names=CONTINUOUS_COLUMNS,\n",
    "            engine=\"parquet\",\n",
    "            shuffle=False,\n",
    "            buffer_size=0.06,  # how many batches to load at once\n",
    "            parts_per_chunk=1,\n",
    "            global_size=hvd.size(),\n",
    "            global_rank=hvd.rank())\n",
    "\n",
    "        history = model.fit(train_ds,\n",
    "                            validation_data=val_ds,\n",
    "                            steps_per_epoch=int(train_rows / args.batch_size / hvd.size()),\n",
    "                            validation_steps=int(val_rows / args.batch_size / hvd.size()),\n",
    "                            callbacks=callbacks,\n",
    "                            verbose=verbose,\n",
    "                            epochs=args.epochs)\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed79d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dimensions, train_rows, val_rows, args):\n",
    "    # Horovod: run training.\n",
    "    history = horovod.spark.run(train_fn,\n",
    "                                args=(dimensions, train_rows, val_rows, args),\n",
    "                                env={'PATH':os.environ['PATH']},\n",
    "                                num_proc=args.num_proc,\n",
    "                                extra_mpi_args='-mca btl_tcp_if_include enp134s0f0 -x NCCL_IB_GID_INDEX=3',\n",
    "                                stdout=sys.stdout,\n",
    "                                stderr=sys.stderr,\n",
    "                                verbose=2,\n",
    "                                nics={},\n",
    "                                prefix_output_with_timestamp=True)[0]\n",
    "\n",
    "    best_val_loss = min(history['val_loss'])\n",
    "    print('Best Loss: %f' % best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b2339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Criteo Spark Keras Training Example',\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--data-dir', default='file:///opt/data/criteo/parquet',\n",
    "                        help='location of the transformed Criteo dataset in Parquet format')\n",
    "    parser.add_argument('--logs-dir', default='/opt/experiments/criteo', help='location of TensorFlow logs')\n",
    "    parser.add_argument('--dataloader', default=PETASTORM_DATALOADER,\n",
    "                        choices=[PETASTORM_DATALOADER, NVTABULAR_DATALOADER],\n",
    "                        help='dataloader to use')\n",
    "    parser.add_argument('--num-proc', type=int, default=1, help='number of worker processes for training')\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.0001, help='initial learning rate')\n",
    "    parser.add_argument('--batch-size', type=int, default=64 * 1024, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=3, help='number of epochs to train')\n",
    "    parser.add_argument('--local-checkpoint-file', default='checkpoint', help='model checkpoint')\n",
    "    args = parser.parse_args(args=['--num-proc', '1', '--data-dir', 'file:///data/', \n",
    "                                   '--dataloader', 'nvtabular', '--learning-rate', '0.001',\n",
    "                                   '--batch-size', '6553','--epochs', '1', '--logs-dir', 'tf_logs',\n",
    "                                   '--local-checkpoint-file', 'ckpt_file'])\n",
    "                                   \n",
    "\n",
    "    dimensions = get_category_dimensions(spark, args.data_dir)\n",
    "\n",
    "    train_df = spark.read.parquet(f'{args.data_dir}/train')\n",
    "    val_df = spark.read.parquet(f'{args.data_dir}/val')\n",
    "    test_df = spark.read.parquet(f'{args.data_dir}/test')\n",
    "    train_rows, val_rows, test_rows = train_df.count(), val_df.count(), test_df.count()\n",
    "    print('Training: %d' % train_rows)\n",
    "    print('Validation: %d' % val_rows)\n",
    "    print('Test: %d' % test_rows)\n",
    "\n",
    "    train(dimensions, train_rows, val_rows, args)\n",
    "\n",
    "#     spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4067c7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 12:55:13 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "21/12/22 12:55:13 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <Invoke> value#5359.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "    @Expression <AttributeReference> value#5359 could run on GPU\n",
      "  !Expression <AttributeReference> obj#5373 cannot run on GPU because expression AttributeReference obj#5373 produces an unsupported type ObjectType(class java.lang.String)\n",
      "  !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_c0': '0',\n",
      " '_c14': '327597',\n",
      " '_c15': '21882',\n",
      " '_c16': '14396',\n",
      " '_c17': '7063',\n",
      " '_c18': '19090',\n",
      " '_c19': '3',\n",
      " '_c20': '6484',\n",
      " '_c21': '1261',\n",
      " '_c22': '49',\n",
      " '_c23': '296349',\n",
      " '_c24': '103536',\n",
      " '_c25': '86419',\n",
      " '_c26': '10',\n",
      " '_c27': '2182',\n",
      " '_c28': '8235',\n",
      " '_c29': '61',\n",
      " '_c30': '3',\n",
      " '_c31': '938',\n",
      " '_c32': '14',\n",
      " '_c33': '327656',\n",
      " '_c34': '217544',\n",
      " '_c35': '319293',\n",
      " '_c36': '85130',\n",
      " '_c37': '9545',\n",
      " '_c38': '72',\n",
      " '_c39': '33'}\n",
      "Training: 152115810\n",
      "Validation: 12171190\n",
      "Test: 12163666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether extension tensorflow was built with MPI.\n",
      "Extension tensorflow was built with MPI.\n",
      "mpirun --allow-run-as-root --tag-output -np 1 -H 7123d402fe1d-43f1b58d65cc025cf85d240d6e66ce8a:1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib --timestamp-output     -x PATH -mca btl_tcp_if_include enp134s0f0 -x NCCL_IB_GID_INDEX=3 -x NCCL_DEBUG=INFO -mca plm_rsh_agent \"/databricks/conda/bin/python -m horovod.spark.driver.mpirun_rsh gAWVOgAAAAAAAAB9lCiMAmxvlF2UjAkxMjcuMC4wLjGUTUdOhpRhjARldGgwlF2UjAoxNzIuMTcuMC4ylE1HToaUYXUu gAWVBQMAAAAAAACMI2hvcm92b2QucnVubmVyLmNvbW1vbi51dGlsLnNldHRpbmdzlIwIU2V0dGluZ3OUk5QpgZR9lCiMCG51bV9wcm9jlEsBjAd2ZXJib3NllEsCjAhzc2hfcG9ydJROjBFzc2hfaWRlbnRpdHlfZmlsZZROjA5leHRyYV9tcGlfYXJnc5SMOS1tY2EgYnRsX3RjcF9pZl9pbmNsdWRlIGVucDEzNHMwZjAgLXggTkNDTF9JQl9HSURfSU5ERVg9M5SMCHRjcF9mbGFnlE6MDGJpbmRpbmdfYXJnc5ROjANrZXmUTowNc3RhcnRfdGltZW91dJSMImhvcm92b2QucnVubmVyLmNvbW1vbi51dGlsLnRpbWVvdXSUjAdUaW1lb3V0lJOUKYGUfZQojAhfdGltZW91dJRNWAKMC190aW1lb3V0X2F0lEdB2HDIooA8aYwIX21lc3NhZ2WUWA4BAABUaW1lZCBvdXQgd2FpdGluZyBmb3Ige2FjdGl2aXR5fS4gUGxlYXNlIGNoZWNrIHRoYXQgeW91IGhhdmUgZW5vdWdoIHJlc291cmNlcyB0byBydW4gYWxsIEhvcm92b2QgcHJvY2Vzc2VzLiBFYWNoIEhvcm92b2QgcHJvY2VzcyBydW5zIGluIGEgU3BhcmsgdGFzay4gWW91IG1heSBuZWVkIHRvIGluY3JlYXNlIHRoZSBzdGFydF90aW1lb3V0IHBhcmFtZXRlciB0byBhIGxhcmdlciB2YWx1ZSBpZiB5b3VyIFNwYXJrIHJlc291cmNlcyBhcmUgYWxsb2NhdGVkIG9uLWRlbWFuZC6UdWKMD291dHB1dF9maWxlbmFtZZROjA1ydW5fZnVuY19tb2RllIiMBG5pY3OUfZSMB2VsYXN0aWOUiYwccHJlZml4X291dHB1dF93aXRoX3RpbWVzdGFtcJSIjAVob3N0c5SMLzcxMjNkNDAyZmUxZC00M2YxYjU4ZDY1Y2MwMjVjZjg1ZDI0MGQ2ZTY2Y2U4YToxlHViLg==\" /databricks/conda/bin/python -m horovod.spark.task.mpirun_exec_fn gAWVOgAAAAAAAAB9lCiMAmxvlF2UjAkxMjcuMC4wLjGUTUdOhpRhjARldGgwlF2UjAoxNzIuMTcuMC4ylE1HToaUYXUu gAWV5gcAAAAAAACMI2hvcm92b2QucnVubmVyLmNvbW1vbi51dGlsLnNldHRpbmdzlIwIU2V0dGluZ3OUk5QpgZR9lCiMCG51bV9wcm9jlEsBjAd2ZXJib3NllEsCjAhzc2hfcG9ydJROjBFzc2hfaWRlbnRpdHlfZmlsZZROjA5leHRyYV9tcGlfYXJnc5RYFwUAAC1tY2EgYnRsX3RjcF9pZl9pbmNsdWRlIGVucDEzNHMwZjAgLXggTkNDTF9JQl9HSURfSU5ERVg9MyAteCBOQ0NMX0RFQlVHPUlORk8gLW1jYSBwbG1fcnNoX2FnZW50ICIvZGF0YWJyaWNrcy9jb25kYS9iaW4vcHl0aG9uIC1tIGhvcm92b2Quc3BhcmsuZHJpdmVyLm1waXJ1bl9yc2ggZ0FXVk9nQUFBQUFBQUFCOWxDaU1BbXh2bEYyVWpBa3hNamN1TUM0d0xqR1VUVWRPaHBSaGpBUmxkR2d3bEYyVWpBb3hOekl1TVRjdU1DNHlsRTFIVG9hVVlYVXUgZ0FXVkJRTUFBQUFBQUFDTUkyaHZjbTkyYjJRdWNuVnVibVZ5TG1OdmJXMXZiaTUxZEdsc0xuTmxkSFJwYm1kemxJd0lVMlYwZEdsdVozT1VrNVFwZ1pSOWxDaU1DRzUxYlY5d2NtOWpsRXNCakFkMlpYSmliM05sbEVzQ2pBaHpjMmhmY0c5eWRKUk9qQkZ6YzJoZmFXUmxiblJwZEhsZlptbHNaWlJPakE1bGVIUnlZVjl0Y0dsZllYSm5jNVNNT1MxdFkyRWdZblJzWDNSamNGOXBabDlwYm1Oc2RXUmxJR1Z1Y0RFek5ITXdaakFnTFhnZ1RrTkRURjlKUWw5SFNVUmZTVTVFUlZnOU01U01DSFJqY0Y5bWJHRm5sRTZNREdKcGJtUnBibWRmWVhKbmM1Uk9qQU5yWlhtVVRvd05jM1JoY25SZmRHbHRaVzkxZEpTTUltaHZjbTkyYjJRdWNuVnVibVZ5TG1OdmJXMXZiaTUxZEdsc0xuUnBiV1Z2ZFhTVWpBZFVhVzFsYjNWMGxKT1VLWUdVZlpRb2pBaGZkR2x0Wlc5MWRKUk5XQUtNQzE5MGFXMWxiM1YwWDJGMGxFZEIySERJb29BOGFZd0lYMjFsYzNOaFoyV1VXQTRCQUFCVWFXMWxaQ0J2ZFhRZ2QyRnBkR2x1WnlCbWIzSWdlMkZqZEdsMmFYUjVmUzRnVUd4bFlYTmxJR05vWldOcklIUm9ZWFFnZVc5MUlHaGhkbVVnWlc1dmRXZG9JSEpsYzI5MWNtTmxjeUIwYnlCeWRXNGdZV3hzSUVodmNtOTJiMlFnY0hKdlkyVnpjMlZ6TGlCRllXTm9JRWh2Y205MmIyUWdjSEp2WTJWemN5QnlkVzV6SUdsdUlHRWdVM0JoY21zZ2RHRnpheTRnV1c5MUlHMWhlU0J1WldWa0lIUnZJR2x1WTNKbFlYTmxJSFJvWlNCemRHRnlkRjkwYVcxbGIzVjBJSEJoY21GdFpYUmxjaUIwYnlCaElHeGhjbWRsY2lCMllXeDFaU0JwWmlCNWIzVnlJRk53WVhKcklISmxjMjkxY21ObGN5QmhjbVVnWVd4c2IyTmhkR1ZrSUc5dUxXUmxiV0Z1WkM2VWRXS01EMjkxZEhCMWRGOW1hV3hsYm1GdFpaUk9qQTF5ZFc1ZlpuVnVZMTl0YjJSbGxJaU1CRzVwWTNPVWZaU01CMlZzWVhOMGFXT1VpWXdjY0hKbFptbDRYMjkxZEhCMWRGOTNhWFJvWDNScGJXVnpkR0Z0Y0pTSWpBVm9iM04wYzVTTUx6Y3hNak5rTkRBeVptVXhaQzAwTTJZeFlqVTRaRFkxWTJNd01qVmpaamcxWkRJME1HUTJaVFkyWTJVNFlUb3hsSFZpTGc9PSKUjAh0Y3BfZmxhZ5ROjAxiaW5kaW5nX2FyZ3OUTowDa2V5lE6MDXN0YXJ0X3RpbWVvdXSUjCJob3Jvdm9kLnJ1bm5lci5jb21tb24udXRpbC50aW1lb3V0lIwHVGltZW91dJSTlCmBlH2UKIwIX3RpbWVvdXSUTVgCjAtfdGltZW91dF9hdJRHQdhwyKKAPGmMCF9tZXNzYWdllFgOAQAAVGltZWQgb3V0IHdhaXRpbmcgZm9yIHthY3Rpdml0eX0uIFBsZWFzZSBjaGVjayB0aGF0IHlvdSBoYXZlIGVub3VnaCByZXNvdXJjZXMgdG8gcnVuIGFsbCBIb3Jvdm9kIHByb2Nlc3Nlcy4gRWFjaCBIb3Jvdm9kIHByb2Nlc3MgcnVucyBpbiBhIFNwYXJrIHRhc2suIFlvdSBtYXkgbmVlZCB0byBpbmNyZWFzZSB0aGUgc3RhcnRfdGltZW91dCBwYXJhbWV0ZXIgdG8gYSBsYXJnZXIgdmFsdWUgaWYgeW91ciBTcGFyayByZXNvdXJjZXMgYXJlIGFsbG9jYXRlZCBvbi1kZW1hbmQulHVijA9vdXRwdXRfZmlsZW5hbWWUTowNcnVuX2Z1bmNfbW9kZZSIjARuaWNzlH2UjAdlbGFzdGljlImMHHByZWZpeF9vdXRwdXRfd2l0aF90aW1lc3RhbXCUiIwFaG9zdHOUjC83MTIzZDQwMmZlMWQtNDNmMWI1OGQ2NWNjMDI1Y2Y4NWQyNDBkNmU2NmNlOGE6MZR1Yi4=\n",
      "Wed Dec 22 12:55:17 2021[1,0]<stdout>:Changing cwd from / to /spark-3.1.2-bin-hadoop3.2/work/app-20211222124852-0005/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:2021-12-22 12:55:20.339416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:2021-12-22 12:55:20.339945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:2021-12-22 12:55:20.340149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: An invalid value was given for btl_tcp_if_include.  This\n",
      "value will be ignored.\n",
      "\n",
      "  Local host: 7123d402fe1d\n",
      "  Value:      enp134s0f0\n",
      "  Message:    Unknown interface name\n",
      "--------------------------------------------------------------------------\n",
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:2021-12-22 12:55:20.455369: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:2021-12-22 12:55:20.456247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:2021-12-22 12:55:20.456452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:20 2021[1,0]<stderr>:2021-12-22 12:55:20.456586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:2021-12-22 12:55:22.425424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:2021-12-22 12:55:22.425691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:2021-12-22 12:55:22.425864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:2021-12-22 12:55:22.426047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1504] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6635 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:01:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:Model: \"model\"\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:==================================================================================================\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c29 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c31 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c32 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c33 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c34 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c35 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________Wed Dec 22 12:55:22 2021[1,0]<stdout>:\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c36 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c37 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c38 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c39 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c14 (Embedding)      (None, 1, 2)         655196      ['_c14[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c15 (Embedding)      (None, 1, 2)         43766       ['_c15[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c16 (Embedding)      (None, 1, 2)         28794       ['_c16[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c17 (Embedding)      (None, 1, 2)         14128       ['_c17[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c18 (Embedding)      (None, 1, 2)         38182       ['_c18[0][0]']                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c20 (Embedding)      (None, 1, 2)         12970       ['_c20[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c21 (Embedding)      (None, 1, 2)         2524        ['_c21[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c22 (Embedding)      (None, 1, 2)         100         ['_c22[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c23 (Embedding)      (None, 1, 2)         592700      ['_c23[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c24 (Embedding)      (None, 1, 2)         207074      ['_c24[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c25 (Embedding)      (None, 1, 2)         172840      ['_c25[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c26 (Embedding)      (None, 1, 2)         22          ['_c26[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c27 (Embedding)      (None, 1, 2)         4366        ['_c27[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c28 (Embedding)      (None, 1, 2)         16472       ['_c28[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c29 (Embedding)      (None, 1, 2)         124         ['_c29[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c31 (Embedding)      (None, 1, 2)         1878        ['_c31[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c32 (Embedding)      (None, 1, 2)         30          ['_c32[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c33 (Embedding)      (None, 1, 2)         655314      ['_c33[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c34 (Embedding)      (None, 1, 2)         435090      ['_c34[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________Wed Dec 22 12:55:22 2021[1,0]<stdout>:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:/databricks/conda/lib/python3.8/site-packages/horovod/_keras/callbacks.py:58: UserWarning: Some callbacks may not have access to the averaged metrics, see https://github.com/horovod/horovod/issues/2440\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c35 (Embedding)      (None, 1, 2)         638588      ['_c35[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:2021-12-22 12:55:22.770363: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:2021-12-22 12:55:22.770391: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c36 (Embedding)      (None, 1, 2)         170262      ['_c36[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________Wed Dec 22 12:55:22 2021[1,0]<stdout>:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stderr>:2021-12-22 12:55:22.771391: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c37 (Embedding)      (None, 1, 2)         19092       ['_c37[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________Wed Dec 22 12:55:22 2021[1,0]<stdout>:\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c38 (Embedding)      (None, 1, 2)         146         ['_c38[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embedding__c39 (Embedding)      (None, 1, 2)         68          ['_c39[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embeddings_concat (Concatenate  (None, 1, 48)        0           ['embedding__c14[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:)                                                                 'embedding__c15[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c16[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c17[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c18[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c20[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c21[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c22[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c23[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c24[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c25[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c26[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c27[0][0]',         Wed Dec 22 12:55:22 2021[1,0]<stdout>:\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c28[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c29[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c31[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c32[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c33[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c34[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c35[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c36[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c37[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c38[0][0]',         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'embedding__c39[0][0]']         \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c30 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:embeddings_flatten (Flatten)    (None, 48)           0           ['embeddings_concat[0][0]']      \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:one_hot__c19 (CategoryEncoding  (None, 4)            0           ['_c19[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:)                                                                                                 \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:one_hot__c30 (CategoryEncoding  (None, 4)            0           ['_c30[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:)                                                                                                 \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c6 (InputLayer)                [(None, 1)]          0           []                               Wed Dec 22 12:55:22 2021[1,0]<stdout>:\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:_c13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:inputs_concat (Concatenate)     (None, 69)           0           ['embeddings_flatten[0][0]',     \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'one_hot__c19[0][0]',           Wed Dec 22 12:55:22 2021[1,0]<stdout>:\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  'one_hot__c30[0][0]',           \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c1[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c2[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c3[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c4[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c5[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c6[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c7[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c8[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c9[0][0]',                    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c10[0][0]',                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c11[0][0]',                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c12[0][0]',                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:                                                                  '_c13[0][0]']                   \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:batch_normalization (BatchNorm  (None, 69)           276         ['inputs_concat[0][0]']          \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:alization)                                                                                        \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:dense (Dense)                   (None, 64)           4480        ['batch_normalization[0][0]']    \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:batch_normalization_1 (BatchNo  (None, 64)           256         ['dense[0][0]']                  \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:rmalization)                                                                                      \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:dense_1 (Dense)                 (None, 64)           4160        ['batch_normalization_1[0][0]']  \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:batch_normalization_2 (BatchNo  (None, 64)           256         ['dense_1[0][0]']                \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:rmalization)                                                                                      \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:dense_2 (Dense)                 (None, 64)           4160        ['batch_normalization_2[0][0]']  \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:batch_normalization_3 (BatchNo  (None, 64)           256         ['dense_2[0][0]']                \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:rmalization)                                                                                      \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:dense_3 (Dense)                 (None, 32)           2080        ['batch_normalization_3[0][0]']  \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:output (Dense)                  (None, 1)            33          ['dense_3[0][0]']                \n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:==================================================================================================\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:Total params: 3,725,683\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:Trainable params: 3,725,161\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:Non-trainable params: 522\n",
      "Wed Dec 22 12:55:22 2021[1,0]<stdout>:__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:23 2021[1,0]<stderr>:2021-12-22 12:55:23.242916: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "Wed Dec 22 12:55:23 2021[1,0]<stderr>:2021-12-22 12:55:23.244633: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:32 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:33 2021[1,0]<stderr>:2021-12-22 12:55:33.491105: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "Wed Dec 22 12:55:33 2021[1,0]<stderr>:2021-12-22 12:55:33.491136: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/23213 [..............................]Wed Dec 22 12:55:32 2021[1,0]<stdout>: - ETA: 47:44:40 - loss: 0.8806 - auc: 0.4889Wed Dec 22 12:55:33 2021[1,0]<stdout>:\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bWed Dec 22 12:55:33 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:33 2021[1,0]<stderr>:2021-12-22 12:55:33.912884: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "Wed Dec 22 12:55:33 2021[1,0]<stderr>:2021-12-22 12:55:33.913423: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "Wed Dec 22 12:55:33 2021[1,0]<stderr>:2021-12-22 12:55:33.973720: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1624 callback api events and 1606 activity events. \n",
      "Wed Dec 22 12:55:33 2021[1,0]<stderr>:2021-12-22 12:55:33.999093: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:2021-12-22 12:55:34.048614: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: tf_logs/plugins/profile/2021_12_22_12_55_34\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:2021-12-22 12:55:34.080719: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to tf_logs/plugins/profile/2021_12_22_12_55_34/7123d402fe1d.trace.json.gz\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:2021-12-22 12:55:34.156716: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: tf_logs/plugins/profile/2021_12_22_12_55_34\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:2021-12-22 12:55:34.164842: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to tf_logs/plugins/profile/2021_12_22_12_55_34/7123d402fe1d.memory_profile.json.gz\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:2021-12-22 12:55:34.167036: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: tf_logs/plugins/profile/2021_12_22_12_55_34\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:Dumped tool data for xplane.pb to tf_logs/plugins/profile/2021_12_22_12_55_34/7123d402fe1d.xplane.pb\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:Dumped tool data for overview_page.pb to tf_logs/plugins/profile/2021_12_22_12_55_34/7123d402fe1d.overview_page.pb\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:Dumped tool data for input_pipeline.pb to tf_logs/plugins/profile/2021_12_22_12_55_34/7123d402fe1d.input_pipeline.pb\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:Dumped tool data for tensorflow_stats.pb to tf_logs/plugins/profile/2021_12_22_12_55_34/7123d402fe1d.tensorflow_stats.pb\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:Dumped tool data for kernel_stats.pb to tf_logs/plugins/profile/2021_12_22_12_55_34/7123d402fe1d.kernel_stats.pb\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:33 2021[1,0]<stdout>:    2/23213 [..............................]Wed Dec 22 12:55:33 2021[1,0]<stdout>: - ETA: 7:46:49 - loss: 0.8471 - auc: 0.5131 Wed Dec 22 12:55:34 2021[1,0]<stdout>:\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bWed Dec 22 12:55:34 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_begin` time: 0.0664s). Check your callbacks.\n",
      "Wed Dec 22 12:55:34 2021[1,0]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.1761s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:56:14 2021[1,0]<stdout>: 2658/23213 [==>...........................]Wed Dec 22 12:56:14 2021[1,0]<stdout>: - ETA: 5:26 - loss: 0.1375 - auc: 0.7458Wed Dec 22 12:56:14 2021[1,0]<stdout>Wed Dec 22 12:56:14 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6495/23213 [=======>......................]Wed Dec 22 12:57:14 2021[1,0]<stdout>: - ETA: 4:22 - loss: 0.1339 - auc: 0.7624Wed Dec 22 12:57:14 2021[1,0]<stdout>4Wed Dec 22 12:57:14 2021[1,0]<stdout>Wed Dec 22 12:57:12 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10354/23213 [============>.................]Wed Dec 22 12:58:14 2021[1,0]<stdout>: - ETA: 3:21 - loss: 0.1327 - auc: 0.7685Wed Dec 22 12:58:15 2021[1,0]<stdout>5Wed Dec 22 12:58:14 2021[1,0]<stdout>Wed Dec 22 12:58:14 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 12:59:14 2021[1,0]<stdout>:14214/23213 [=================>............]Wed Dec 22 12:59:14 2021[1,0]<stdout>: - ETA: 2:20 - loss: 0.1320 - auc: 0.7721Wed Dec 22 12:59:15 2021[1,0]<stdout>Wed Dec 22 12:59:15 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 13:00:14 2021[1,0]<stdout>:18094/23213 [======================>.......]Wed Dec 22 13:00:14 2021[1,0]<stdout>: - ETA: 1:19 - loss: 0.1315 - auc: 0.7746Wed Dec 22 13:00:15 2021[1,0]<stdout>Wed Dec 22 13:00:14 2021[1,0]<stdout>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21902/23213 [===========================>..]Wed Dec 22 13:01:15 2021[1,0]<stdout>: - ETA: 20s - loss: 0.1312 - auc: 0.7765Wed Dec 22 13:01:15 2021[1,0]<stdoutWed Dec 22 13:01:14 2021[1,0]<stdout>:Wed Dec 22 13:01:14 2021[1,0]<stdout>:>:\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 22 13:01:43 2021[1,0]<stdout>:23213/23213 [==============================]Wed Dec 22 13:01:43 2021[1,0]<stdout>: - 378s 16ms/step - loss: 0.1311 - auc: 0.7770 - val_loss: 0.1280 - val_auc: 0.7929 22 13:01:20 2021[1,0]<stdout>:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: [0.12802616] of type <class 'list'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-4ccef04208e0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#     spark.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a8e5fa22b3e4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dimensions, train_rows, val_rows, args)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                 prefix_output_with_timestamp=True)[0]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best Loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mmin\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mAggregate\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mminimum\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexpression\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function_over_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_invoke_function_over_column\u001b[0;34m(name, col)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34m\"Invalid argument, not a string or column: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: [0.12802616] of type <class 'list'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d91cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
