{
 "cells": [
  {
   "cell_type": "raw",
   "id": "Td_alkbOv3Aj",
   "metadata": {
    "id": "Td_alkbOv3Aj"
   },
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"id\": \"Td_alkbOv3Aj\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"Td_alkbOv3Aj\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Spark RAPIDS Parquet acceleration\\n\",\n",
    "        \"\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"id\": \"c6ed860b\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"c6ed860b\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"<a target=\\\"_blank\\\" href=\\\"https://colab.research.google.com/github/NVIDIA/spark-rapids-examples/blob/main/examples/SQL%2BDF-Examples/demo/Spark_parquet_microkernels.ipynb\\\">\\n\",\n",
    "        \"  <img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/>\\n\",\n",
    "        \"</a>\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"id\": \"AhUsdz6jLdMi\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"AhUsdz6jLdMi\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"\\n\",\n",
    "        \"Before getting started - be sure to change your runtime to use a GPU Hardware accelerator! Use the Runtime -> \\\"Change runtime type\\\" menu option to add a GPU.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"id\": \"ZfNDlz0SM0DB\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ZfNDlz0SM0DB\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Let's get started using the RAPIDS Accelerator for Apache Spark\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"PzW61-K04A1E\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"PzW61-K04A1E\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"!nvidia-smi\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"!cat /proc/cpuinfo\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"OIEun51OCyC4\"\n",
    "      },\n",
    "      \"id\": \"OIEun51OCyC4\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"spark_version='3.5.0'\\n\",\n",
    "        \"rapids_version='24.12.0'\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"NEGt46X7nEqf\"\n",
    "      },\n",
    "      \"id\": \"NEGt46X7nEqf\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"%pip install --quiet \\\\\\n\",\n",
    "        \"  pyspark=={spark_version}\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"g9XK28gcnHiG\"\n",
    "      },\n",
    "      \"id\": \"g9XK28gcnHiG\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"from importlib.resources import files\\n\",\n",
    "        \"from pyspark.sql import SparkSession\\n\",\n",
    "        \"import glob\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import statistics\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"gr2msGD1nLh-\"\n",
    "      },\n",
    "      \"id\": \"gr2msGD1nLh-\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"pyspark_files = files('pyspark')\\n\",\n",
    "        \"spark_sql_jar_path, *_ = glob.glob(f\\\"{pyspark_files}/*/spark-sql_*jar\\\")\\n\",\n",
    "        \"spark_sql_jar = os.path.basename(spark_sql_jar_path)\\n\",\n",
    "        \"scala_version = re.search(r'^spark-sql_(\\\\d+.\\\\d+)-.*\\\\.jar$', spark_sql_jar).group(1)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"0uXK6z8KoFUt\"\n",
    "      },\n",
    "      \"id\": \"0uXK6z8KoFUt\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"spark = (\\n\",\n",
    "        \"    SparkSession.builder\\n\",\n",
    "        \"      .appName('Parquet Spark GPU Acceleration')\\n\",\n",
    "        \"      .master('local[*]')\\n\",\n",
    "        \"      .config('spark.driver.memory', '5g')\\n\",\n",
    "        \"      .config('spark.plugins', 'com.nvidia.spark.SQLPlugin')\\n\",\n",
    "        \"      .config('spark.jars.packages', f\\\"com.nvidia:rapids-4-spark_{scala_version}:{rapids_version}\\\")\\n\",\n",
    "        \"      .getOrCreate()\\n\",\n",
    "        \")\\n\",\n",
    "        \"spark\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ayT5VJQvnQv4\"\n",
    "      },\n",
    "      \"id\": \"ayT5VJQvnQv4\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"location = \\\"./TMP_DATA\\\"\\n\",\n",
    "        \"iters = 5\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"3VsYyTATpNG1\"\n",
    "      },\n",
    "      \"id\": \"3VsYyTATpNG1\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"from pyspark.sql.types import IntegerType, StringType, StructType, StructField\\n\",\n",
    "        \"from pyspark.sql import functions as F\\n\",\n",
    "        \"import random\\n\",\n",
    "        \"import string\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Define schema\\n\",\n",
    "        \"schema = StructType([\\n\",\n",
    "        \"    StructField(\\\"id\\\", IntegerType(), False),\\n\",\n",
    "        \"    StructField(\\\"name\\\", StringType(), False),\\n\",\n",
    "        \"    StructField(\\\"age\\\", IntegerType(), False),\\n\",\n",
    "        \"    StructField(\\\"salary\\\", IntegerType(), False)\\n\",\n",
    "        \"])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Function to generate random strings\\n\",\n",
    "        \"def random_string(length=10):\\n\",\n",
    "        \"    return ''.join(random.choices(string.ascii_letters, k=length))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Generate DataFrame with 20M rows\\n\",\n",
    "        \"df = spark.range(0, 20_000_000).toDF(\\\"id\\\") \\\\\\n\",\n",
    "        \"    .withColumn(\\\"name\\\", F.udf(lambda: random_string(), StringType())()) \\\\\\n\",\n",
    "        \"    .withColumn(\\\"age\\\", (F.rand() * 50 + 20).cast(IntegerType())) \\\\\\n\",\n",
    "        \"    .withColumn(\\\"salary\\\", (F.rand() * 100000 + 30000).cast(IntegerType()))\\n\",\n",
    "        \"\\n\",\n",
    "        \"df.write.mode(\\\"overwrite\\\").parquet(location)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"diUi3mxWh91X\"\n",
    "      },\n",
    "      \"id\": \"diUi3mxWh91X\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"# Run the Parquet scan test on the GPU\\n\",\n",
    "        \"spark.conf.set(\\\"spark.rapids.sql.enabled\\\",True)\\n\",\n",
    "        \"gpu_times = []\\n\",\n",
    "        \"for i in range(iters):\\n\",\n",
    "        \"    start = time.time()\\n\",\n",
    "        \"    df = spark.read.parquet(location).selectExpr(\\\"count(name) as rows\\\", \\\"avg(salary) as average_salary\\\", \\\"median(salary) as median_salary\\\", \\\"sum(salary) as total_salary\\\", \\\"avg(age) as average_age\\\", \\\"median(age) as median_age\\\")\\n\",\n",
    "        \"    if i == 0:\\n\",\n",
    "        \"      df.show()\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"      df.collect()\\n\",\n",
    "        \"    end = time.time()\\n\",\n",
    "        \"    gpu_times.append(end - start)\\n\",\n",
    "        \"\\n\",\n",
    "        \"gpu_median = statistics.median(gpu_times)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Median execution time of {iters} runs for GPU Parquet scan: {gpu_median:.3f}\\\")\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"iXaXVgBNt4pK\"\n",
    "      },\n",
    "      \"id\": \"iXaXVgBNt4pK\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"# Run the Parquet scan test on the CPU\\n\",\n",
    "        \"spark.conf.set(\\\"spark.rapids.sql.enabled\\\",False)\\n\",\n",
    "        \"cpu_times = []\\n\",\n",
    "        \"for i in range(iters):\\n\",\n",
    "        \"    start = time.time()\\n\",\n",
    "        \"    df = spark.read.parquet(location).selectExpr(\\\"count(name) as rows\\\", \\\"avg(salary) as average_salary\\\", \\\"median(salary) as median_salary\\\", \\\"sum(salary) as total_salary\\\", \\\"avg(age) as average_age\\\", \\\"median(age) as median_age\\\")\\n\",\n",
    "        \"    if i == 0:\\n\",\n",
    "        \"      df.show()\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"      df.collect()\\n\",\n",
    "        \"    end = time.time()\\n\",\n",
    "        \"    cpu_times.append(end - start)\\n\",\n",
    "        \"\\n\",\n",
    "        \"cpu_median = statistics.median(cpu_times)\\n\",\n",
    "        \"print(f\\\"Median execution time of {iters} runs for CPU Parquet scan: {cpu_median:.3f}\\\")\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"lUmVe12Wic5X\"\n",
    "      },\n",
    "      \"id\": \"lUmVe12Wic5X\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"# GPU speedup should be in the range of 5-10x\\n\",\n",
    "        \"speedup = cpu_median / gpu_median\\n\",\n",
    "        \"print(f\\\"GPU speedup: {speedup:.2f}x\\\")\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"CxROFk_AoQQl\"\n",
    "      },\n",
    "      \"id\": \"CxROFk_AoQQl\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"accelerator\": \"GPU\",\n",
    "    \"colab\": {\n",
    "      \"provenance\": []\n",
    "    },\n",
    "    \"gpuClass\": \"standard\",\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3.9.12 ('base')\",\n",
    "      \"language\": \"python\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.9.12\"\n",
    "    },\n",
    "    \"vscode\": {\n",
    "      \"interpreter\": {\n",
    "        \"hash\": \"5327a248d9883bedf47bfd9e608af95bf318797e621edcc550c6b5b3fdc820cc\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AhUsdz6jLdMi",
   "metadata": {
    "id": "AhUsdz6jLdMi"
   },
   "source": [
    "\n",
    "Before getting started - be sure to change your runtime to use a GPU Hardware accelerator! Use the Runtime -> \"Change runtime type\" menu option to add a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PzW61-K04A1E",
   "metadata": {
    "id": "PzW61-K04A1E"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "spark_version='3.5.0'\n",
    "rapids_version='24.12.0'"
   ],
   "metadata": {
    "id": "NEGt46X7nEqf"
   },
   "id": "NEGt46X7nEqf",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from importlib.resources import files\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import statistics"
   ],
   "metadata": {
    "id": "gr2msGD1nLh-"
   },
   "id": "gr2msGD1nLh-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName('Parquet Spark GPU Acceleration')\n",
    "      .master('local[*]')\n",
    "      .config('spark.driver.memory', '5g')\n",
    "      .config('spark.plugins', 'com.nvidia.spark.SQLPlugin')\n",
    "      .config('spark.jars.packages', f\"com.nvidia:rapids-4-spark_{scala_version}:{rapids_version}\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "spark"
   ],
   "metadata": {
    "id": "ayT5VJQvnQv4"
   },
   "id": "ayT5VJQvnQv4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Function to generate random strings\n",
    "def random_string(length=10):\n",
    "    return ''.join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "# Generate DataFrame with 20M rows\n",
    "df = spark.range(0, 20_000_000).toDF(\"id\") \\\n",
    "    .withColumn(\"name\", F.udf(lambda: random_string(), StringType())()) \\\n",
    "    .withColumn(\"age\", (F.rand() * 50 + 20).cast(IntegerType())) \\\n",
    "    .withColumn(\"salary\", (F.rand() * 100000 + 30000).cast(IntegerType()))\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(location)"
   ],
   "metadata": {
    "id": "diUi3mxWh91X"
   },
   "id": "diUi3mxWh91X",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the Parquet scan test on the CPU\n",
    "spark.conf.set(\"spark.rapids.sql.enabled\",False)\n",
    "cpu_times = []\n",
    "for i in range(iters):\n",
    "    start = time.time()\n",
    "    df = spark.read.parquet(location).selectExpr(\"count(name) as rows\", \"avg(salary) as average_salary\", \"median(salary) as median_salary\", \"sum(salary) as total_salary\", \"avg(age) as average_age\", \"median(age) as median_age\")\n",
    "    if i == 0:\n",
    "      df.show()\n",
    "    else:\n",
    "      df.collect()\n",
    "    end = time.time()\n",
    "    cpu_times.append(end - start)\n",
    "\n",
    "cpu_median = statistics.median(cpu_times)\n",
    "print(f\"Median execution time of {iters} runs for CPU Parquet scan: {cpu_median:.3f}\")"
   ],
   "metadata": {
    "id": "lUmVe12Wic5X"
   },
   "id": "lUmVe12Wic5X",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5327a248d9883bedf47bfd9e608af95bf318797e621edcc550c6b5b3fdc820cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
