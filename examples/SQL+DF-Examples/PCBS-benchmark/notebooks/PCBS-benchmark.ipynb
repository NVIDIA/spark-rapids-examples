{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee5437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d825f88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mnio\u001b[39m.\u001b[32mfile\u001b[39m.\u001b[32mPath\u001b[39m = /home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\n",
       "\u001b[36mx\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'yuanli\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m'pcbs\u001b[39m/\u001b[32m\"re-build\"\u001b[39m/\u001b[32m\"spark-rapids\"\u001b[39m/\u001b[32m'dist\u001b[39m/\u001b[32m'target\u001b[39m/\u001b[32m\"rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = java.nio.file.FileSystems.getDefault().getPath(\"/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\")\n",
    "val x = ammonite.ops.Path(path)\n",
    "interp.load.cp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345523e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-avro:3.1.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85cd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.lang.reflect.Method\n",
    "\n",
    "import org.apache.spark.{sql, TaskContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.catalyst.InternalRow\n",
    "import org.apache.spark.sql.catalyst.expressions.Attribute\n",
    "import org.apache.spark.sql.columnar.{CachedBatch, CachedBatchSerializer}\n",
    "import org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\n",
    "import org.apache.spark.sql.execution.SparkPlan\n",
    "import org.apache.spark.sql.internal.SQLConf\n",
    "import org.apache.spark.sql.vectorized.ColumnarBatch\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.storage.StorageLevel.MEMORY_ONLY\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.execution.columnar.DefaultCachedBatch\n",
    "import org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
    "import com.nvidia.spark.GpuCachedBatchSerializer\n",
    "import com.nvidia.spark.rapids.ParquetCachedBatch\n",
    "import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "import org.apache.spark.sql.execution.columnar.InMemoryRelation\n",
    "import com.nvidia.spark.ParquetCachedBatchSerializer\n",
    "\n",
    "import scala.reflect.runtime.universe._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261bce43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAPIDS_JAR = /home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createSparkSession: ()org.apache.spark.sql.SparkSession\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RAPIDS_JAR = \"/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\"\n",
    "\n",
    "def createSparkSession(): SparkSession = {\n",
    "    val conf = new SparkConf()\n",
    "    conf.setMaster(\"spark://yuanli-System-Product-Name:7077\")\n",
    "    conf.setAppName(\"PCBS Benchmark\")\n",
    "    conf.set(\"spark.jars\", \"/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\")\n",
    "    conf.set(\"spark.driver.memory\", \"4G\")\n",
    "    conf.set(\"spark.executor.memory\", \"4G\")\n",
    "    conf.set(\"spark.executor.cores\", \"4\")\n",
    "    conf.set(\"spark.locality.wait\", \"0\")\n",
    "    conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "\n",
    "    //rapids config\n",
    "    conf.set(\"spark.rapids.sql.enabled\", \"true\") \n",
    "    conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.25\") \n",
    "    conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "    conf.set(\"spark.rapids.memory.pinnedPool.size\", \"1G\")\n",
    "    \n",
    "    conf.set(\"spark.driver.extraJavaOptions\",\"-ea -Duser.timezone=UTC\")\n",
    "    conf.set(\"spark.executor.extraJavaOptions\",\"-ea -Duser.timezone=UTC -Dai.rapids.cudf.prefer-pinned=true\")\n",
    "    conf.set(\"spark.sql.cache.serializer\",\"com.nvidia.spark.ParquetCachedBatchSerializer\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", RAPIDS_JAR)\n",
    "    conf.set(\"spark.executor.extraClassPath\", RAPIDS_JAR)\n",
    "\n",
    "    SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b27fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file = /home/yuanli/work/reviews/raza-p...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/yuanli/work/reviews/raza-p..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.nvidia.spark.ParquetCachedBatchSerializer\n",
    "\n",
    "import java.lang.reflect.Method\n",
    "import org.apache.spark.{sql, TaskContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.catalyst.InternalRow\n",
    "import org.apache.spark.sql.catalyst.expressions.Attribute\n",
    "import org.apache.spark.sql.columnar.{CachedBatch, CachedBatchSerializer}\n",
    "import org.apache.spark.sql.execution.SparkPlan\n",
    "import org.apache.spark.sql.internal.SQLConf\n",
    "\n",
    "import org.apache.spark.sql.vectorized.ColumnarBatch\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.storage.StorageLevel.MEMORY_ONLY\n",
    "\n",
    "import java.io.{BufferedWriter, File, FileWriter, IOException}\n",
    "\n",
    "\n",
    "// // import org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
    "import org.apache.spark.sql.rapids._\n",
    "\n",
    "\n",
    "val file = new File(\"/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/aaaaa.txt\")\n",
    "val bw = new BufferedWriter(new FileWriter(file, true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97336659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class TestCachedBatchSerializer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class TestCachedBatchSerializer(\n",
    "    useCompression: Boolean,\n",
    "    batchSize: Int) extends DefaultCachedBatchSerializer {\n",
    "\n",
    "  override def convertInternalRowToCachedBatch(input: RDD[InternalRow],\n",
    "      schema: Seq[Attribute],\n",
    "      storageLevel: StorageLevel,\n",
    "      conf: SQLConf): RDD[CachedBatch] = {\n",
    "    convertForCacheInternal(input, schema, batchSize, useCompression)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809146ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class CloseableColumnBatchIterator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class CloseableColumnBatchIterator(iter: Iterator[ColumnarBatch]) extends\n",
    "    Iterator[ColumnarBatch] {\n",
    "  var cb: ColumnarBatch = _\n",
    "\n",
    "  private def closeCurrentBatch(): Unit = {\n",
    "    if (cb != null) {\n",
    "      cb.close()\n",
    "      cb = null\n",
    "    }\n",
    "  }\n",
    "\n",
    "  TaskContext.get().addTaskCompletionListener[Unit]((_: TaskContext) => {\n",
    "    closeCurrentBatch()\n",
    "  })\n",
    "\n",
    "  override def hasNext: Boolean = iter.hasNext\n",
    "\n",
    "  override def next(): ColumnarBatch = {\n",
    "    closeCurrentBatch()\n",
    "    cb = iter.next()\n",
    "    cb\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e715ccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "callPrivate: (obj: AnyRef, methodName: String, parameters: AnyRef*)Object\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callPrivate(obj: AnyRef, methodName: String, parameters:AnyRef*) = {\n",
    "  val parameterTypes = parameters.map(_.getClass())\n",
    "  val method = obj.getClass.getDeclaredMethod(methodName, parameterTypes:_*)\n",
    "  method.setAccessible(true)\n",
    "  method.invoke(obj, parameters:_*)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef13bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getReflectLogicalPlan: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.reflect.runtime.universe._\n",
    "def getReflectLogicalPlan(df: DataFrame) : LogicalPlan = {\n",
    "    val mirror = runtimeMirror(scala.reflect.runtime.universe.getClass.getClassLoader)\n",
    "    val instanceMirror = mirror.reflect(df)\n",
    "    val field = scala.reflect.runtime.universe.typeOf[DataFrame].decl(TermName(\"logicalPlan\")).asTerm\n",
    "    val fieldMirror = instanceMirror.reflectField(field)\n",
    "    fieldMirror.get.asInstanceOf[LogicalPlan]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea80ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readWriteCache: (acc: Boolean, spark: org.apache.spark.sql.SparkSession, ser: org.apache.spark.sql.columnar.CachedBatchSerializer, func: org.apache.spark.sql.DataFrame => (java.lang.reflect.Method, Seq[org.apache.spark.sql.execution.SparkPlan]), verifyFunc: org.apache.spark.sql.columnar.CachedBatch => Any, query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame)(Long, Long, Long)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def readWriteCache(\n",
    "      acc: Boolean,\n",
    "      spark: SparkSession,\n",
    "      ser: CachedBatchSerializer,\n",
    "      func: DataFrame => (Method, Seq[SparkPlan]),\n",
    "      verifyFunc: CachedBatch => Any,\n",
    "      query: SparkSession => sql.DataFrame) = {\n",
    "//     CachePerfWriter.appendLine(\"Writing cache 5 times\")\n",
    "    val writes = for (_ <- 0 until 5) yield {\n",
    "        \n",
    "      val df = query(spark).cache()\n",
    "      val storageLevel = MEMORY_ONLY\n",
    "//       val logicplan = df.logicalPlan\n",
    "    //try 1:  java.lang.ClassCastException: java.lang.reflect.Field cannot be cast\n",
    "        //to org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "//       val logicalPlan = df.getClass.getDeclaredField(\"logicalPlan\").asInstanceOf[LogicalPlan]\n",
    "        //\n",
    "        //try 2: java.lang.NoSuchMethodException: \n",
    "        //org.apache.spark.sql.internal.SessionState.executePlan\n",
    "        //(org.apache.spark.sql.execution.datasources.LogicalRelation)\n",
    "\n",
    "      \n",
    "//       val mirror = runtimeMirror(scala.reflect.runtime.universe.getClass.getClassLoader)\n",
    "//       val instanceMirror = mirror.reflect(df)\n",
    "//       val field = scala.reflect.runtime.universe.typeOf[DataFrame].decl(TermName(\"logicalPlan\")).asTerm\n",
    "//       val fieldMirror = instanceMirror.reflectField(field)\n",
    "//       val logicalPlan = fieldMirror.get.asInstanceOf[LogicalPlan]\n",
    "      val logicalPlan = getReflectLogicalPlan(df)\n",
    "    // try 3:\n",
    "//        val field = classOf[DataFrame].getDeclaredField(\"logicalPlan\")\n",
    "//        field.setAccessible(true)\n",
    "//        val logicalPlan = field.get(this).asInstanceOf[LogicalPlan]\n",
    "      \n",
    "//       val plan = callPrivate(spark.sessionState,\"executePlan\",logicalPlan).getClass.getDeclaredField(\"sparkPlan\").asInstanceOf[SparkPlan]\n",
    "      println(\"---------------------11111---------\")\n",
    "        val plan = spark.sessionState.executePlan(logicalPlan).sparkPlan\n",
    "      println(\"---------------------2222---------\")  \n",
    "        val relation = InMemoryRelation(ser, storageLevel, plan, None, logicalPlan)\n",
    "//       val relation = InMemoryRelation\n",
    "      val start = System.currentTimeMillis()\n",
    "        println(\"---------------------3333-----!!!----\") \n",
    "      val cb = relation.cacheBuilder.cachedColumnBuffers.first()\n",
    "        println(\"---------------------444---------\") \n",
    "      val defaWriteTime = System.currentTimeMillis() - start\n",
    "      verifyFunc(cb)\n",
    "        println(\"---------------------555---------\") \n",
    "      df.unpersist(true)\n",
    "      println(\"-----------write cache----cost time:\"+defaWriteTime+\"-----\")  \n",
    "      defaWriteTime\n",
    "        \n",
    "    }\n",
    "//     CachePerfWriter.appendLine(s\"Time taken for writes: $writes\")\n",
    "    val defaWriteTime = writes.sum / 5\n",
    "    println(\"-----------write cache----avg cost time:\"+defaWriteTime+\"-----\")  \n",
    "    val df = query(spark).cache()\n",
    "    val storageLevel = MEMORY_ONLY\n",
    "//     val logicalPlan = df.getClass.getDeclaredField(\"logicalPlan\").asInstanceOf[LogicalPlan]\n",
    "//     val mirror = runtimeMirror(scala.reflect.runtime.universe.getClass.getClassLoader)\n",
    "//       val instanceMirror = mirror.reflect(df)\n",
    "//       val field = scala.reflect.runtime.universe.typeOf[DataFrame].decl(TermName(\"logicalPlan\")).asTerm\n",
    "//       val fieldMirror = instanceMirror.reflectField(field)\n",
    "//       val logicalPlan = fieldMirror.get.asInstanceOf[LogicalPlan]\n",
    "    val logicalPlan = getReflectLogicalPlan(df)\n",
    "//     val plan = callPrivate(spark.sessionState,\"executePlan\",logicalPlan).getClass.getDeclaredField(\"sparkPlan\").asInstanceOf[SparkPlan]\n",
    "    val plan = spark.sessionState.executePlan(logicalPlan).sparkPlan\n",
    "    \n",
    "    \n",
    "    val relation = InMemoryRelation(ser, storageLevel, plan, None, logicalPlan)\n",
    "//     val relation = InMemoryRelation\n",
    "    \n",
    "    \n",
    "    relation.cacheBuilder.cachedColumnBuffers\n",
    "    val (doExecuteMethod, inMemoryScans) = func(df)\n",
    "//     CachePerfWriter.appendLine(\"Reading cache 10 times\")\n",
    "    val reads = for (_ <- 0 until 5) yield {\n",
    "      val start = System.currentTimeMillis()\n",
    "      val inMemoryScan = inMemoryScans.head\n",
    "      println(\"---------------------AAAAAA---------\")\n",
    "      println(inMemoryScan.getClass)\n",
    "      val rdd = doExecuteMethod.invoke(inMemoryScan).asInstanceOf[RDD[ColumnarBatch]]\n",
    "      println(\"---------------------BBBBB---------\")\n",
    "      if (ser.isInstanceOf[ParquetCachedBatchSerializer] && acc) {\n",
    "        println(\"---------------------CCCC111-----!!!----\")\n",
    "        rdd.mapPartitions(iter => CloseableColumnBatchIterator(iter)).count()\n",
    "        println(\"---------------------CCCC222---------\")\n",
    "        rdd.foreach {\n",
    "          println(\"---------------------CCCC333---------\")\n",
    "          cb => cb.close()\n",
    "        }\n",
    "        println(\"---------------------CCCC4444---------\")\n",
    "      } else {\n",
    "        println(\"---------------------DDDDD---------\")\n",
    "        rdd.count()\n",
    "      }\n",
    "      \n",
    "      val defaReadTime = System.currentTimeMillis() - start\n",
    "      println(\"-----------read cache-----cost time:\"+defaReadTime+\"----\") \n",
    "      defaReadTime\n",
    "\n",
    "    }\n",
    "//     CachePerfWriter.appendLine(s\"Time taken for reads: $reads\")\n",
    "    val defaReadTime = reads.slice(1, reads.length).sum / 9\n",
    "    println(\"-----------read cache----avg cost time:\"+defaReadTime+\"-----\")  \n",
    "\n",
    "    df.unpersist()\n",
    "    (defaWriteTime, defaReadTime, reads(0))\n",
    "//     (defaWriteTime, 1, 1)\n",
    "  }\n",
    "\n",
    "// println(\"aaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8411165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:100: warning: abstract type pattern T is unchecked since it is eliminated by erasure\n",
       "             case _: T =>\n",
       "                     ^\n",
       "runDefaInternal: [T](query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame, acc: Boolean, ser: org.apache.spark.sql.columnar.CachedBatchSerializer)(Long, Long, Long)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
    "\n",
    "\n",
    "def runDefaInternal[T](\n",
    "      query: SparkSession => DataFrame,\n",
    "      acc: Boolean,\n",
    "      ser: CachedBatchSerializer) = {\n",
    "    val spark = createSparkSession()\n",
    "    if (acc) {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    } else {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"false\")\n",
    "    }\n",
    "    println(\"---------------------\"+acc+\"---------\")\n",
    "     \n",
    "  readWriteCache(acc, spark, ser, { df =>\n",
    "    val doExecuteMethod =\n",
    "      classOf[InMemoryTableScanExec].getDeclaredMethod(\"doExecute\")\n",
    "    doExecuteMethod.setAccessible(true)\n",
    "    val inMemScans = df.queryExecution.executedPlan.collect {\n",
    "      case m: InMemoryTableScanExec => m\n",
    "    }\n",
    "    (doExecuteMethod, inMemScans)\n",
    "  }, cb =>\n",
    "    cb match {\n",
    "      case _: T =>\n",
    "      case other => throw new IllegalStateException(s\"Unexpected cached batch type: ${other.getClass.getName}\")\n",
    "    }, query )\n",
    "    \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eed262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runPcbsInternal: (query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame, acc: Boolean)(Boolean, Long, Long, Long)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def runPcbsInternal(query: SparkSession => DataFrame, acc: Boolean): (Boolean, Long, Long, Long) = {\n",
    "    val spark = createSparkSession()\n",
    "    if (acc) {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    } else {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"false\")\n",
    "    }\n",
    "    println(\"----------\"+acc+\"--------------\")\n",
    "    val (pcbsWriteTime, pcbsReadTime, firstRead) =\n",
    "      readWriteCache(acc, spark, new ParquetCachedBatchSerializer(), { df =>\n",
    "        val doExecuteMethod =\n",
    "          classOf[GpuInMemoryTableScanExec].getDeclaredMethod(\"doExecuteColumnar\")\n",
    "        doExecuteMethod.setAccessible(true)\n",
    "        val inMemScans = df.queryExecution.executedPlan.collect {\n",
    "          case g: GpuInMemoryTableScanExec => g\n",
    "          case m: InMemoryTableScanExec => m\n",
    "        }\n",
    "        (doExecuteMethod, inMemScans)\n",
    "      }, cb =>\n",
    "        cb match {\n",
    "          case _: ParquetCachedBatch =>\n",
    "          case other => throw new IllegalStateException(s\"Unexpected cached batch type: ${other.getClass.getName}\")\n",
    "        }, query)\n",
    "\n",
    "    (acc, pcbsWriteTime, firstRead, pcbsReadTime)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7701b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runPcbs: (query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "def runPcbs(query: SparkSession => DataFrame): Unit = {\n",
    "    println(\"------runPcbsInternal--------\")\n",
    "     runPcbsInternal(query, true)\n",
    "    println(\"------runDefaInternal--------\")\n",
    "    val pcbs = new ParquetCachedBatchSerializer()\n",
    "      runDefaInternal[ParquetCachedBatch](query, false, pcbs)\n",
    "    println(\"------recording--------\")\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7893113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------runPcbsInternal--------\n",
      "----------true--------------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:695-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:318-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:346-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:295-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:267-----\n",
      "-----------write cache----avg cost time:384-----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache-----cost time:207----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache-----cost time:100----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache-----cost time:95----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache-----cost time:101----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache-----cost time:114----\n",
      "-----------read cache----avg cost time:45-----\n",
      "------runDefaInternal--------\n",
      "---------------------false---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:1492-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:878-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:806-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:790-----\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache----cost time:782-----\n",
      "-----------write cache----avg cost time:949-----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache-----cost time:834----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache-----cost time:182----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache-----cost time:169----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache-----cost time:179----\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = file:///home/yuanli/work/pcbs/part-00070-086b31d9-2f77-4f9b-8d82-a526703fd9d8-c000.snappy.parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------read cache-----cost time:165----\n",
      "-----------read cache----avg cost time:77-----\n",
      "------recording--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file:///home/yuanli/work/pcbs/part-00070-086b31d9-2f77-4f9b-8d82-a526703fd9d8-c000.snappy.parquet"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"file:///home/yuanli/work/pcbs/part-00070-086b31d9-2f77-4f9b-8d82-a526703fd9d8-c000.snappy.parquet\"\n",
    "try {\n",
    "//   CachePerfWriter.appendLine(s\"Reading file: $path\")\n",
    "  runPcbs(spark => spark.read.parquet(path))\n",
    "} finally {\n",
    "//   CachePerfWriter.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68150072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcbs - Scala",
   "language": "scala",
   "name": "pcbs_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
