{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee5437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d825f88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mnio\u001b[39m.\u001b[32mfile\u001b[39m.\u001b[32mPath\u001b[39m = /home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\n",
       "\u001b[36mx\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'yuanli\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m'pcbs\u001b[39m/\u001b[32m\"re-build\"\u001b[39m/\u001b[32m\"spark-rapids\"\u001b[39m/\u001b[32m'dist\u001b[39m/\u001b[32m'target\u001b[39m/\u001b[32m\"rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import $ivy.`com.nvidia::rapids-4-spark:22.04.0`\n",
    "// import ammonite.ops.\n",
    "// ammonite.ops.\n",
    "\n",
    "val path = java.nio.file.FileSystems.getDefault().getPath(\"/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\")\n",
    "\n",
    "val x = ammonite.ops.Path(path)\n",
    "\n",
    "interp.load.cp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345523e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-avro:3.1.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890b1d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version 2.12.10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.Properties.versionString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85cd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.{BufferedWriter, File, FileWriter, IOException}\n",
    "import java.lang.reflect.Method\n",
    "import java.util.{Locale, TimeZone}\n",
    "\n",
    "// import com.nvidia.spark.ParquetCachedBatchSerializer\n",
    "// import spark312.com.nvidia.spark.rapids.ParquetCachedBatch\n",
    "\n",
    "// // import com.nvidia.CachePerfWriter\n",
    "\n",
    "import org.apache.spark.{sql, TaskContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.catalyst.InternalRow\n",
    "import org.apache.spark.sql.catalyst.expressions.Attribute\n",
    "import org.apache.spark.sql.columnar.{CachedBatch, CachedBatchSerializer}\n",
    "import org.apache.spark.sql.execution.SparkPlan\n",
    "import org.apache.spark.sql.internal.SQLConf\n",
    "import org.apache.spark.sql.vectorized.ColumnarBatch\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.storage.StorageLevel.MEMORY_ONLY\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.execution.columnar.DefaultCachedBatch\n",
    "\n",
    "// import org.apache.spark.sql.rapids._\n",
    "import com.nvidia.spark.GpuCachedBatchSerializer\n",
    "// import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a09da7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAPIDS_JAR = /home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RAPIDS_JAR = \"/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\"\n",
    "\n",
    "// val conf = new SparkConf()\n",
    "// conf.setMaster(\"spark://yuanli-System-Product-Name:7077\")\n",
    "// conf.setAppName(\"PCBS Benchmark\")\n",
    "// conf.set(\"spark.jars\", \"/home/yuanli/work/jars/v22.06.0/rapids-4-spark_2.12-22.06.0-20220512.134355-33-cuda11.jar\")\n",
    "// conf.set(\"spark.driver.memory\", \"4G\")\n",
    "// conf.set(\"spark.executor.memory\", \"4G\")\n",
    "// conf.set(\"spark.executor.cores\", \"4\")\n",
    "// conf.set(\"spark.locality.wait\", \"0\")\n",
    "// conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "\n",
    "// //rapids config\n",
    "// conf.set(\"spark.rapids.sql.enabled\", \"true\") \n",
    "// conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "// conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "// conf.set(\"spark.task.resource.gpu.amount\", \"0.25\") \n",
    "// conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "// conf.set(\"spark.rapids.memory.pinnedPool.size\", \"false\")\n",
    "// conf.set(\"spark.driver.extraClassPath\", RAPIDS_JAR)\n",
    "// conf.set(\"spark.executor.extraClassPath\", RAPIDS_JAR)\n",
    "\n",
    "\n",
    "// val spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "// // val spark = {\n",
    "// //   NotebookSparkSession.builder()\n",
    "// //     .master(\"local[*]\")\n",
    "// //     .getOrCreate()\n",
    "// // }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261bce43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createSparkSession: ()org.apache.spark.sql.SparkSession\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.util.{Locale, TimeZone}\n",
    "def createSparkSession(): SparkSession = {\n",
    "    // Timezone is fixed to UTC to allow timestamps to work by default\n",
    "    TimeZone.setDefault(TimeZone.getTimeZone(\"UTC\"))\n",
    "    // Add Locale setting\n",
    "    Locale.setDefault(Locale.US)\n",
    "    val sparkMasterUrl = System.getenv(\"SPARK_MASTER_URL\")\n",
    "    //-------------\n",
    "    val conf = new SparkConf()\n",
    "    conf.setMaster(\"spark://yuanli-System-Product-Name:7077\")\n",
    "    conf.setAppName(\"PCBS Benchmark\")\n",
    "    conf.set(\"spark.jars\", \"/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\")\n",
    "    conf.set(\"spark.driver.memory\", \"4G\")\n",
    "    conf.set(\"spark.executor.memory\", \"4G\")\n",
    "    conf.set(\"spark.executor.cores\", \"4\")\n",
    "    conf.set(\"spark.locality.wait\", \"0\")\n",
    "    conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "\n",
    "    //rapids config\n",
    "    conf.set(\"spark.rapids.sql.enabled\", \"true\") \n",
    "    conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.25\") \n",
    "    conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "    conf.set(\"spark.rapids.memory.pinnedPool.size\", \"1G\")\n",
    "    \n",
    "    conf.set(\"spark.driver.extraJavaOptions\",\"-ea -Duser.timezone=UTC\")\n",
    "    conf.set(\"spark.executor.extraJavaOptions\",\"-ea -Duser.timezone=UTC -Dai.rapids.cudf.prefer-pinned=true\")\n",
    "    conf.set(\"spark.sql.cache.serializer\",\"com.nvidia.spark.ParquetCachedBatchSerializer\")\n",
    "//     conf.set(\"\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", RAPIDS_JAR)\n",
    "    conf.set(\"spark.executor.extraClassPath\", RAPIDS_JAR)\n",
    "    //-------------\n",
    "    SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "//     val builder = SparkSession.builder()\n",
    "//         .master(sparkMasterUrl)\n",
    "//         .appName(\"pcbs perf (scala)\")\n",
    "//     builder.getOrCreate()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10aafae",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// %AddJar file:///home/yuanli/work/jars/v22.06.0/rapids-4-spark_2.12-22.06.0-20220512.134355-33-cuda11.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9562126b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// %AddJar https://repo1.maven.org/maven2/us/codecraft/webmagic-core/0.7.5/webmagic-core-0.7.5.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e8c247",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// import us.codecraft.webmagic.Site;\n",
    "// val site = Site.me().setRetryTimes(3).setSleepTime(100).setTimeOut(10000);\n",
    "// val file = new File(\"/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/aaaaa.txt\")\n",
    "// val bw = new BufferedWriter(new FileWriter(file, true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b27fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file = /home/yuanli/work/reviews/raza-p...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/yuanli/work/reviews/raza-p..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.nvidia.spark.ParquetCachedBatchSerializer\n",
    "// import org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\n",
    "// import com.nvidia.spark.rapids.ParquetCachedBatch\n",
    "import java.lang.reflect.Method\n",
    "import org.apache.spark.{sql, TaskContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.catalyst.InternalRow\n",
    "import org.apache.spark.sql.catalyst.expressions.Attribute\n",
    "import org.apache.spark.sql.columnar.{CachedBatch, CachedBatchSerializer}\n",
    "import org.apache.spark.sql.execution.SparkPlan\n",
    "import org.apache.spark.sql.internal.SQLConf\n",
    "\n",
    "import org.apache.spark.sql.vectorized.ColumnarBatch\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.storage.StorageLevel.MEMORY_ONLY\n",
    "\n",
    "import java.io.{BufferedWriter, File, FileWriter, IOException}\n",
    "\n",
    "\n",
    "// // import org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
    "import org.apache.spark.sql.rapids._\n",
    "\n",
    "\n",
    "val file = new File(\"/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/aaaaa.txt\")\n",
    "val bw = new BufferedWriter(new FileWriter(file, true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d167d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object CachePerfWriter\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.io.{BufferedWriter, File, FileWriter, IOException}\n",
    "object CachePerfWriter{\n",
    "  val file = new File(\"/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/aaaaa.txt\")\n",
    "  lazy val bw = new BufferedWriter(new FileWriter(file, true))\n",
    "\n",
    "  @throws[IOException]\n",
    "  def appendLine(a: String): Unit = {\n",
    "    bw.append(s\"$a\\n\")\n",
    "  }\n",
    "\n",
    "  def appendTimes(\n",
    "      acc: Boolean,\n",
    "      write: Long,\n",
    "      firstRead: Long,\n",
    "      readAvg: Long): Unit ={\n",
    "    appendLine(s\"acc: $acc\")\n",
    "    appendLine(\"Average write: \" + write)\n",
    "    appendLine(\"Time taken for frst read: \" + firstRead)\n",
    "    appendLine(\"Average read (without first read): \" + readAvg)\n",
    "  }\n",
    "\n",
    "  def close(): Unit = {\n",
    "    bw.close()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6023fd6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// // @throws[IOException]\n",
    "// def CachePerfWriter(a: String){\n",
    "//   val file = new File(\"/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/aaaaa.txt\")\n",
    "//   lazy val bw = new BufferedWriter(new FileWriter(file, true))\n",
    "//   bw.append(s\"$a\\n\")  \n",
    "    \n",
    "    \n",
    "// //   bw.close()  \n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b06e22",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// try{\n",
    "//     CachePerfWriter.appendLine(\"efwfewfw\")\n",
    "// } finally {\n",
    "//     CachePerfWriter.close()\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e92d6ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// // Build the spark session and data reader as usual\n",
    "// val sparkSession = SparkSession.builder.appName(\"pcbs-benchmark\").getOrCreate\n",
    "// val reader = sparkSession.read.option(\"header\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b715e5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// import sys.process._\n",
    "// %AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-5.3.jar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb708972",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// // %Ismagic\n",
    "// import java.lang.ClassLoader\n",
    "// val cl = ClassLoader.getSystemClassLoader\n",
    "// cl.asInstanceOf[java.net.URLClassLoader].getURLs.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97336659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class TestCachedBatchSerializer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\n",
    "\n",
    "// class TestCachedBatchSerializer(\n",
    "//     useCompression: Boolean,\n",
    "//     batchSize: Int) extends DefaultCachedBatchSerializer {\n",
    "\n",
    "//     def convertInternalRowToCachedBatch() = {\n",
    "//       println(\"aa\")\n",
    "// //     convertForCacheInternal(input, schema, batchSize, useCompression)\n",
    "//   }\n",
    "\n",
    "// }\n",
    "class TestCachedBatchSerializer(\n",
    "    useCompression: Boolean,\n",
    "    batchSize: Int) extends DefaultCachedBatchSerializer {\n",
    "\n",
    "  override def convertInternalRowToCachedBatch(input: RDD[InternalRow],\n",
    "      schema: Seq[Attribute],\n",
    "      storageLevel: StorageLevel,\n",
    "      conf: SQLConf): RDD[CachedBatch] = {\n",
    "    convertForCacheInternal(input, schema, batchSize, useCompression)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28c4971",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// val c1 = new DefaultCachedBatchSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809146ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class CloseableColumnBatchIterator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class CloseableColumnBatchIterator(iter: Iterator[ColumnarBatch]) extends\n",
    "    Iterator[ColumnarBatch] {\n",
    "  var cb: ColumnarBatch = _\n",
    "\n",
    "  private def closeCurrentBatch(): Unit = {\n",
    "    if (cb != null) {\n",
    "      cb.close()\n",
    "      cb = null\n",
    "    }\n",
    "  }\n",
    "\n",
    "  TaskContext.get().addTaskCompletionListener[Unit]((_: TaskContext) => {\n",
    "    closeCurrentBatch()\n",
    "  })\n",
    "\n",
    "  override def hasNext: Boolean = iter.hasNext\n",
    "\n",
    "  override def next(): ColumnarBatch = {\n",
    "    closeCurrentBatch()\n",
    "    cb = iter.next()\n",
    "    cb\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70675d7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// // package a\n",
    "// var myList = Array(1.9, 2.9, 3.4, 3.5)\n",
    "     \n",
    "//       // 输出所有数组元素\n",
    "//       for ( x <- myList ) {\n",
    "//          println( x )\n",
    "//       }\n",
    "\n",
    "//       // 计算数组所有元素的总和\n",
    "//       var total = 0.0;\n",
    "//       for ( i <- 0 to (myList.length - 1)) {\n",
    "//          total += myList(i);\n",
    "//       }\n",
    "//       println(\"总和为 \" + total);\n",
    "\n",
    "//       // 查找数组中的最大元素\n",
    "//       var max = myList(0);\n",
    "//       for ( i <- 1 to (myList.length - 1) ) {\n",
    "//          if (myList(i) > max) max = myList(i);\n",
    "//       }\n",
    "//       println(\"最大值为 \" + max);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d833ab1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// // %package a\n",
    "// import org.apache.spark.sql.execution.columnar.Util\n",
    "// println(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3c82cc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// println(classOf[DataFrame].getDeclaredField(\"logicalPlan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e715ccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "callPrivate: (obj: AnyRef, methodName: String, parameters: AnyRef*)Object\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callPrivate(obj: AnyRef, methodName: String, parameters:AnyRef*) = {\n",
    "  val parameterTypes = parameters.map(_.getClass())\n",
    "  val method = obj.getClass.getDeclaredMethod(methodName, parameterTypes:_*)\n",
    "  method.setAccessible(true)\n",
    "  method.invoke(obj, parameters:_*)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39edb5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getPrivateField: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getPrivateField() = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dea80ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readWriteCache: (acc: Boolean, spark: org.apache.spark.sql.SparkSession, ser: org.apache.spark.sql.columnar.CachedBatchSerializer, func: org.apache.spark.sql.DataFrame => (java.lang.reflect.Method, Seq[org.apache.spark.sql.execution.SparkPlan]), verifyFunc: org.apache.spark.sql.columnar.CachedBatch => Any, query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame)(Long, Long, Long)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import java.lang.reflect.Method\n",
    "import org.apache.spark.sql.execution.SparkPlan\n",
    "// import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "import org.apache.spark.sql.execution.columnar.InMemoryRelation\n",
    "// import com.nvidia.spark.ParquetCachedBatchSerializer\n",
    "\n",
    "import scala.reflect.runtime.universe._\n",
    "\n",
    "\n",
    "\n",
    "def readWriteCache(\n",
    "      acc: Boolean,\n",
    "      spark: SparkSession,\n",
    "      ser: CachedBatchSerializer,\n",
    "      func: DataFrame => (Method, Seq[SparkPlan]),\n",
    "      verifyFunc: CachedBatch => Any,\n",
    "      query: SparkSession => sql.DataFrame) = {\n",
    "//     CachePerfWriter.appendLine(\"Writing cache 5 times\")\n",
    "    val writes = for (_ <- 0 until 5) yield {\n",
    "        \n",
    "      val df = query(spark).cache()\n",
    "      val storageLevel = MEMORY_ONLY\n",
    "//       val logicplan = df.logicalPlan\n",
    "    //try 1:  java.lang.ClassCastException: java.lang.reflect.Field cannot be cast\n",
    "        //to org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "//       val logicalPlan = df.getClass.getDeclaredField(\"logicalPlan\").asInstanceOf[LogicalPlan]\n",
    "        //\n",
    "        //try 2: java.lang.NoSuchMethodException: \n",
    "        //org.apache.spark.sql.internal.SessionState.executePlan\n",
    "        //(org.apache.spark.sql.execution.datasources.LogicalRelation)\n",
    "\n",
    "      import scala.reflect.runtime.universe._\n",
    "      val mirror = runtimeMirror(scala.reflect.runtime.universe.getClass.getClassLoader)\n",
    "      val instanceMirror = mirror.reflect(df)\n",
    "      val field = scala.reflect.runtime.universe.typeOf[DataFrame].decl(TermName(\"logicalPlan\")).asTerm\n",
    "      val fieldMirror = instanceMirror.reflectField(field)\n",
    "      val logicalPlan = fieldMirror.get.asInstanceOf[LogicalPlan]\n",
    "    // try 3:\n",
    "//        val field = classOf[DataFrame].getDeclaredField(\"logicalPlan\")\n",
    "//        field.setAccessible(true)\n",
    "//        val logicalPlan = field.get(this).asInstanceOf[LogicalPlan]\n",
    "      \n",
    "//       val plan = callPrivate(spark.sessionState,\"executePlan\",logicalPlan).getClass.getDeclaredField(\"sparkPlan\").asInstanceOf[SparkPlan]\n",
    "      println(\"---------------------11111---------\")\n",
    "        val plan = spark.sessionState.executePlan(logicalPlan).sparkPlan\n",
    "      println(\"---------------------2222---------\")  \n",
    "        val relation = InMemoryRelation(ser, storageLevel, plan, None, logicalPlan)\n",
    "//       val relation = InMemoryRelation\n",
    "      val start = System.currentTimeMillis()\n",
    "        println(\"---------------------3333-----!!!----\") \n",
    "      val cb = relation.cacheBuilder.cachedColumnBuffers.first()\n",
    "        println(\"---------------------444---------\") \n",
    "      val defaWriteTime = System.currentTimeMillis() - start\n",
    "      verifyFunc(cb)\n",
    "        println(\"---------------------555---------\") \n",
    "      df.unpersist(true)\n",
    "      println(\"-----------write cache---------\")  \n",
    "      defaWriteTime\n",
    "        \n",
    "    }\n",
    "//     CachePerfWriter.appendLine(s\"Time taken for writes: $writes\")\n",
    "    val defaWriteTime = writes.sum / 5\n",
    "\n",
    "    val df = query(spark).cache()\n",
    "    val storageLevel = MEMORY_ONLY\n",
    "//     val logicalPlan = df.getClass.getDeclaredField(\"logicalPlan\").asInstanceOf[LogicalPlan]\n",
    "    val mirror = runtimeMirror(scala.reflect.runtime.universe.getClass.getClassLoader)\n",
    "      val instanceMirror = mirror.reflect(df)\n",
    "      val field = scala.reflect.runtime.universe.typeOf[DataFrame].decl(TermName(\"logicalPlan\")).asTerm\n",
    "      val fieldMirror = instanceMirror.reflectField(field)\n",
    "      val logicalPlan = fieldMirror.get.asInstanceOf[LogicalPlan]\n",
    "    \n",
    "//     val plan = callPrivate(spark.sessionState,\"executePlan\",logicalPlan).getClass.getDeclaredField(\"sparkPlan\").asInstanceOf[SparkPlan]\n",
    "    val plan = spark.sessionState.executePlan(logicalPlan).sparkPlan\n",
    "    \n",
    "    \n",
    "    val relation = InMemoryRelation(ser, storageLevel, plan, None, logicalPlan)\n",
    "//     val relation = InMemoryRelation\n",
    "    \n",
    "    \n",
    "    relation.cacheBuilder.cachedColumnBuffers\n",
    "    val (doExecuteMethod, inMemoryScans) = func(df)\n",
    "//     CachePerfWriter.appendLine(\"Reading cache 10 times\")\n",
    "    val reads = for (_ <- 0 until 10) yield {\n",
    "      val start = System.currentTimeMillis()\n",
    "      val inMemoryScan = inMemoryScans.head\n",
    "      println(\"---------------------AAAAAA---------\")\n",
    "      println(inMemoryScan.getClass)\n",
    "      val rdd = doExecuteMethod.invoke(inMemoryScan).asInstanceOf[RDD[ColumnarBatch]]\n",
    "      println(\"---------------------BBBBB---------\")\n",
    "      if (ser.isInstanceOf[ParquetCachedBatchSerializer] && acc) {\n",
    "        println(\"---------------------CCCC111-----!!!----\")\n",
    "        rdd.mapPartitions(iter => CloseableColumnBatchIterator(iter)).count()\n",
    "        println(\"---------------------CCCC222---------\")\n",
    "        rdd.foreach {\n",
    "          println(\"---------------------CCCC333---------\")\n",
    "          cb => cb.close()\n",
    "        }\n",
    "        println(\"---------------------CCCC4444---------\")\n",
    "      } else {\n",
    "        println(\"---------------------DDDDD---------\")\n",
    "        rdd.count()\n",
    "      }\n",
    "      println(\"-----------read cache---------\") \n",
    "      System.currentTimeMillis() - start\n",
    "        \n",
    "\n",
    "    }\n",
    "//     CachePerfWriter.appendLine(s\"Time taken for reads: $reads\")\n",
    "    val defaReadTime = reads.slice(1, reads.length).sum / 9\n",
    "    df.unpersist()\n",
    "    (defaWriteTime, defaReadTime, reads(0))\n",
    "//     (defaWriteTime, 1, 1)\n",
    "  }\n",
    "\n",
    "// println(\"aaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e05a7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "\n",
    "// import java.lang.reflect.Method\n",
    "// import org.apache.spark.sql.execution.SparkPlan\n",
    "// // import org.apache.spark.sql.DataFrame\n",
    "// import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "// import org.apache.spark.sql.execution.columnar.InMemoryRelation\n",
    "// // import com.nvidia.spark.ParquetCachedBatchSerializer\n",
    "\n",
    "// def readWriteCache(\n",
    "//       acc: Boolean,\n",
    "//       spark: SparkSession,\n",
    "//       ser: CachedBatchSerializer,\n",
    "//       func: sql.DataFrame => (Method, Seq[SparkPlan]),\n",
    "//       verifyFunc: CachedBatch => Any,\n",
    "//       query: SparkSession => sql.DataFrame): (Long, Long, Long) = {\n",
    "//     CachePerfWriter.appendLine(\"Writing cache 5 times\")\n",
    "//     val writes = for (_ <- 0 until 5) yield {\n",
    "//       val df = query(spark).cache()\n",
    "//       val storageLevel = MEMORY_ONLY\n",
    "//       val plan = spark.sessionState.executePlan(df.logicalPlan).sparkPlan\n",
    "//       val relation = InMemoryRelation(ser, storageLevel, plan, None, df.logicalPlan)\n",
    "//       val start = System.currentTimeMillis()\n",
    "//       val cb = relation.cacheBuilder.cachedColumnBuffers.first()\n",
    "//       val defaWriteTime = System.currentTimeMillis() - start\n",
    "//       verifyFunc(cb)\n",
    "//       df.unpersist(true)\n",
    "//       defaWriteTime\n",
    "//     }\n",
    "//     CachePerfWriter.appendLine(s\"Time taken for writes: $writes\")\n",
    "//     val defaWriteTime = writes.sum / 5\n",
    "\n",
    "//     val df = query(spark).cache()\n",
    "//     val storageLevel = MEMORY_ONLY\n",
    "//     val plan = spark.sessionState.executePlan(df.logicalPlan).sparkPlan\n",
    "//     val relation = InMemoryRelation(ser, storageLevel, plan, None, df.logicalPlan)\n",
    "//     relation.cacheBuilder.cachedColumnBuffers\n",
    "//     val (doExecuteMethod, inMemoryScans) = func(df)\n",
    "//     CachePerfWriter.appendLine(\"Reading cache 10 times\")\n",
    "//     val reads = for (_ <- 0 until 10) yield {\n",
    "//       val start = System.currentTimeMillis()\n",
    "//       val inMemoryScan = inMemoryScans.head\n",
    "//       val rdd = doExecuteMethod.invoke(inMemoryScan).asInstanceOf[RDD[ColumnarBatch]]\n",
    "//       if (ser.isInstanceOf[ParquetCachedBatchSerializer] && acc) {\n",
    "//         rdd.mapPartitions(iter => CloseableColumnBatchIterator(iter)).count()\n",
    "//         rdd.foreach {\n",
    "//           cb => cb.close()\n",
    "//         }\n",
    "//       } else {\n",
    "//         rdd.count()\n",
    "//       }\n",
    "//       System.currentTimeMillis() - start\n",
    "//     }\n",
    "//     CachePerfWriter.appendLine(s\"Time taken for reads: $reads\")\n",
    "//     val defaReadTime = reads.slice(1, reads.length).sum / 9\n",
    "//     df.unpersist()\n",
    "//     (defaWriteTime, defaReadTime, reads(0))\n",
    "//   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8411165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:119: warning: abstract type pattern T is unchecked since it is eliminated by erasure\n",
       "                 case _: T =>\n",
       "                         ^\n",
       "runDefaInternal: [T](query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame, acc: Boolean, ser: org.apache.spark.sql.columnar.CachedBatchSerializer)(Boolean, Long, Long, Long)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
    "\n",
    "\n",
    "def runDefaInternal[T](\n",
    "      query: SparkSession => DataFrame,\n",
    "      acc: Boolean,\n",
    "      ser: CachedBatchSerializer): (Boolean, Long, Long, Long) = {\n",
    "    val spark = createSparkSession()\n",
    "    if (acc) {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    } else {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"false\")\n",
    "    }\n",
    "    println(\"---------------------\"+acc+\"---------\")\n",
    "    val (defaWriteTime, defaReadTime, firstRead) =\n",
    "      readWriteCache(acc, spark, ser, { df =>\n",
    "        val doExecuteMethod =\n",
    "          classOf[InMemoryTableScanExec].getDeclaredMethod(\"doExecute\")\n",
    "        doExecuteMethod.setAccessible(true)\n",
    "        val inMemScans = df.queryExecution.executedPlan.collect {\n",
    "          case m: InMemoryTableScanExec => m\n",
    "        }\n",
    "        (doExecuteMethod, inMemScans)\n",
    "      }, cb =>\n",
    "        cb match {\n",
    "          case _: T =>\n",
    "          case other => throw new IllegalStateException(s\"Unexpected cached batch type: ${other.getClass.getName}\")\n",
    "        }, query )\n",
    "    (acc, defaWriteTime, firstRead, defaReadTime)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eed262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runPcbsInternal: (query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame, acc: Boolean)(Boolean, Long, Long, Long)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
    "// import spark3xx-common.com.nvidia.spark.rapids.shims.ParquetCachedBatch\n",
    "// // import org.apache.spark.sql.rapids._\n",
    "import com.nvidia.spark.GpuCachedBatchSerializer\n",
    "import com.nvidia.spark.rapids.ParquetCachedBatch\n",
    "\n",
    "def runPcbsInternal(query: SparkSession => DataFrame, acc: Boolean): (Boolean, Long, Long, Long) = {\n",
    "    val spark = createSparkSession()\n",
    "    if (acc) {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    } else {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"false\")\n",
    "    }\n",
    "    println(\"----------\"+acc+\"--------------\")\n",
    "    val (pcbsWriteTime, pcbsReadTime, firstRead) =\n",
    "      readWriteCache(acc, spark, new ParquetCachedBatchSerializer(), { df =>\n",
    "        val doExecuteMethod =\n",
    "          classOf[GpuInMemoryTableScanExec].getDeclaredMethod(\"doExecuteColumnar\")\n",
    "        doExecuteMethod.setAccessible(true)\n",
    "        val inMemScans = df.queryExecution.executedPlan.collect {\n",
    "          case g: GpuInMemoryTableScanExec => g\n",
    "          case m: InMemoryTableScanExec => m\n",
    "        }\n",
    "        (doExecuteMethod, inMemScans)\n",
    "      }, cb =>\n",
    "        cb match {\n",
    "          case _: ParquetCachedBatch =>\n",
    "          case other => throw new IllegalStateException(s\"Unexpected cached batch type: ${other.getClass.getName}\")\n",
    "        }, query)\n",
    "\n",
    "    (acc, pcbsWriteTime, firstRead, pcbsReadTime)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a7701b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runPcbs: (query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// import spark312.com.nvidia.spark.rapids.ParquetCachedBatch\n",
    "// import spark312.com.nvidia.spark.ParquetCachedBatchSerializer\n",
    "\n",
    "// def runPcbs(query: SparkSession => DataFrame): Unit = {\n",
    "// //     val (acc, write, firstRead, readAvg) = runPcbsInternal(query, true)\n",
    "//     val pcbs = new ParquetCachedBatchSerializer()\n",
    "//     val (acc0, write0, firstRead0, readAvg0) = runDefaInternal[ParquetCachedBatch](query, false, pcbs)\n",
    "// //     CachePerfWriter.appendLine(\"PCBS\")\n",
    "// //     CachePerfWriter.appendTimes(acc, write, firstRead, readAvg)\n",
    "// //     CachePerfWriter.appendTimes(acc0, write0, firstRead0, readAvg0)\n",
    "//   }\n",
    "def runPcbs(query: SparkSession => DataFrame): Unit = {\n",
    "    println(\"------runPcbsInternal--------\")\n",
    "    val (acc, write, firstRead, readAvg) = runPcbsInternal(query, true)\n",
    "    println(\"------runDefaInternal--------\")\n",
    "    val pcbs = new ParquetCachedBatchSerializer()\n",
    "    val (acc0, write0, firstRead0, readAvg0) = runDefaInternal[ParquetCachedBatch](query, false, pcbs)\n",
    "    println(\"------recording--------\")\n",
    "    CachePerfWriter.appendLine(\"PCBS\")\n",
    "//     CachePerfWriter.appendTimes(acc, write, firstRead, readAvg)\n",
    "    CachePerfWriter.appendTimes(acc0, write0, firstRead0, readAvg0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "410af078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runDefa: (query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def runDefa(query: SparkSession => DataFrame): Unit = {\n",
    "    println(\"------runDEFAInternal-part1-------\")\n",
    "    val ser = new TestCachedBatchSerializer(useCompression = true, 10000)\n",
    "    println(\"------runDEFAInternal-part2-------\")\n",
    "    val (acc, write, firstRead, readAvg) = runDefaInternal[DefaultCachedBatch](query, true, ser)\n",
    "    println(\"------runDEFAInternal-part3-------\")\n",
    "    val (acc0, write0, firstRead0, readAvg0) = runDefaInternal[DefaultCachedBatch](query, false, ser)\n",
    "\n",
    "    CachePerfWriter.appendLine(\"DefaultSerializer\")\n",
    "    CachePerfWriter.appendTimes(acc, write, firstRead, readAvg)\n",
    "    CachePerfWriter.appendTimes(acc0, write0, firstRead0, readAvg0)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7893113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------runPcbsInternal--------\n",
      "----------true--------------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------CCCC111-----!!!----\n",
      "---------------------CCCC222---------\n",
      "---------------------CCCC333---------\n",
      "---------------------CCCC4444---------\n",
      "-----------read cache---------\n",
      "------runDefaInternal--------\n",
      "---------------------false---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------11111---------\n",
      "---------------------2222---------\n",
      "---------------------3333-----!!!----\n",
      "---------------------444---------\n",
      "---------------------555---------\n",
      "-----------write cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n",
      "-----------read cache---------\n",
      "---------------------AAAAAA---------\n",
      "class org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
      "---------------------BBBBB---------\n",
      "---------------------DDDDD---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n",
       "path = file:///home/yuanli/work/pcbs/part-00070-086b31d9-2f77-4f9b-8d82-a526703fd9d8-c000.snappy.parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------read cache---------\n",
      "------recording--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file:///home/yuanli/work/pcbs/part-00070-086b31d9-2f77-4f9b-8d82-a526703fd9d8-c000.snappy.parquet"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"file:///home/yuanli/work/pcbs/part-00070-086b31d9-2f77-4f9b-8d82-a526703fd9d8-c000.snappy.parquet\"\n",
    "try {\n",
    "  CachePerfWriter.appendLine(s\"Reading file: $path\")\n",
    "  runPcbs(spark => spark.read.parquet(path))\n",
    "} finally {\n",
    "//   CachePerfWriter.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca600f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------runDEFAInternal-part1-------\n",
      "------runDEFAInternal-part2-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/26 09:01:48 WARN Utils: Your hostname, yuanli-System-Product-Name resolves to a loopback address: 127.0.1.1; using 10.19.183.210 instead (on interface enp5s0)\n",
      "22/05/26 09:01:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/26 09:01:48 INFO SparkContext: Running Spark version 3.1.2\n",
      "22/05/26 09:01:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/26 09:01:49 INFO ResourceUtils: ==============================================================\n",
      "22/05/26 09:01:49 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/26 09:01:49 INFO ResourceUtils: ==============================================================\n",
      "22/05/26 09:01:49 INFO SparkContext: Submitted application: PCBS Benchmark\n",
      "22/05/26 09:01:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: , gpu -> name: gpu, amount: 1, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0, gpu -> name: gpu, amount: 0.25)\n",
      "22/05/26 09:01:49 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor\n",
      "22/05/26 09:01:49 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/26 09:01:49 INFO SecurityManager: Changing view acls to: yuanli\n",
      "22/05/26 09:01:49 INFO SecurityManager: Changing modify acls to: yuanli\n",
      "22/05/26 09:01:49 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/26 09:01:49 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/26 09:01:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yuanli); groups with view permissions: Set(); users  with modify permissions: Set(yuanli); groups with modify permissions: Set()\n",
      "22/05/26 09:01:49 INFO Utils: Successfully started service 'sparkDriver' on port 37215.\n",
      "22/05/26 09:01:49 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/26 09:01:49 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/26 09:01:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/26 09:01:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/26 09:01:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/26 09:01:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c5bc642f-40e0-4611-b5b3-45e8cc5694c3\n",
      "22/05/26 09:01:49 INFO MemoryStore: MemoryStore started with capacity 8.2 GiB\n",
      "22/05/26 09:01:49 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/26 09:01:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/05/26 09:01:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.19.183.210:4040\n",
      "22/05/26 09:01:49 INFO SparkContext: Added JAR /home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar at spark://10.19.183.210:37215/jars/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar with timestamp 1653555708919\n",
      "22/05/26 09:01:49 INFO ShimLoader: Loading shim for Spark version: 3.1.2\n",
      "22/05/26 09:01:49 INFO ShimLoader: Complete Spark build info: 3.1.2, https://github.com/apache/spark, HEAD, de351e30a90dd988b133b3d00fa6218bfcaba8b8, 2021-05-24T05:24:04Z\n",
      "22/05/26 09:01:49 INFO ShimLoader: Conventional shim jar layout for a single Spark verision detected\n",
      "22/05/26 09:01:49 INFO RapidsPluginUtils: RAPIDS Accelerator build: {version=22.06.0-SNAPSHOT, user=yuanli, url=https://github.com/NVIDIA/spark-rapids.git, date=2022-05-25T06:29:44Z, revision=c588a1f41b43cecb63131dc58a268e8ad79d490d, cudf_version=22.06.0-SNAPSHOT, branch=branch-22.06}\n",
      "22/05/26 09:01:49 INFO RapidsPluginUtils: RAPIDS Accelerator JNI build: {version=22.06.0-SNAPSHOT, user=, url=https://github.com/NVIDIA/spark-rapids-jni.git, date=2022-05-25T02:32:53Z, revision=a3b9e5404212b0f0dd3e339b55c46d507bb014a3, branch=HEAD}\n",
      "22/05/26 09:01:49 INFO RapidsPluginUtils: cudf build: {version=22.06.0-SNAPSHOT, user=, url=https://github.com/rapidsai/cudf.git, date=2022-05-25T02:32:53Z, revision=29f0b5a2c5286750a132f5076028cea1c95b3e03, branch=HEAD}\n",
      "22/05/26 09:01:49 WARN RapidsPluginUtils: RAPIDS Accelerator 22.06.0-SNAPSHOT using cudf 22.06.0-SNAPSHOT.\n",
      "22/05/26 09:01:49 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "22/05/26 09:01:49 INFO DriverPluginContainer: Initialized driver component for plugin com.nvidia.spark.SQLPlugin.\n",
      "22/05/26 09:01:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://yuanli-System-Product-Name:7077...\n",
      "22/05/26 09:01:49 INFO TransportClientFactory: Successfully created connection to yuanli-System-Product-Name/127.0.1.1:7077 after 17 ms (0 ms spent in bootstraps)\n",
      "22/05/26 09:01:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220526170150-0057\n",
      "22/05/26 09:01:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220526170150-0057/0 on worker-20220517173921-10.19.183.210-45229 (10.19.183.210:45229) with 4 core(s)\n",
      "22/05/26 09:01:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20220526170150-0057/0 on hostPort 10.19.183.210:45229 with 4 core(s), 4.0 GiB RAM\n",
      "22/05/26 09:01:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41873.\n",
      "22/05/26 09:01:50 INFO NettyBlockTransferService: Server created on 10.19.183.210:41873\n",
      "22/05/26 09:01:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/26 09:01:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220526170150-0057/0 is now RUNNING\n",
      "22/05/26 09:01:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.19.183.210, 41873, None)\n",
      "22/05/26 09:01:50 INFO BlockManagerMasterEndpoint: Registering block manager 10.19.183.210:41873 with 8.2 GiB RAM, BlockManagerId(driver, 10.19.183.210, 41873, None)\n",
      "22/05/26 09:01:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.19.183.210, 41873, None)\n",
      "22/05/26 09:01:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.19.183.210, 41873, None)\n",
      "22/05/26 09:01:50 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "22/05/26 09:01:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/data/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/spark-warehouse').\n",
      "22/05/26 09:01:50 INFO SharedState: Warehouse path is 'file:/data/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/spark-warehouse'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------true---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 09:01:50 INFO InMemoryFileIndex: It took 17 ms to list leaf files for 1 paths.\n",
      "22/05/26 09:01:51 INFO SparkContext: Starting job: parquet at cmd17.sc:4\n",
      "22/05/26 09:01:51 INFO DAGScheduler: Got job 0 (parquet at cmd17.sc:4) with 1 output partitions\n",
      "22/05/26 09:01:51 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at cmd17.sc:4)\n",
      "22/05/26 09:01:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/05/26 09:01:51 INFO DAGScheduler: Missing parents: List()\n",
      "22/05/26 09:01:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at cmd17.sc:4), which has no missing parents\n",
      "22/05/26 09:01:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 86.0 KiB, free 8.2 GiB)\n",
      "22/05/26 09:01:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 8.2 GiB)\n",
      "22/05/26 09:01:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.19.183.210:41873 (size: 30.5 KiB, free: 8.2 GiB)\n",
      "22/05/26 09:01:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1388\n",
      "22/05/26 09:01:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at cmd17.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/05/26 09:01:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/05/26 09:01:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.19.183.210:58498) with ID 0,  ResourceProfileId 0\n",
      "22/05/26 09:01:56 INFO BlockManagerMasterEndpoint: Registering block manager 10.19.183.210:39429 with 2004.6 MiB RAM, BlockManagerId(0, 10.19.183.210, 39429, None)\n",
      "22/05/26 09:02:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.19.183.210, executor 0, partition 0, PROCESS_LOCAL, 4675 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
      "22/05/26 09:02:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.19.183.210:39429 (size: 30.5 KiB, free: 2004.6 MiB)\n",
      "22/05/26 09:02:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 689 ms on 10.19.183.210 (executor 0) (1/1)\n",
      "22/05/26 09:02:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/05/26 09:02:01 INFO DAGScheduler: ResultStage 0 (parquet at cmd17.sc:4) finished in 10.787 s\n",
      "22/05/26 09:02:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/05/26 09:02:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/05/26 09:02:01 INFO DAGScheduler: Job 0 finished: parquet at cmd17.sc:4, took 10.820428 s\n",
      "22/05/26 09:02:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.19.183.210:41873 in memory (size: 30.5 KiB, free: 8.2 GiB)\n",
      "22/05/26 09:02:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.19.183.210:39429 in memory (size: 30.5 KiB, free: 2004.6 MiB)\n",
      "22/05/26 09:02:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/05/26 09:02:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/05/26 09:02:02 INFO FileSourceStrategy: Output Data Schema: struct<c_customer_sk: int, c_customer_id: string, c_current_cdemo_sk: int, c_current_hdemo_sk: int, c_current_addr_sk: int ... 16 more fields>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------11111---------\n",
      "---------------------2222---!!------\n",
      "---------------------3333---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 09:02:03 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/05/26 09:02:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 310.4 KiB, free 8.2 GiB)\n",
      "22/05/26 09:02:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 8.2 GiB)\n",
      "22/05/26 09:02:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.19.183.210:41873 (size: 28.6 KiB, free: 8.2 GiB)\n",
      "22/05/26 09:02:03 INFO SparkContext: Created broadcast 1 from executeColumnar at GpuColumnarToRowExec.scala:320\n",
      "22/05/26 09:02:03 INFO SparkContext: Starting job: first at cmd12.sc:52\n",
      "22/05/26 09:02:03 INFO DAGScheduler: Got job 1 (first at cmd12.sc:52) with 1 output partitions\n",
      "22/05/26 09:02:03 INFO DAGScheduler: Final stage: ResultStage 1 (first at cmd12.sc:52)\n",
      "22/05/26 09:02:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/05/26 09:02:03 INFO DAGScheduler: Missing parents: List()\n",
      "22/05/26 09:02:03 INFO DAGScheduler: Submitting ResultStage 1 (InMemoryTableScan [c_customer_sk#0, c_customer_id#1, c_current_cdemo_sk#2, c_current_hdemo_sk#3, c_current_addr_sk#4, c_first_shipto_date_sk#5, c_first_sales_date_sk#6, c_salutation#7, c_first_name#8, c_last_name#9, c_preferred_cust_flag#10, c_birth_day#11, c_birth_month#12, c_birth_year#13, c_birth_country#14, c_login#15, c_email_address#16, c_last_review_date#17]\n",
      "   +- InMemoryRelation [c_customer_sk#0, c_customer_id#1, c_current_cdemo_sk#2, c_current_hdemo_sk#3, c_current_addr_sk#4, c_first_shipto_date_sk#5, c_first_sales_date_sk#6, c_salutation#7, c_first_name#8, c_last_name#9, c_preferred_cust_flag#10, c_birth_day#11, c_birth_month#12, c_birth_year#13, c_birth_country#14, c_login#15, c_email_address#16, c_last_review_date#17], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- GpuColumnarToRow false\n",
      "            +- GpuFileGpuScan parquet [c_customer_sk#0,c_customer_id#1,c_current_cdemo_sk#2,c_current_hdemo_sk#3,c_current_addr_sk#4,c_first_shipto_date_sk#5,c_first_sales_date_sk#6,c_salutat... MapPartitionsRDD[11] at cachedColumnBuffers at cmd12.sc:52), which has no missing parents\n",
      "22/05/26 09:02:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.0 KiB, free 8.2 GiB)\n",
      "22/05/26 09:02:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 8.2 GiB)\n",
      "22/05/26 09:02:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.19.183.210:41873 (size: 8.8 KiB, free: 8.2 GiB)\n",
      "22/05/26 09:02:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1388\n",
      "22/05/26 09:02:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (InMemoryTableScan [c_customer_sk#0, c_customer_id#1, c_current_cdemo_sk#2, c_current_hdemo_sk#3, c_current_addr_sk#4, c_first_shipto_date_sk#5, c_first_sales_date_sk#6, c_salutation#7, c_first_name#8, c_last_name#9, c_preferred_cust_flag#10, c_birth_day#11, c_birth_month#12, c_birth_year#13, c_birth_country#14, c_login#15, c_email_address#16, c_last_review_date#17]\n",
      "   +- InMemoryRelation [c_customer_sk#0, c_customer_id#1, c_current_cdemo_sk#2, c_current_hdemo_sk#3, c_current_addr_sk#4, c_first_shipto_date_sk#5, c_first_sales_date_sk#6, c_salutation#7, c_first_name#8, c_last_name#9, c_preferred_cust_flag#10, c_birth_day#11, c_birth_month#12, c_birth_year#13, c_birth_country#14, c_login#15, c_email_address#16, c_last_review_date#17], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- GpuColumnarToRow false\n",
      "            +- GpuFileGpuScan parquet [c_customer_sk#0,c_customer_id#1,c_current_cdemo_sk#2,c_current_hdemo_sk#3,c_current_addr_sk#4,c_first_shipto_date_sk#5,c_first_sales_date_sk#6,c_salutat... MapPartitionsRDD[11] at cachedColumnBuffers at cmd12.sc:52) (first 15 tasks are for partitions Vector(0))\n",
      "22/05/26 09:02:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/05/26 09:02:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.19.183.210, executor 0, partition 0, PROCESS_LOCAL, 5095 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
      "22/05/26 09:02:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.19.183.210:39429 (size: 8.8 KiB, free: 2004.6 MiB)\n",
      "22/05/26 09:02:03 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (10.19.183.210 executor 0): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD\n",
      "\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\n",
      "\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/05/26 09:02:03 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 2) (10.19.183.210, executor 0, partition 0, PROCESS_LOCAL, 5095 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
      "22/05/26 09:02:03 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2) on 10.19.183.210, executor 0: java.lang.ClassCastException (cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 1]\n",
      "22/05/26 09:02:03 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 3) (10.19.183.210, executor 0, partition 0, PROCESS_LOCAL, 5095 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
      "22/05/26 09:02:03 INFO TaskSetManager: Lost task 0.2 in stage 1.0 (TID 3) on 10.19.183.210, executor 0: java.lang.ClassCastException (cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 2]\n",
      "22/05/26 09:02:03 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 4) (10.19.183.210, executor 0, partition 0, PROCESS_LOCAL, 5095 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
      "22/05/26 09:02:03 INFO TaskSetManager: Lost task 0.3 in stage 1.0 (TID 4) on 10.19.183.210, executor 0: java.lang.ClassCastException (cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD) [duplicate 3]\n",
      "22/05/26 09:02:03 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job\n",
      "22/05/26 09:02:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/05/26 09:02:03 INFO TaskSchedulerImpl: Cancelling stage 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 09:02:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled\n",
      "22/05/26 09:02:03 INFO DAGScheduler: ResultStage 1 (first at cmd12.sc:52) failed in 0.181 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4) (10.19.183.210 executor 0): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD\n",
      "\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\n",
      "\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "22/05/26 09:02:03 INFO DAGScheduler: Job 1 failed: first at cmd12.sc:52, took 0.190967 s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4) (10.19.183.210 executor 0): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2258\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2207\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2206\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2206\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1079\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1079\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m274\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1079\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2445\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2387\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2376\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m868\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2196\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2217\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2236\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$take$1(\u001b[32mRDD.scala\u001b[39m:\u001b[32m1449\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)\n  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  org.apache.spark.rdd.RDD.take(\u001b[32mRDD.scala\u001b[39m:\u001b[32m1422\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$first$1(\u001b[32mRDD.scala\u001b[39m:\u001b[32m1463\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)\n  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  org.apache.spark.rdd.RDD.first(\u001b[32mRDD.scala\u001b[39m:\u001b[32m1463\u001b[39m)\n  ammonite.$sess.cmd12$Helper.$anonfun$readWriteCache$1(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m52\u001b[39m)\n  scala.runtime.java8.JFunction1$mcJI$sp.apply(\u001b[32mJFunction1$mcJI$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m237\u001b[39m)\n  scala.collection.immutable.Range.foreach(\u001b[32mRange.scala\u001b[39m:\u001b[32m158\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m237\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m230\u001b[39m)\n  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  ammonite.$sess.cmd12$Helper.readWriteCache(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m20\u001b[39m)\n  ammonite.$sess.cmd13$Helper.runDefaInternal(\u001b[32mcmd13.sc\u001b[39m:\u001b[32m28\u001b[39m)\n  ammonite.$sess.cmd16$Helper.runDefa(\u001b[32mcmd16.sc\u001b[39m:\u001b[32m5\u001b[39m)\n  ammonite.$sess.cmd17$Helper.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m4\u001b[39m)\n  ammonite.$sess.cmd17$.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd17$.<clinit>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD\u001b[39m\n  java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(\u001b[32mObjectStreamClass.java\u001b[39m:\u001b[32m2301\u001b[39m)\n  java.io.ObjectStreamClass.setObjFieldValues(\u001b[32mObjectStreamClass.java\u001b[39m:\u001b[32m1431\u001b[39m)\n  java.io.ObjectInputStream.defaultReadFields(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2411\u001b[39m)\n  java.io.ObjectInputStream.readSerialData(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2329\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2187\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1667\u001b[39m)\n  java.io.ObjectInputStream.defaultReadFields(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2405\u001b[39m)\n  java.io.ObjectInputStream.readSerialData(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2329\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2187\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1667\u001b[39m)\n  java.io.ObjectInputStream.readObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m503\u001b[39m)\n  java.io.ObjectInputStream.readObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m461\u001b[39m)\n  org.apache.spark.serializer.JavaDeserializationStream.readObject(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m76\u001b[39m)\n  org.apache.spark.serializer.JavaSerializerInstance.deserialize(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m115\u001b[39m)\n  org.apache.spark.scheduler.ResultTask.runTask(\u001b[32mResultTask.scala\u001b[39m:\u001b[32m83\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m497\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1439\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m500\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val path = \"/home/yuanli/work/pcbs/part-00070-086b31d9-2f77-4f9b-8d82-a526703fd9d8-c000.snappy.parquet\"\n",
    "try {\n",
    "  CachePerfWriter.appendLine(s\"Reading file: $path\")\n",
    "  runDefa(spark => spark.read.parquet(path))\n",
    "} finally {\n",
    "//   CachePerfWriter.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1980dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/part-00195-fb5fd9cb-2b04-445b-98a4-b72a1eb1ccc8-c000.snappy.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/part-00195-fb5fd9cb-2b04-445b-98a4-b72a1eb1ccc8-c000.snappy.parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/part-00195-fb5fd9cb-2b04-445b-98a4-b72a1eb1ccc8-c000.snappy.parquet"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"/home/yuanli/work/reviews/raza-pcbs-pr159/spark-rapids-examples/examples/pcbs-benchmark/part-00195-fb5fd9cb-2b04-445b-98a4-b72a1eb1ccc8-c000.snappy.parquet\"\n",
    "\n",
    "println(path)\n",
    "//runPcbs(spark => spark.read.parquet(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4b21848",
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:42: error: not found: value CachePerfWriter\n             CachePerfWriter.appendLine(s\"Reading file: $path\")\n             ^\n<console>:43: error: not found: value Util\n             Util.runPcbs(spark => spark.read.parquet(path))\n             ^\n<console>:45: error: not found: value CachePerfWriter\n             CachePerfWriter.close()\n             ^\n<console>:51: error: type mismatch;\n found   : String\n required: Array[String]\n       main(path)\n            ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "\n",
    "def main(args: Array[String]) {\n",
    "    val path = args(0)\n",
    "    try {\n",
    "      CachePerfWriter.appendLine(s\"Reading file: $path\")\n",
    "      Util.runPcbs(spark => spark.read.parquet(path))\n",
    "    } finally {\n",
    "      CachePerfWriter.close()\n",
    "    }\n",
    "}\n",
    "\n",
    "main(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286bba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcbs - Scala",
   "language": "scala",
   "name": "pcbs_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}