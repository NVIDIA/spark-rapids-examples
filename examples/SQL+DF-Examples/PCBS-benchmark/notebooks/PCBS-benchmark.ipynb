{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34310860",
   "metadata": {},
   "source": [
    "## PCBS benchmark\n",
    "The benchmark on [RAPIDS Accelerator For Apache Spark](https://nvidia.github.io/spark-rapids/) is to time the\n",
    "conversion of incoming RDDs to and from a CachedBatch. Specifically to compare the performance of\n",
    "ParquetCachedBatchSerializer to DefaultCachedBatchSerializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85cd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.lang.reflect.Method\n",
    "\n",
    "import org.apache.spark.{sql, TaskContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.catalyst.InternalRow\n",
    "import org.apache.spark.sql.catalyst.expressions.Attribute\n",
    "import org.apache.spark.sql.columnar.{CachedBatch, CachedBatchSerializer}\n",
    "import org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\n",
    "import org.apache.spark.sql.execution.SparkPlan\n",
    "import org.apache.spark.sql.internal.SQLConf\n",
    "import org.apache.spark.sql.vectorized.ColumnarBatch\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.storage.StorageLevel.MEMORY_ONLY\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.execution.columnar.DefaultCachedBatch\n",
    "import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n",
    "import org.apache.spark.sql.rapids.GpuInMemoryTableScanExec\n",
    "import com.nvidia.spark.GpuCachedBatchSerializer\n",
    "import com.nvidia.spark.rapids.ParquetCachedBatch\n",
    "import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "import org.apache.spark.sql.execution.columnar.InMemoryRelation\n",
    "import com.nvidia.spark.ParquetCachedBatchSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261bce43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAPIDS_JAR = /home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createSparkSession: ()org.apache.spark.sql.SparkSession\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/yuanli/work/pcbs/re-build/spark-rapids/dist/target/rapids-4-spark_2.12-22.06.0-SNAPSHOT-cuda11.jar"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RAPIDS_JAR = \"rapids-plugin-jar-file\"\n",
    "\n",
    "def createSparkSession(): SparkSession = {\n",
    "    val conf = new SparkConf()\n",
    "    conf.setMaster(\"your-spark-master-url\")\n",
    "    conf.setAppName(\"PCBS Benchmark\")\n",
    "    conf.set(\"spark.jars\", RAPIDS_JAR)\n",
    "    conf.set(\"spark.driver.memory\", \"4G\")\n",
    "    conf.set(\"spark.executor.memory\", \"4G\")\n",
    "    conf.set(\"spark.executor.cores\", \"4\")\n",
    "    conf.set(\"spark.locality.wait\", \"0\")\n",
    "    conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "\n",
    "    //rapids config\n",
    "    conf.set(\"spark.rapids.sql.enabled\", \"true\") \n",
    "    conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.25\") \n",
    "    conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "    conf.set(\"spark.rapids.memory.pinnedPool.size\", \"1G\")\n",
    "    \n",
    "    conf.set(\"spark.driver.extraJavaOptions\",\"-ea -Duser.timezone=UTC\")\n",
    "    conf.set(\"spark.executor.extraJavaOptions\",\"-ea -Duser.timezone=UTC -Dai.rapids.cudf.prefer-pinned=true\")\n",
    "    conf.set(\"spark.sql.cache.serializer\",\"com.nvidia.spark.ParquetCachedBatchSerializer\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", RAPIDS_JAR)\n",
    "    conf.set(\"spark.executor.extraClassPath\", RAPIDS_JAR)\n",
    "\n",
    "    SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97336659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class TestCachedBatchSerializer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TestCachedBatchSerializer(\n",
    "    useCompression: Boolean,\n",
    "    batchSize: Int) extends DefaultCachedBatchSerializer {\n",
    "\n",
    "  override def convertInternalRowToCachedBatch(input: RDD[InternalRow],\n",
    "      schema: Seq[Attribute],\n",
    "      storageLevel: StorageLevel,\n",
    "      conf: SQLConf): RDD[CachedBatch] = {\n",
    "    convertForCacheInternal(input, schema, batchSize, useCompression)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809146ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class CloseableColumnBatchIterator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class CloseableColumnBatchIterator(iter: Iterator[ColumnarBatch]) extends\n",
    "    Iterator[ColumnarBatch] {\n",
    "  var cb: ColumnarBatch = _\n",
    "\n",
    "  private def closeCurrentBatch(): Unit = {\n",
    "    if (cb != null) {\n",
    "      cb.close()\n",
    "      cb = null\n",
    "    }\n",
    "  }\n",
    "\n",
    "  TaskContext.get().addTaskCompletionListener[Unit]((_: TaskContext) => {\n",
    "    closeCurrentBatch()\n",
    "  })\n",
    "\n",
    "  override def hasNext: Boolean = iter.hasNext\n",
    "\n",
    "  override def next(): ColumnarBatch = {\n",
    "    closeCurrentBatch()\n",
    "    cb = iter.next()\n",
    "    cb\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e715ccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "callPrivate: (obj: AnyRef, methodName: String, parameters: AnyRef*)Object\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callPrivate(obj: AnyRef, methodName: String, parameters:AnyRef*) = {\n",
    "  val parameterTypes = parameters.map(_.getClass())\n",
    "  val method = obj.getClass.getDeclaredMethod(methodName, parameterTypes:_*)\n",
    "  method.setAccessible(true)\n",
    "  method.invoke(obj, parameters:_*)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef13bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getReflectLogicalPlan: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.reflect.runtime.universe._\n",
    "def getReflectLogicalPlan(df: DataFrame) : LogicalPlan = {\n",
    "    val mirror = runtimeMirror(scala.reflect.runtime.universe.getClass.getClassLoader)\n",
    "    val instanceMirror = mirror.reflect(df)\n",
    "    val field = scala.reflect.runtime.universe.typeOf[DataFrame].decl(TermName(\"logicalPlan\")).asTerm\n",
    "    val fieldMirror = instanceMirror.reflectField(field)\n",
    "    fieldMirror.get.asInstanceOf[LogicalPlan]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea80ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readWriteCache: (acc: Boolean, spark: org.apache.spark.sql.SparkSession, ser: org.apache.spark.sql.columnar.CachedBatchSerializer, func: org.apache.spark.sql.DataFrame => (java.lang.reflect.Method, Seq[org.apache.spark.sql.execution.SparkPlan]), verifyFunc: org.apache.spark.sql.columnar.CachedBatch => Any, query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def readWriteCache(\n",
    "      acc: Boolean,\n",
    "      spark: SparkSession,\n",
    "      ser: CachedBatchSerializer,\n",
    "      func: DataFrame => (Method, Seq[SparkPlan]),\n",
    "      verifyFunc: CachedBatch => Any,\n",
    "      query: SparkSession => sql.DataFrame) = {\n",
    "    \n",
    "      val df = query(spark).cache()\n",
    "      val storageLevel = MEMORY_ONLY\n",
    "      val logicalPlan = getReflectLogicalPlan(df)\n",
    "      val plan = spark.sessionState.executePlan(logicalPlan).sparkPlan\n",
    "      val relation = InMemoryRelation(ser, storageLevel, plan, None, logicalPlan)  \n",
    "      val (doExecuteMethod, inMemoryScans) = func(df)\n",
    "        \n",
    "      val start = System.currentTimeMillis()\n",
    "      val cb = relation.cacheBuilder.cachedColumnBuffers.first()\n",
    "      val defaWriteTime = System.currentTimeMillis() - start\n",
    "      verifyFunc(cb)\n",
    "      df.unpersist(true)\n",
    "        \n",
    "      println(s\"write cache with ${doExecuteMethod}, cost ${defaWriteTime} milliseconds.\")  \n",
    "    \n",
    "      relation.cacheBuilder.cachedColumnBuffers\n",
    "    \n",
    "      val startR = System.currentTimeMillis()\n",
    "      val inMemoryScan = inMemoryScans.head\n",
    "      val rdd = doExecuteMethod.invoke(inMemoryScan).asInstanceOf[RDD[ColumnarBatch]]\n",
    "      if (ser.isInstanceOf[ParquetCachedBatchSerializer] && acc) {\n",
    "        rdd.mapPartitions(iter => CloseableColumnBatchIterator(iter)).count()\n",
    "        rdd.foreach {\n",
    "          cb => cb.close()\n",
    "        }\n",
    "      } else {\n",
    "        rdd.count()\n",
    "      }\n",
    "      \n",
    "      val defaReadTime = System.currentTimeMillis() - startR\n",
    "      println(s\"read cache with ${doExecuteMethod}, cost ${defaReadTime} milliseconds.\")\n",
    "\n",
    "      df.unpersist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8411165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:81: warning: abstract type pattern T is unchecked since it is eliminated by erasure\n",
       "             case _: T =>\n",
       "                     ^\n",
       "runDefaInternal: [T](query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame, acc: Boolean, ser: org.apache.spark.sql.columnar.CachedBatchSerializer)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def runDefaInternal[T](\n",
    "      query: SparkSession => DataFrame,\n",
    "      acc: Boolean,\n",
    "      ser: CachedBatchSerializer) = {\n",
    "    val spark = createSparkSession()\n",
    "    if (acc) {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    } else {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"false\")\n",
    "    }\n",
    "\n",
    "    println(s\"spark.rapids.sql.enabled is ${acc} .\") \n",
    "    readWriteCache(acc, spark, ser, { df =>\n",
    "        val doExecuteMethod =\n",
    "          classOf[InMemoryTableScanExec].getDeclaredMethod(\"doExecute\")\n",
    "        doExecuteMethod.setAccessible(true)\n",
    "        val inMemScans = df.queryExecution.executedPlan.collect {\n",
    "          case m: InMemoryTableScanExec => m\n",
    "        }\n",
    "        (doExecuteMethod, inMemScans)\n",
    "        }, cb =>\n",
    "        cb match {\n",
    "          case _: T =>\n",
    "          case other => throw new IllegalStateException(s\"Unexpected cached batch type: ${other.getClass.getName}\")\n",
    "        }, query )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eed262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runPcbsInternal: (query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame, acc: Boolean)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def runPcbsInternal(query: SparkSession => DataFrame, acc: Boolean) = {\n",
    "  val spark = createSparkSession()\n",
    "  if (acc) {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"true\")\n",
    "  } else {\n",
    "      spark.conf.set(\"spark.rapids.sql.enabled\", \"false\")\n",
    "  }\n",
    "  println(s\"spark.rapids.sql.enabled is ${acc} .\") \n",
    "\n",
    "  readWriteCache(acc, spark, new ParquetCachedBatchSerializer(), { df =>\n",
    "    val doExecuteMethod =\n",
    "      classOf[GpuInMemoryTableScanExec].getDeclaredMethod(\"doExecuteColumnar\")\n",
    "    doExecuteMethod.setAccessible(true)\n",
    "    val inMemScans = df.queryExecution.executedPlan.collect {\n",
    "      case g: GpuInMemoryTableScanExec => g\n",
    "      case m: InMemoryTableScanExec => m\n",
    "    }\n",
    "    (doExecuteMethod, inMemScans)\n",
    "  }, cb =>\n",
    "   cb match {\n",
    "      case _: ParquetCachedBatch =>\n",
    "      case other => throw new IllegalStateException(s\"Unexpected cached batch type: ${other.getClass.getName}\")\n",
    "   }, query)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7701b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runPcbs: (query: org.apache.spark.sql.SparkSession => org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def runPcbs(query: SparkSession => DataFrame): Unit = {\n",
    "    println(\"----------write and read cache with Rapids PBCS----------\")\n",
    "    runPcbsInternal(query, true)\n",
    "    println(\"----------write and read cache with Spark Default Cache----------\")\n",
    "    val pcbs = new ParquetCachedBatchSerializer()\n",
    "    runDefaInternal[ParquetCachedBatch](query, false, pcbs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7893113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------write and read cache with Rapids PBCS----------\n",
      "spark.rapids.sql.enabled is true .\n",
      "write cache with public org.apache.spark.rdd.RDD org.apache.spark.sql.rapids.GpuInMemoryTableScanExec.doExecuteColumnar(), cost 671 milliseconds.\n",
      "read cache with public org.apache.spark.rdd.RDD org.apache.spark.sql.rapids.GpuInMemoryTableScanExec.doExecuteColumnar(), cost 269 milliseconds.\n",
      "----------write and read cache with Spark Default Cache----------\n",
      "spark.rapids.sql.enabled is false .\n",
      "write cache with public org.apache.spark.rdd.RDD org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(), cost 1487 milliseconds.\n",
      "read cache with public org.apache.spark.rdd.RDD org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(), cost 990 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "runPcbs(spark => spark.read.parquet(\"your-parquet-file\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcbs - Scala",
   "language": "scala",
   "name": "pcbs_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
