{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import broadcast, SparkConf\n",
    "if \"sc\" in globals():\n",
    "    sc.stop()\n",
    "\n",
    "### Configure the parameters based on your dataproc cluster ###\n",
    "conf = SparkConf().setAppName(\"data gen\")\n",
    "conf.set(\"spark.executor.instances\", \"12\")\n",
    "conf.set(\"spark.executor.cores\", \"5\")\n",
    "conf.set(\"spark.driver.memory\", \"16g\")\n",
    "conf.set(\"spark.executor.memory\", \"20g\")\n",
    "conf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\n",
    "conf.set(\"spark.executor.memoryOverhead\", \"5G\")\n",
    "conf.set(\"spark.sql.broadcastTimeout\", \"700\")\n",
    "conf.set('spark.rapids.sql.enabled', 'false')\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(conf=conf) \\\n",
    "                    .getOrCreate()\n",
    "",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# You need to update these to your real paths!\n",
    "dataRoot = os.getenv(\"DATA_ROOT\", 'gs://path/to/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sales data\n",
    "sales_data = []\n",
    "for i in range(10000000):\n",
    "    sales_id = \"s_{}\".format(i)\n",
    "    product_name = \"Product_{}\".format(random.randint(1,100))\n",
    "    price = random.uniform(1,100)\n",
    "    quantity_sold = random.randint(1,100)\n",
    "    date_of_sale = \"2022-{}-{}\".format(random.randint(1,12), random.randint(1,28))\n",
    "    customer_id = \"c_{}\".format(random.randint(1,1000))\n",
    "    sales_data.append((sales_id,product_name, price, quantity_sold, date_of_sale,customer_id))\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"sales_id\",\"product_name\", \"price\", \"quantity_sold\", \"date_of_sale\",\"customer_id\"])\n",
    "sales_df.write.format(\"csv\").save(dataRoot+\"/sales/\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate stock data\n",
    "stock_data = []\n",
    "for i in range(10000):\n",
    "    product_name = \"Product_{}\".format(i)\n",
    "    shelf_life = random.randint(1,365)\n",
    "    contains_promotion = \"{} % off\".format(random.randint(0,10))\n",
    "    quantity_in_stock = random.randint(1,1000)\n",
    "    location = \"Location_{}\".format(random.randint(1,100))\n",
    "    date_received = \"2022-{}-{}\".format(random.randint(1,12), random.randint(1,28))\n",
    "    stock_data.append((product_name,shelf_life,contains_promotion,quantity_in_stock, location, date_received))\n",
    "\n",
    "stock_df = spark.createDataFrame(stock_data, [\"product_name\",\"shelf_life\",\"contains_promotion\",\"quantity_in_stock\", \"location\", \"date_received\"])\n",
    "# write data to different formats\n",
    "stock_df.repartition(20)\n",
    "stock_df.write.format(\"json\").save(dataRoot+\"/stock/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate supplier data\n",
    "supplier_data = []\n",
    "for i in range(100000):\n",
    "    sup_id = \"s_{}\".format(i)\n",
    "    product_name = \"Product_{}\".format(random.randint(1,10000))\n",
    "    quantity_ordered = random.randint(1,1000)\n",
    "    price = random.uniform(1,100)\n",
    "    date_ordered = \"2022-{}-{}\".format(random.randint(1,12), random.randint(1,28))\n",
    "    supplier_data.append((sup_id,product_name, quantity_ordered, price, date_ordered))\n",
    "\n",
    "supplier_df = spark.createDataFrame(supplier_data, [\"sup_id\",\"product_name\", \"quantity_ordered\", \"price\", \"date_ordered\"])\n",
    "# shuffle the dataframe to ensure it is evenly distributed\n",
    "supplier_df = supplier_df.sort(rand())\n",
    "# divide the dataframe into smaller partitions\n",
    "supplier_df = supplier_df.repartition(100)\n",
    "supplier_df.write.format(\"json\").save(dataRoot+\"/supplier/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate customer data\n",
    "customer_data = []\n",
    "for i in range(100000):\n",
    "    customer_id = \"c_{}\".format(i)\n",
    "    customer_name = \"Customer_{}\".format(i)\n",
    "    age = random.randint(20,70)\n",
    "    gender = random.choice([\"male\", \"female\"])\n",
    "    purchase_history = random.randint(1,100)\n",
    "    contact_info = \"email_{}@gmail.com\".format(i)\n",
    "    customer_data.append((customer_id,customer_name, age, gender, purchase_history, contact_info))\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data, [\"customer_id\",\"customer_name\", \"age\", \"gender\", \"purchase_history\", \"contact_info\"])\n",
    "\n",
    "# shuffle the dataframe to ensure it is evenly distributed\n",
    "customer_df = customer_df.sort(rand())\n",
    "# divide the dataframe into smaller partitions\n",
    "customer_df = customer_df.repartition(100)\n",
    "\n",
    "customer_df.write.format(\"csv\").save(dataRoot+\"/customer/\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate market data\n",
    "market_data = []\n",
    "for i in range(10000):\n",
    "    product_name = \"Product_{}\".format(i)\n",
    "    competitor_price = random.uniform(1,100)\n",
    "    sales_trend = random.randint(1,100)\n",
    "    demand_forecast = random.randint(1,100)\n",
    "    market_data.append((product_name, competitor_price, sales_trend, demand_forecast))\n",
    "\n",
    "market_df = spark.createDataFrame(market_data, [\"product_name\", \"competitor_price\", \"sales_trend\", \"demand_forecast\"])\n",
    "\n",
    "# shuffle the dataframe to ensure it is evenly distributed\n",
    "market_df = market_df.sort(rand())\n",
    "# divide the dataframe into smaller partitions\n",
    "market_df = market_df.repartition(100)\n",
    "market_df.write.format(\"csv\").save(dataRoot+\"/market/\",header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate logistic data\n",
    "logistic_data = []\n",
    "for i in range(10000):\n",
    "    product_name = \"Product_{}\".format(i)\n",
    "    shipping_cost = random.uniform(1,100)\n",
    "    transportation_cost = random.uniform(1,100)\n",
    "    warehouse_cost = random.uniform(1,100)\n",
    "    logistic_data.append((product_name, shipping_cost, transportation_cost, warehouse_cost))\n",
    "\n",
    "logistic_df = spark.createDataFrame(logistic_data, [\"product_name\", \"shipping_cost\", \"transportation_cost\", \"warehouse_cost\"])\n",
    "# shuffle the dataframe to ensure it is evenly distributed\n",
    "logistic_df = logistic_df.sort(rand())\n",
    "# divide the dataframe into smaller partitions\n",
    "logistic_df = logistic_df.repartition(100)\n",
    "logistic_df.write.format(\"csv\").save(dataRoot+\"/logistic/\",header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
