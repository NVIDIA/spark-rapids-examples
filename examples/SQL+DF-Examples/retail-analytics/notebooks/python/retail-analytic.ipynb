{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f4c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 17:40:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 17:40:41 WARN RapidsPluginUtils: RAPIDS Accelerator 24.10.0 using cudf 24.10.0, private revision bd4e99e18e20234ee0c54f95f4b0bfce18a6255e\n",
      "24/11/01 17:40:41 WARN RapidsPluginUtils: spark.rapids.sql.multiThreadedRead.numThreads is set to 20.\n",
      "24/11/01 17:40:41 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "24/11/01 17:40:41 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `NOT_ON_GPU`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n",
      "24/11/01 17:40:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import broadcast, SparkConf\n",
    "import time\n",
    "import os\n",
    "\n",
    "RAPIDS_JAR = os.getenv(\"RAPIDS_JAR\", \"/home/yuanli/work/jars/rapids.jar\")\n",
    "SPARK_MASTER = os.getenv(\"SPARK_MASTER_URL\", \"spark://ip:port\")\n",
    "print(\"RAPIDS_JAR: {}\".format(RAPIDS_JAR))\n",
    "if \"sc\" in globals():\n",
    "    sc.stop()\n",
    "\n",
    "### Configure the parameters based on your dataproc cluster ###\n",
    "conf = SparkConf().setAppName(\"Retail Analytics\")\n",
    "conf.setMaster(SPARK_MASTER)\n",
    "conf.set(\"spark.driver.extraClassPath\", RAPIDS_JAR)\n",
    "conf.set(\"spark.executor.extraClassPath\", RAPIDS_JAR)\n",
    "conf.set(\"spark.jars\", RAPIDS_JAR)\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "conf.set(\"spark.executor.cores\", \"4\")\n",
    "conf.set(\"spark.task.resource.gpu.amount\", \"0.25\")\n",
    "conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "conf.set(\"spark.rapids.memory.pinnedPool.size\", \"2048m\")\n",
    "conf.set(\"spark.executor.memoryOverhead\", \"4096m\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "conf.set(\"spark.rapids.sql.format.json.read.enabled\",True)\n",
    "conf.set(\"spark.rapids.sql.castStringToTimestamp.enabled\",True)\n",
    "conf.set(\"spark.rapids.sql.expression.PercentRank\",False)\n",
    "conf.set(\"spark.rapids.sql.castDecimalToString.enabled\",True)\n",
    "conf.set(\"spark.rapids.sql.hasExtendedYearValues\",False)\n",
    "conf.set(\"spark.rapids.sql.enabled\",True)\n",
    "conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "conf.set(\"spark.rapids.sql.allowMultipleJars\", \"ALWAYS\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(conf=conf) \\\n",
    "                    .getOrCreate()\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"RetailInvMgmt\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973db943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# You need to update these to your real paths!\n",
    "dataRoot = os.getenv(\"DATA_ROOT\", 'file:/home/yuanli/work/example-repo-tests/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba4fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 17:40:44 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 17:40:51 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <Invoke> value#0.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "    @Expression <AttributeReference> value#0 could run on GPU\n",
      "  !Expression <AttributeReference> obj#15 cannot run on GPU because expression AttributeReference obj#15 produces an unsupported type ObjectType(class java.lang.String)\n",
      "  !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:51 WARN GpuOverrides: \n",
      "!Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:51 WARN GpuOverrides: \n",
      "!Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:51 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:51 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <Invoke> value#67.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "    @Expression <AttributeReference> value#67 could run on GPU\n",
      "  !Expression <AttributeReference> obj#82 cannot run on GPU because expression AttributeReference obj#82 produces an unsupported type ObjectType(class java.lang.String)\n",
      "  !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:51 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:51 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <Invoke> value#96.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "    @Expression <AttributeReference> value#96 could run on GPU\n",
      "  !Expression <AttributeReference> obj#111 cannot run on GPU because expression AttributeReference obj#111 produces an unsupported type ObjectType(class java.lang.String)\n",
      "  !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <Invoke> value#121.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "    @Expression <AttributeReference> value#121 could run on GPU\n",
      "  !Expression <AttributeReference> obj#136 cannot run on GPU because expression AttributeReference obj#136 produces an unsupported type ObjectType(class java.lang.String)\n",
      "  !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "                                  !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "                              !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "                                  !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "                              !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "                                  !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "                              !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "                                  !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "                              !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "      !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "\n",
      "24/11/01 17:40:52 WARN GpuOverrides: \n",
      "      !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken on GPU for Data Cleaning:  13.701324701309204\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def clean_data(df):\n",
    "    # remove missing values\n",
    "    df = df.dropna()\n",
    "    # remove duplicate data\n",
    "    df = df.dropDuplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data(spark, format, file_path):\n",
    "    if format==\"csv\":\n",
    "        return spark.read.format(format).load(file_path,header=True)\n",
    "    else:\n",
    "        return spark.read.format(format).load(file_path)\n",
    "\n",
    "# read sales data\n",
    "sales_df = read_data(spark, \"csv\", dataRoot+\"/sales/\")\n",
    "\n",
    "# read stock data\n",
    "stock_df = read_data(spark, \"json\", dataRoot+\"/stock/\")\n",
    "\n",
    "# read supplier data\n",
    "supplier_df = read_data(spark, \"json\", dataRoot+\"/supplier/\")\n",
    "\n",
    "# read customer data\n",
    "customer_df = read_data(spark, \"csv\", dataRoot+\"/customer/\")\n",
    "\n",
    "# read market data\n",
    "market_df = read_data(spark, \"csv\", dataRoot+\"/market/\")\n",
    "\n",
    "# read logistic data\n",
    "logistic_df = read_data(spark, \"csv\", dataRoot+\"/logistic/\")\n",
    "\n",
    "\n",
    "# data cleaning\n",
    "sales_df = clean_data(sales_df)\n",
    "stock_df = clean_data(stock_df)\n",
    "supplier_df = clean_data(supplier_df)\n",
    "customer_df = clean_data(customer_df)\n",
    "market_df = clean_data(market_df)\n",
    "logistic_df = clean_data(logistic_df)\n",
    "\n",
    "\n",
    "# convert date columns to date type\n",
    "sales_df = sales_df.withColumn(\"date_of_sale\", to_date(col(\"date_of_sale\")))\n",
    "stock_df = stock_df.withColumn(\"date_received\", to_date(col(\"date_received\")))\n",
    "supplier_df = supplier_df.withColumn(\"date_ordered\", to_date(col(\"date_ordered\")))\n",
    "\n",
    "# standardize case of string columns\n",
    "sales_df = sales_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"location\", upper(col(\"location\")))\n",
    "supplier_df = supplier_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "customer_df = customer_df.withColumn(\"customer_name\", upper(col(\"customer_name\")))\n",
    "market_df = market_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "logistic_df = logistic_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "\n",
    "# remove leading and trailing whitespaces\n",
    "sales_df = sales_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"location\", trim(col(\"location\")))\n",
    "\n",
    "supplier_df = supplier_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "customer_df = customer_df.withColumn(\"customer_name\", trim(col(\"customer_name\")))\n",
    "market_df = market_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "logistic_df = logistic_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "\n",
    "# check for invalid values\n",
    "sales_df = sales_df.filter(col(\"product_name\").isNotNull())\n",
    "stock_df = stock_df.filter(col(\"location\").isNotNull())\n",
    "customer_df = customer_df.filter(col(\"gender\").isin(\"male\",\"female\"))\n",
    "market_df = market_df.filter(col(\"product_name\").isNotNull())\n",
    "logistic_df = logistic_df.filter(col(\"product_name\").isNotNull())\n",
    "\n",
    "#drop extra columns\n",
    "market_df = market_df.drop(\"price\")\n",
    "supplier_df = supplier_df.drop(\"price\")\n",
    "\n",
    "# join all data\n",
    "data_int = sales_df.join(stock_df, \"product_name\",\"leftouter\").join(supplier_df, \"product_name\",\"leftouter\").join(market_df, \"product_name\",\"leftouter\").join(logistic_df, \"product_name\",\"leftouter\").join(customer_df, \"customer_id\",\"leftouter\")  \n",
    "\n",
    "# write the cleaned data\n",
    "os.makedirs(dataRoot+\"cleaned/\", exist_ok=True)\n",
    "data_int.write.mode(\"overwrite\").format(\"parquet\").save(dataRoot+\"/cleaned/\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken on GPU for Data Cleaning: \", end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c30bfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 17:40:56 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:56 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:56 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "+-------+------------------+\n",
      "| sup_id|        avg(price)|\n",
      "+-------+------------------+\n",
      "|s_22736|4.8419754770028565|\n",
      "|s_36893| 6.789329988514952|\n",
      "|s_26966| 4.991058806855542|\n",
      "|s_25691| 5.345836603483571|\n",
      "| s_8060| 6.274069150056099|\n",
      "| s_1008| 5.938679139361477|\n",
      "|s_23617| 5.336400703491437|\n",
      "|s_16430| 5.392223271239198|\n",
      "| s_6017|5.4330533558043825|\n",
      "|s_38091|4.8774972845432405|\n",
      "|s_34696|1.8911774306887992|\n",
      "|s_38346| 9.368280340566102|\n",
      "| s_2962| 4.605086218567397|\n",
      "|s_12482| 6.262010806447502|\n",
      "|s_38155| 7.304620100874118|\n",
      "|s_24181|3.7258667274635098|\n",
      "|s_41753| 1.468567949456597|\n",
      "|s_22938| 4.092865528899452|\n",
      "|s_22259|1.0563332603599025|\n",
      "| s_1258|1.6635887270568768|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "+-------+------------------+----------------------+\n",
      "| sup_id|        sum(price)|sum(quantity_in_stock)|\n",
      "+-------+------------------+----------------------+\n",
      "|s_22736|4.8419754770028565|                     3|\n",
      "|s_36893| 6.789329988514952|                    14|\n",
      "|s_26966| 4.991058806855542|                    83|\n",
      "|s_25691| 5.345836603483571|                    54|\n",
      "| s_8060| 6.274069150056099|                    12|\n",
      "| s_1008| 5.938679139361477|                    88|\n",
      "|s_23617| 5.336400703491437|                    17|\n",
      "|s_16430| 5.392223271239198|                     5|\n",
      "| s_6017|5.4330533558043825|                    86|\n",
      "|s_38091|4.8774972845432405|                    46|\n",
      "|s_34696|1.8911774306887992|                     2|\n",
      "|s_38346| 9.368280340566102|                    31|\n",
      "| s_2962| 4.605086218567397|                    25|\n",
      "|s_12482| 6.262010806447502|                    61|\n",
      "|s_38155| 7.304620100874118|                    98|\n",
      "|s_24181|3.7258667274635098|                    36|\n",
      "|s_41753| 1.468567949456597|                     3|\n",
      "|s_22938| 4.092865528899452|                    73|\n",
      "|s_22259|1.0563332603599025|                    86|\n",
      "| s_1258|1.6635887270568768|                    11|\n",
      "+-------+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "+----------+-----------------+\n",
      "|perishable|count(perishable)|\n",
      "+----------+-----------------+\n",
      "|        no|           995739|\n",
      "|       yes|             4261|\n",
      "+----------+-----------------+\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "24/11/01 17:40:57 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "\n",
      "+------------+-------------------+\n",
      "|sales_status|count(sales_status)|\n",
      "+------------+-------------------+\n",
      "|         bad|            1000000|\n",
      "+------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Time taken on GPU for Data Analysis:  3.733241081237793\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#DO VARIOUS RETAIL DATA ANALYTICS \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# read cleaned data\n",
    "\n",
    "data = spark.read.format(\"parquet\").load(dataRoot+\"/cleaned/\")\n",
    "\n",
    "#Case when statement to create a new column to indicate whether the product is perishable or not:\n",
    "\n",
    "data = data.withColumn(\"perishable\", when(col(\"shelf_life\") <= 30, \"yes\").otherwise(\"no\"))\n",
    "\n",
    "# You can use the when() and otherwise() functions to create new columns based on certain conditions:\n",
    "\n",
    "data = data.withColumn(\"sales_status\", when(col(\"quantity_sold\") > 50, \"good\").otherwise(\"bad\"))\n",
    "\n",
    "# create a window to perform time series analysis\n",
    "window = Window.partitionBy(\"product_name\").orderBy(\"date_of_sale\")\n",
    "\n",
    "# calculate the rolling average of sales for each product\n",
    "time_series_df = data.withColumn(\"rolling_avg_sales\", avg(\"quantity_sold\").over(window))\n",
    "\n",
    "# use window function for forecasting\n",
    "\n",
    "forecast_df = time_series_df.withColumn(\"prev_sales\", lag(\"rolling_avg_sales\").over(window))\\\n",
    "    .withColumn(\"next_sales\", lead(\"rolling_avg_sales\").over(window))\n",
    "\n",
    "\n",
    "# Calculate the average price of a product, grouped by supplier\n",
    "forecast_df.groupBy(\"sup_id\").agg({\"price\": \"avg\"}).show()\n",
    "\n",
    "\n",
    "# Calculate the total quantity in stock and total sales by supplier\n",
    "forecast_df.groupBy(\"sup_id\").agg({\"quantity_in_stock\": \"sum\", \"price\": \"sum\"}).show()\n",
    "\n",
    "#Calculate the number of perishable v/s non-perishable product per location\n",
    "forecast_df.groupBy(\"perishable\").agg({\"perishable\": \"count\"}).show()\n",
    "\n",
    "\n",
    "#Calculate number of good v/s bad sales status per location\n",
    "forecast_df.groupBy(\"sales_status\").agg({\"sales_status\": \"count\"}).show()\n",
    "\n",
    "# Count the number of sales that contain a 10% off promotion\n",
    "countt = forecast_df.filter(forecast_df[\"contains_promotion\"].contains(\"10% off\")).count()\n",
    "print(countt)\n",
    "# Perform some complex analysis on the DataFrame\n",
    "\n",
    "# Calculate the total sales, quantity sold by product and location\n",
    "total_sales_by_product_location = forecast_df.groupBy(\"product_name\", \"location\").agg(sum(\"price\").alias(\"total_price\"),sum(\"quantity_ordered\").alias(\"total_quantity_sold\"),avg(\"quantity_sold\").alias(\"avg_quantity_sold\")).sort(desc(\"total_price\"))\n",
    "\n",
    "# Group the data by product_name\n",
    "grouped_df = forecast_df.groupBy(\"product_name\")\n",
    "\n",
    "#Sum the quantity_in_stock, quantity_ordered, quantity_sold, and (price * quantity_sold) for each group\n",
    "aggregated_df = grouped_df.agg(sum(\"quantity_in_stock\").alias(\"total_quantity_in_stock\"),avg(\"price\").alias(\"average_price\"),sum(\"quantity_ordered\").alias(\"total_quantity_ordered\"),sum(\"quantity_sold\").alias(\"total_quantity_sold\"),sum(col(\"price\") * col(\"quantity_sold\")).alias(\"total_sales\"),sum(\"prev_sales\").alias(\"total_prev_sales\"),sum(\"next_sales\").alias(\"total_next_sales\"),).sort(desc(\"total_sales\"))\n",
    "\n",
    "#WRITE THE AGGREGATES TO DISK\n",
    "aggregated_df.write.mode(\"overwrite\").format(\"parquet\").save(dataRoot+\"/app/data.parquet\")\n",
    "total_sales_by_product_location.write.mode(\"overwrite\").format(\"parquet\").save(dataRoot+\"/app1/data.parquet\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken on GPU for Data Analysis: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467ab04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a24318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
