{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ceb94f-8fe1-4b5e-aca0-95603ed385c6",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221bdbaf-997e-4e6d-b6f7-2e870ee94ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_version='3.5.3'\n",
    "rapids_version='25.10.0'\n",
    "sparkmeasure_version='0.27'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67246397-0aab-4c0f-bd7d-36063dd6386b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \\\n",
    "  tpcds_pyspark \\\n",
    "  pandas \\\n",
    "  sparkmeasure=={sparkmeasure_version}.0 \\\n",
    "  matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77190a13-c5aa-454a-925c-b3aa2c6fa99d",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ec0b9b-a94b-4176-9123-48329941cd69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib.resources import files\n",
    "from pyspark.sql import SparkSession\n",
    "from tpcds_pyspark import TPCDS\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c86bb-f557-4f20-8899-d7d27023ad50",
   "metadata": {},
   "source": [
    "# Init a SparkSession with RAPIDS Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54c758-44df-4aa1-afaa-f9c23c569313",
   "metadata": {},
   "source": [
    "# Detect Scala Version used in PySpark package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5745a225-d3b6-4613-8de5-d56d838e2548",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_sql_jar_path, *_ = glob.glob(f\"/usr/lib/spark/jars/spark-sql_*jar\")\n",
    "spark_sql_jar = os.path.basename(spark_sql_jar_path)\n",
    "scala_version = re.search(r'^spark-sql_(\\d+.\\d+)-.*\\.jar$', spark_sql_jar).group(1)\n",
    "scala_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c44f7f-5079-407b-9ecc-e9e862847590",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f87e87a-a78a-4462-9a41-176412850cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 23:46:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://testbyhao2-ubuntu22-m.c.rapids-spark.internal:46705\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7cf27d1a5310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"NDS Example\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b75e8-8074-4ce3-9456-a75749ccf3a2",
   "metadata": {},
   "source": [
    "# Verify SQL Acceleration on GPU can be enabled by checking the query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3342862a-30dd-4610-af7f-d3ef69af7038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   GpuColumnarToRow false, [loreId=22]\n",
      "   +- GpuHashAggregate (keys=[], functions=[gpubasicsum(id#0L, LongType, false)]), filters=ArrayBuffer(None)) [loreId=21]\n",
      "      +- GpuShuffleCoalesce 1073741824, [loreId=20]\n",
      "         +- ShuffleQueryStage 0\n",
      "            +- GpuColumnarExchange gpusinglepartitioning$(), ENSURE_REQUIREMENTS, [plan_id=64], [loreId=17]\n",
      "               +- GpuHashAggregate (keys=[], functions=[partial_gpubasicsum(id#0L, LongType, false)]), filters=ArrayBuffer(None)) [loreId=16]\n",
      "                  +- GpuRange (0, 1000, step=1, splits=2)\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[], functions=[sum(id#0L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#0L)])\n",
      "         +- Range (0, 1000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled', True)\n",
    "sum_df = spark.range(1000).selectExpr('SUM(*)')\n",
    "sum_df.collect()\n",
    "sum_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0b86e-4ed6-40a4-9b5f-926a982ccd95",
   "metadata": {},
   "source": [
    "# TPCDS App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec05e43-464c-4d98-b8d7-f775dd2196eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkMeasure jar path: /opt/conda/lib/python3.11/site-packages/tpcds_pyspark/spark-measure_2.13-0.25.jar\n",
      "TPCDS queries path: /opt/conda/lib/python3.11/site-packages/tpcds_pyspark/Queries\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/LucaCanali/Miscellaneous/tree/master/Performance_Testing/TPCDS_PySpark/tpcds_pyspark/Queries\n",
    "# queries = None to run all (takes much longer)\n",
    "queries = None\n",
    "queries = [\n",
    "    'q14a',\n",
    "    'q14b',\n",
    "    'q23a',\n",
    "    'q23b',\n",
    "    # 'q24a',\n",
    "    # 'q24b',\n",
    "    # 'q88',\n",
    "]\n",
    "\n",
    "demo_start = time.time()\n",
    "tpcds = TPCDS(data_path='gs://gcs_bucket/parquet_sf3k_decimal/', num_runs=1, queries_repeat_times=1, queries=queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea56daf-3cd4-4f8a-ac4d-d19518b936ac",
   "metadata": {},
   "source": [
    "# Register TPC-DS tables before running queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56becf51-525d-412f-b89b-f3d593441428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view catalog_returns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 23:47:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view catalog_sales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view inventory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view store_returns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view store_sales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view web_returns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view web_sales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view call_center\n",
      "Creating temporary view catalog_page\n",
      "Creating temporary view customer\n",
      "Creating temporary view customer_address\n",
      "Creating temporary view customer_demographics\n",
      "Creating temporary view date_dim\n",
      "Creating temporary view household_demographics\n",
      "Creating temporary view income_band\n",
      "Creating temporary view item\n",
      "Creating temporary view promotion\n",
      "Creating temporary view reason\n",
      "Creating temporary view ship_mode\n",
      "Creating temporary view store\n",
      "Creating temporary view time_dim\n",
      "Creating temporary view warehouse\n",
      "Creating temporary view web_page\n",
      "Creating temporary view web_site\n"
     ]
    }
   ],
   "source": [
    "tpcds.map_tables() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a0c59-582f-40c0-939f-d11c639776d6",
   "metadata": {},
   "source": [
    "# Measure Apache Spark GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e378bc6f-a26c-4f88-8765-f00ae6fa682d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 0 - query q14a - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 23:48:32 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:32 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:32 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:32 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:32 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:33 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:48:37 WARN GpuOverrides: \n",
      "! <OverwriteByExpressionExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished\n",
      "...Start Time = 2025-11-18 23:48:29\n",
      "...Elapsed Time = 142.7 sec\n",
      "...Executors Run Time = 4031.78 sec\n",
      "...Executors CPU Time = 482.07 sec\n",
      "...Executors JVM GC Time = 27.43 sec\n",
      "...Average Active Tasks = 28.3\n",
      "\n",
      "Run 0 - query q14b - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 23:51:04 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:51:04 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/18 23:51:04 WARN GpuOverrides: \n",
      "! <OverwriteByExpressionExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished\n",
      "...Start Time = 2025-11-18 23:51:03\n",
      "...Elapsed Time = 92.64 sec\n",
      "...Executors Run Time = 2720.97 sec\n",
      "...Executors CPU Time = 462.29 sec\n",
      "...Executors JVM GC Time = 22.78 sec\n",
      "...Average Active Tasks = 29.4\n",
      "\n",
      "Run 0 - query q23a - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 23:52:39 WARN GpuOverrides: \n",
      "! <OverwriteByExpressionExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec\n",
      "\n",
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished\n",
      "...Start Time = 2025-11-18 23:52:38\n",
      "...Elapsed Time = 246.92 sec\n",
      "...Executors Run Time = 7679.47 sec\n",
      "...Executors CPU Time = 2431.34 sec\n",
      "...Executors JVM GC Time = 58.84 sec\n",
      "...Average Active Tasks = 31.1\n",
      "\n",
      "Run 0 - query q23b - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 23:56:47 WARN GpuOverrides: \n",
      "! <OverwriteByExpressionExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec\n",
      "\n",
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished\n",
      "...Start Time = 2025-11-18 23:56:47\n",
      "...Elapsed Time = 349.65 sec\n",
      "...Executors Run Time = 10949.18 sec\n",
      "...Executors CPU Time = 3601.05 sec\n",
      "...Executors JVM GC Time = 124.94 sec\n",
      "...Average Active Tasks = 31.3\n",
      "CPU times: user 2.13 s, sys: 536 ms, total: 2.67 s\n",
      "Wall time: 14min 8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>numStages</th>\n",
       "      <th>numTasks</th>\n",
       "      <th>elapsedTime</th>\n",
       "      <th>stageDuration</th>\n",
       "      <th>executorRunTime</th>\n",
       "      <th>executorCpuTime</th>\n",
       "      <th>executorDeserializeTime</th>\n",
       "      <th>executorDeserializeCpuTime</th>\n",
       "      <th>resultSerializationTime</th>\n",
       "      <th>...</th>\n",
       "      <th>shuffleLocalBlocksFetched</th>\n",
       "      <th>shuffleRemoteBlocksFetched</th>\n",
       "      <th>shuffleTotalBytesRead</th>\n",
       "      <th>shuffleLocalBytesRead</th>\n",
       "      <th>shuffleRemoteBytesRead</th>\n",
       "      <th>shuffleRemoteBytesReadToDisk</th>\n",
       "      <th>shuffleBytesWritten</th>\n",
       "      <th>shuffleRecordsWritten</th>\n",
       "      <th>avg_active_tasks</th>\n",
       "      <th>elapsed_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q14a</td>\n",
       "      <td>33</td>\n",
       "      <td>3029</td>\n",
       "      <td>142702</td>\n",
       "      <td>202073</td>\n",
       "      <td>4031784</td>\n",
       "      <td>482069</td>\n",
       "      <td>35938</td>\n",
       "      <td>26477</td>\n",
       "      <td>737</td>\n",
       "      <td>...</td>\n",
       "      <td>52365</td>\n",
       "      <td>52765</td>\n",
       "      <td>17238723794</td>\n",
       "      <td>10959371688</td>\n",
       "      <td>6279352106</td>\n",
       "      <td>0</td>\n",
       "      <td>17224468696</td>\n",
       "      <td>1453677</td>\n",
       "      <td>28</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q14b</td>\n",
       "      <td>27</td>\n",
       "      <td>2924</td>\n",
       "      <td>92635</td>\n",
       "      <td>109468</td>\n",
       "      <td>2720965</td>\n",
       "      <td>462286</td>\n",
       "      <td>19677</td>\n",
       "      <td>19074</td>\n",
       "      <td>510</td>\n",
       "      <td>...</td>\n",
       "      <td>51274</td>\n",
       "      <td>51791</td>\n",
       "      <td>13772369358</td>\n",
       "      <td>7503041602</td>\n",
       "      <td>6269327756</td>\n",
       "      <td>0</td>\n",
       "      <td>13772258680</td>\n",
       "      <td>1325383</td>\n",
       "      <td>29</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q23a</td>\n",
       "      <td>17</td>\n",
       "      <td>5585</td>\n",
       "      <td>246917</td>\n",
       "      <td>321434</td>\n",
       "      <td>7679465</td>\n",
       "      <td>2431345</td>\n",
       "      <td>59686</td>\n",
       "      <td>57146</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1461555</td>\n",
       "      <td>1459430</td>\n",
       "      <td>167343853198</td>\n",
       "      <td>83745729041</td>\n",
       "      <td>83598124157</td>\n",
       "      <td>0</td>\n",
       "      <td>105548884801</td>\n",
       "      <td>2549000</td>\n",
       "      <td>31</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q23b</td>\n",
       "      <td>20</td>\n",
       "      <td>5650</td>\n",
       "      <td>349650</td>\n",
       "      <td>629014</td>\n",
       "      <td>10949183</td>\n",
       "      <td>3601045</td>\n",
       "      <td>60423</td>\n",
       "      <td>59165</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>2559681</td>\n",
       "      <td>2558336</td>\n",
       "      <td>290890866947</td>\n",
       "      <td>145433890100</td>\n",
       "      <td>145456976847</td>\n",
       "      <td>0</td>\n",
       "      <td>106425012814</td>\n",
       "      <td>4270104</td>\n",
       "      <td>31</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  query  numStages  numTasks  elapsedTime  stageDuration  executorRunTime  \\\n",
       "0  q14a         33      3029       142702         202073          4031784   \n",
       "1  q14b         27      2924        92635         109468          2720965   \n",
       "2  q23a         17      5585       246917         321434          7679465   \n",
       "3  q23b         20      5650       349650         629014         10949183   \n",
       "\n",
       "   executorCpuTime  executorDeserializeTime  executorDeserializeCpuTime  \\\n",
       "0           482069                    35938                       26477   \n",
       "1           462286                    19677                       19074   \n",
       "2          2431345                    59686                       57146   \n",
       "3          3601045                    60423                       59165   \n",
       "\n",
       "   resultSerializationTime  ...  shuffleLocalBlocksFetched  \\\n",
       "0                      737  ...                      52365   \n",
       "1                      510  ...                      51274   \n",
       "2                       22  ...                    1461555   \n",
       "3                       43  ...                    2559681   \n",
       "\n",
       "   shuffleRemoteBlocksFetched  shuffleTotalBytesRead  shuffleLocalBytesRead  \\\n",
       "0                       52765            17238723794            10959371688   \n",
       "1                       51791            13772369358             7503041602   \n",
       "2                     1459430           167343853198            83745729041   \n",
       "3                     2558336           290890866947           145433890100   \n",
       "\n",
       "   shuffleRemoteBytesRead  shuffleRemoteBytesReadToDisk  shuffleBytesWritten  \\\n",
       "0              6279352106                             0          17224468696   \n",
       "1              6269327756                             0          13772258680   \n",
       "2             83598124157                             0         105548884801   \n",
       "3            145456976847                             0         106425012814   \n",
       "\n",
       "   shuffleRecordsWritten  avg_active_tasks  elapsed_time_seconds  \n",
       "0                1453677                28                   142  \n",
       "1                1325383                29                    92  \n",
       "2                2549000                31                   246  \n",
       "3                4270104                31                   349  \n",
       "\n",
       "[4 rows x 33 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpcds.spark.conf.set('spark.rapids.sql.enabled', True)\n",
    "%time tpcds.run_TPCDS()\n",
    "gpu_grouped_results = tpcds.grouped_results_pdf.copy()\n",
    "gpu_grouped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eb475-d7e1-4c79-9d4e-8cbbb072647f",
   "metadata": {},
   "source": [
    "## Measure Apache Spark CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f3ab6-25f9-45db-b0d8-a2c743a45d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 0 - query q14a - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:40 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:41 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:41 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:41 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:41 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:02:41 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished\n",
      "...Start Time = 2025-11-19 00:02:38\n",
      "...Elapsed Time = 623.59 sec\n",
      "...Executors Run Time = 17863.87 sec\n",
      "...Executors CPU Time = 15325.5 sec\n",
      "...Executors JVM GC Time = 199.65 sec\n",
      "...Average Active Tasks = 28.6\n",
      "\n",
      "Run 0 - query q14b - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 00:13:08 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "25/11/19 00:13:08 INFO PlanChangeLogger: \n",
      " Dataproc Rule org.apache.spark.sql.catalyst.optimizer.ReplaceIntersectWithSemiJoin effective 1 times.\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished\n",
      "...Start Time = 2025-11-19 00:13:08\n",
      "...Elapsed Time = 579.5 sec\n",
      "...Executors Run Time = 16634.54 sec\n",
      "...Executors CPU Time = 14340.63 sec\n",
      "...Executors JVM GC Time = 176.47 sec\n",
      "...Average Active Tasks = 28.7\n",
      "\n",
      "Run 0 - query q23a - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished\n",
      "...Start Time = 2025-11-19 00:22:49\n",
      "...Elapsed Time = 1875.39 sec\n",
      "...Executors Run Time = 59083.8 sec\n",
      "...Executors CPU Time = 55319.68 sec\n",
      "...Executors JVM GC Time = 510.15 sec\n",
      "...Average Active Tasks = 31.5\n",
      "\n",
      "Run 0 - query q23b - attempt 0 - starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 465:========>  (816 + 16) / 1000][Stage 468:=========> (823 + 16) / 1000]\r"
     ]
    }
   ],
   "source": [
    "tpcds.spark.conf.set('spark.rapids.sql.enabled', False)\n",
    "%time tpcds.run_TPCDS()\n",
    "cpu_grouped_results = tpcds.grouped_results_pdf.copy()\n",
    "cpu_grouped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b069a0-935d-4f1d-958b-1b4b1c156b85",
   "metadata": {},
   "source": [
    "## Show Speedup Factors achieved by GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48d7ef-00b9-4744-8cc1-c80225170c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = pd.merge(cpu_grouped_results, gpu_grouped_results, on='query', how='inner', suffixes=['_cpu', '_gpu'])\n",
    "res['speedup'] = res['elapsedTime_cpu'] / res['elapsedTime_gpu']\n",
    "res = res.sort_values(by='elapsedTime_cpu', ascending=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ee04a-b3fe-46ab-9c7e-cfd02592b65d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo_dur = time.time() - demo_start\n",
    "print(f\"CPU and GPU run took: {demo_dur=} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267bf53-0147-4a50-9656-ad369873e8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.plot(title='TPC-DS query elapsedTime on CPU vs GPU (lower is better)', \n",
    "         kind='bar', x='query', y=['elapsedTime_cpu', 'elapsedTime_gpu'],\n",
    "         color=['blue', '#76B900'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ec37e-7570-48e3-887c-85e112141988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.plot(title='Speedup factors of TPC-DS queries on GPU', kind='bar', \n",
    "         x='query', y='speedup', color='#76B900')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
