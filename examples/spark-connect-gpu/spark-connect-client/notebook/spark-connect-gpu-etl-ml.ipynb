{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Spark Connect - SQL/DF ETL and MLlib on Mortgage Dataset (Spark 4.0+)\n",
    "\n",
    "Based on the Data and AI Summit 2025 session: [GPU Accelerated Spark Connect](https://www.databricks.com/dataaisummit/session/gpu-accelerated-spark-connect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, FeatureHasher\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark via Spark Connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPU-accelerated Spark session using Spark Connect 4.0+\n",
    "spark = (\n",
    "  SparkSession.builder\n",
    "    .appName('GPU-Accelerated Spark Connect - SQL/ETL and MLlib') \n",
    "    .getOrCreate()\n",
    ")\n",
    "print(f'Spark Connect session id: {spark.session_id}')\n",
    "print(f'Spark version: {spark.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local and Global Storage Access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be a local storage location accessible to the thin Spark Connect app\n",
    "# such as a IPython kernel\n",
    "local_data_dir = 'work'\n",
    "\n",
    "# This would normally be a global storage location such as Cloud Object Storage\n",
    "# This notebook requires a writable directory on the host. It is mounted into containers\n",
    "# requiring access to it as /data from the host \n",
    "# This directory should contain directory `mortgage.input.csv` with files from the Mortgage dataset.\n",
    "# We also store here data useful across the container life cycle such as metrics from the previous runs\n",
    "# and Spark event logs. \n",
    "global_data_dir = '/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize references to the same bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{local_data_dir}/name_mapping.csv', 'r') as name_mapping_file:\n",
    "  nm_reader = csv.reader(name_mapping_file,)\n",
    "  name_mapping = [r for r in nm_reader]\n",
    "name_mapping_df = spark.createDataFrame(name_mapping, ['from_seller_name', 'to_seller_name'])\n",
    "\n",
    "(\n",
    "  name_mapping_df\n",
    "    .where(col('to_seller_name') == 'Wells Fargo' )\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String columns\n",
    "cate_col_names = [\n",
    "  'orig_channel',\n",
    "  'first_home_buyer',\n",
    "  'loan_purpose',\n",
    "  'property_type',\n",
    "  'occupancy_status',\n",
    "  'property_state',\n",
    "  'product_type',\n",
    "  'relocation_mortgage_indicator',\n",
    "  'seller_name',\n",
    "  'mod_flag'\n",
    "]\n",
    "# Numeric columns\n",
    "label_col_name = 'delinquency_12'\n",
    "numeric_col_names = [\n",
    "  'orig_interest_rate',\n",
    "  'orig_upb',\n",
    "  'orig_loan_term',\n",
    "  'orig_ltv',\n",
    "  'orig_cltv',\n",
    "  'num_borrowers',\n",
    "  'dti',\n",
    "  'borrower_credit_score',\n",
    "  'num_units',\n",
    "  'zip',\n",
    "  'mortgage_insurance_percent',\n",
    "  'current_loan_delinquency_status',\n",
    "  'current_actual_upb',\n",
    "  'interest_rate',\n",
    "  'loan_age',\n",
    "  'msa',\n",
    "  'non_interest_bearing_upb',\n",
    "  label_col_name\n",
    "]\n",
    "all_col_names = cate_col_names + numeric_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read raw columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_csv(spark, path):\n",
    "  def _get_quarter_from_csv_file_name():\n",
    "    return substring_index(substring_index(input_file_name(), '.', 1), '/', -1)\n",
    "\n",
    "  with open(f'{local_data_dir}/csv_raw_schema.ddl', 'r') as f:\n",
    "    _csv_raw_schema_str = f.read()\n",
    "  \n",
    "  return (\n",
    "    spark.read\n",
    "    .format('csv') \n",
    "    .option('nullValue', '') \n",
    "    .option('header', False) \n",
    "    .option('delimiter', '|') \n",
    "    .schema(_csv_raw_schema_str) \n",
    "    .load(path) \n",
    "    .withColumn('quarter', _get_quarter_from_csv_file_name())\n",
    "  )\n",
    "\n",
    "def extract_perf_columns(rawDf):\n",
    "  perfDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    date_format(to_date(col('monthly_reporting_period'),'MMyyyy'), 'MM/dd/yyyy').alias('monthly_reporting_period'),\n",
    "    upper(col('servicer')).alias('servicer'),\n",
    "    col('interest_rate'),\n",
    "    col('current_actual_upb'),\n",
    "    col('loan_age'),\n",
    "    col('remaining_months_to_legal_maturity'),\n",
    "    col('adj_remaining_months_to_maturity'),\n",
    "    date_format(to_date(col('maturity_date'),'MMyyyy'), 'MM/yyyy').alias('maturity_date'),\n",
    "    col('msa'),\n",
    "    col('current_loan_delinquency_status'),\n",
    "    col('mod_flag'),\n",
    "    col('zero_balance_code'),\n",
    "    date_format(to_date(col('zero_balance_effective_date'),'MMyyyy'), 'MM/yyyy').alias('zero_balance_effective_date'),\n",
    "    date_format(to_date(col('last_paid_installment_date'),'MMyyyy'), 'MM/dd/yyyy').alias('last_paid_installment_date'),\n",
    "    date_format(to_date(col('foreclosed_after'),'MMyyyy'), 'MM/dd/yyyy').alias('foreclosed_after'),\n",
    "    date_format(to_date(col('disposition_date'),'MMyyyy'), 'MM/dd/yyyy').alias('disposition_date'),\n",
    "    col('foreclosure_costs'),\n",
    "    col('prop_preservation_and_repair_costs'),\n",
    "    col('asset_recovery_costs'),\n",
    "    col('misc_holding_expenses'),\n",
    "    col('holding_taxes'),\n",
    "    col('net_sale_proceeds'),\n",
    "    col('credit_enhancement_proceeds'),\n",
    "    col('repurchase_make_whole_proceeds'),\n",
    "    col('other_foreclosure_proceeds'),\n",
    "    col('non_interest_bearing_upb'),\n",
    "    col('principal_forgiveness_upb'),\n",
    "    col('repurchase_make_whole_proceeds_flag'),\n",
    "    col('foreclosure_principal_write_off_amount'),\n",
    "    col('servicing_activity_indicator'),\n",
    "    col('quarter')\n",
    "  )\n",
    "  return perfDf.select('*').filter('current_actual_upb != 0.0')\n",
    "\n",
    "def extract_acq_columns(rawDf):\n",
    "  acqDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    col('orig_channel'),\n",
    "    upper(col('seller_name')).alias('seller_name'),\n",
    "    col('orig_interest_rate'),\n",
    "    col('orig_upb'),\n",
    "    col('orig_loan_term'),\n",
    "    date_format(to_date(col('orig_date'),'MMyyyy'), 'MM/yyyy').alias('orig_date'),\n",
    "    date_format(to_date(col('first_pay_date'),'MMyyyy'), 'MM/yyyy').alias('first_pay_date'),\n",
    "    col('orig_ltv'),\n",
    "    col('orig_cltv'),\n",
    "    col('num_borrowers'),\n",
    "    col('dti'),\n",
    "    col('borrower_credit_score'),\n",
    "    col('first_home_buyer'),\n",
    "    col('loan_purpose'),\n",
    "    col('property_type'),\n",
    "    col('num_units'),\n",
    "    col('occupancy_status'),\n",
    "    col('property_state'),\n",
    "    col('zip'),\n",
    "    col('mortgage_insurance_percent'),\n",
    "    col('product_type'),\n",
    "    col('coborrow_credit_score'),\n",
    "    col('mortgage_insurance_type'),\n",
    "    col('relocation_mortgage_indicator'),\n",
    "    dense_rank().over(Window.partitionBy('loan_id').orderBy(to_date(col('monthly_reporting_period'),'MMyyyy'))).alias('rank'),\n",
    "    col('quarter')\n",
    "  )\n",
    "\n",
    "  return acqDf.select('*').filter(col('rank')==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to parse date in Performance data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_dates(perf):\n",
    "  return (\n",
    "    perf.withColumn('monthly_reporting_period', to_date(col('monthly_reporting_period'), 'MM/dd/yyyy')) \n",
    "      .withColumn('monthly_reporting_period_month', month(col('monthly_reporting_period'))) \n",
    "      .withColumn('monthly_reporting_period_year', year(col('monthly_reporting_period'))) \n",
    "      .withColumn('monthly_reporting_period_day', dayofmonth(col('monthly_reporting_period'))) \n",
    "      .withColumn('last_paid_installment_date', to_date(col('last_paid_installment_date'), 'MM/dd/yyyy')) \n",
    "      .withColumn('foreclosed_after', to_date(col('foreclosed_after'), 'MM/dd/yyyy')) \n",
    "      .withColumn('disposition_date', to_date(col('disposition_date'), 'MM/dd/yyyy')) \n",
    "      .withColumn('maturity_date', to_date(col('maturity_date'), 'MM/yyyy')) \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create deliquency data frame from Performance data.  \n",
    "\n",
    "The computed `delinquency_12` column denotes whether a loan will become delinquent by 3, 6, or 9 months, \n",
    "or not delinquent, within the next 12 month period.   \n",
    "\n",
    "It will be the target label for ML multi-class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_perf_deliquency(spark, perf):\n",
    "  aggDF = (\n",
    "    perf\n",
    "      .select(\n",
    "        col('quarter'),\n",
    "        col('loan_id'),\n",
    "        col('current_loan_delinquency_status'),\n",
    "        when(col('current_loan_delinquency_status') >= 1, col('monthly_reporting_period')).alias('delinquency_30'),\n",
    "        when(col('current_loan_delinquency_status') >= 3, col('monthly_reporting_period')).alias('delinquency_90'),\n",
    "        when(col('current_loan_delinquency_status') >= 6, col('monthly_reporting_period')).alias('delinquency_180')\n",
    "      ).groupBy('quarter', 'loan_id')\n",
    "       .agg(\n",
    "         max('current_loan_delinquency_status').alias('delinquency_12'),\n",
    "         min('delinquency_30').alias('delinquency_30'),\n",
    "         min('delinquency_90').alias('delinquency_90'),\n",
    "         min('delinquency_180').alias('delinquency_180')\n",
    "       ).select(\n",
    "         col('quarter'),\n",
    "         col('loan_id'),\n",
    "         (col('delinquency_12') >= 1).alias('ever_30'),\n",
    "         (col('delinquency_12') >= 3).alias('ever_90'),\n",
    "         (col('delinquency_12') >= 6).alias('ever_180'),\n",
    "         col('delinquency_30'),\n",
    "         col('delinquency_90'),\n",
    "         col('delinquency_180')\n",
    "       )\n",
    "  )\n",
    "  #aggDF.printSchema()\n",
    "  joinedDf = (\n",
    "    perf\n",
    "      .withColumnRenamed('monthly_reporting_period', 'timestamp')\n",
    "      .withColumnRenamed('monthly_reporting_period_month', 'timestamp_month') \n",
    "      .withColumnRenamed('monthly_reporting_period_year', 'timestamp_year') \n",
    "      .withColumnRenamed('current_loan_delinquency_status', 'delinquency_12') \n",
    "      .withColumnRenamed('current_actual_upb', 'upb_12') \n",
    "      .select('quarter', 'loan_id', 'timestamp', 'delinquency_12', 'upb_12', 'timestamp_month', 'timestamp_year') \n",
    "      .join(aggDF, ['loan_id', 'quarter'], 'left_outer')\n",
    "  )\n",
    "  # calculate the 12 month delinquency and upb values\n",
    "  months = 12\n",
    "  monthArray = [lit(x) for x in range(0, 12)]\n",
    "  \n",
    "  testDf = ( \n",
    "    joinedDf\n",
    "      .withColumn('month_y', explode(array(monthArray)))\n",
    "      .select(\n",
    "        col('quarter'),\n",
    "        floor(((col('timestamp_year') * 12 + col('timestamp_month')) - 24000) / months).alias('josh_mody'),\n",
    "        floor(((col('timestamp_year') * 12 + col('timestamp_month')) - 24000 - col('month_y')) / months).alias('josh_mody_n'),\n",
    "        col('ever_30'),\n",
    "        col('ever_90'),\n",
    "        col('ever_180'),\n",
    "        col('delinquency_30'),\n",
    "        col('delinquency_90'),\n",
    "        col('delinquency_180'),\n",
    "        col('loan_id'),\n",
    "        col('month_y'),\n",
    "        col('delinquency_12'),\n",
    "        col('upb_12')\n",
    "      ).groupBy('quarter', 'loan_id', 'josh_mody_n', 'ever_30', 'ever_90', 'ever_180', 'delinquency_30', 'delinquency_90', 'delinquency_180', 'month_y')\n",
    "    .agg(max('delinquency_12').alias('delinquency_12'), min('upb_12').alias('upb_12')) \n",
    "    .withColumn('timestamp_year', floor((lit(24000) + (col('josh_mody_n') * lit(months)) + (col('month_y') - 1)) / lit(12))) \n",
    "    .selectExpr('*', f'pmod(24000 + (josh_mody_n * {months}) + month_y, 12) as timestamp_month_tmp') \n",
    "    .withColumn('timestamp_month', when(col('timestamp_month_tmp') == lit(0), lit(12)).otherwise(col('timestamp_month_tmp'))) \n",
    "    .withColumn('delinquency_12', ((col('delinquency_12') > 9).cast('int') + (col('delinquency_12') > 6).cast('int') + (col('delinquency_12') > 3).cast('int') + (col('upb_12') == 0).cast('int')).alias('delinquency_12')) \n",
    "    .drop('timestamp_month_tmp', 'josh_mody_n', 'month_y')\n",
    "  )\n",
    "\n",
    "  return (\n",
    "    perf\n",
    "      .withColumnRenamed('monthly_reporting_period_month', 'timestamp_month')\n",
    "      .withColumnRenamed('monthly_reporting_period_year', 'timestamp_year')\n",
    "      .join(testDf, ['quarter', 'loan_id', 'timestamp_year', 'timestamp_month'], 'left')\n",
    "      .drop('timestamp_year', 'timestamp_month')\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create acquisition data frame from Acquisition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_acquisition(spark, acq):\n",
    "  return (\n",
    "    acq.join(name_mapping_df, col('seller_name') == col('from_seller_name'), 'left')\n",
    "      .drop('from_seller_name') \n",
    "      .withColumn('old_name', col('seller_name')) \n",
    "      .withColumn('seller_name', coalesce(col('to_seller_name'), col('seller_name'))) \n",
    "      .drop('to_seller_name') \n",
    "      .withColumn('orig_date', to_date(col('orig_date'), 'MM/yyyy')) \n",
    "      .withColumn('first_pay_date', to_date(col('first_pay_date'), 'MM/yyyy')) \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Casting Process\n",
    "\n",
    "\n",
    "This part is casting String column to Numeric one. \n",
    "Example:\n",
    "```\n",
    "col_1\n",
    " \"a\"\n",
    " \"b\"\n",
    " \"c\"\n",
    " \"a\"\n",
    "# After String ====> Numeric\n",
    "col_1\n",
    " 0\n",
    " 1\n",
    " 2\n",
    " 0\n",
    "```  \n",
    "\n",
    "### Define function to get column dictionary\n",
    "\n",
    "Example\n",
    "\n",
    "```\n",
    "col1 = [row(data=\"a\",id=0), row(data=\"b\",id=1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_dictionary(etl_df, col_names):\n",
    "  cnt_table = (\n",
    "    etl_df.select(posexplode(array([col(i) for i in col_names])))\n",
    "      .withColumnRenamed('pos', 'column_id')\n",
    "      .withColumnRenamed('col', 'data')\n",
    "      .filter('data is not null')\n",
    "      .groupBy('column_id', 'data')\n",
    "      .count()\n",
    "  )\n",
    "  windowed = Window.partitionBy('column_id').orderBy(desc('count'))\n",
    "  return cnt_table.withColumn('id', row_number().over(windowed)).drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to convert string columns to numeric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cast_string_columns_to_numeric(spark, input_df):\n",
    "  cached_dict_df = _gen_dictionary(input_df, cate_col_names)  \n",
    "  # .cache()\n",
    "  #  Uncomment above line to cache the dictionary dataframe. You need to spark.catalog.clearCache()\n",
    "  #  when running the notebook multiple times switching between CPU and GPU.\n",
    "  \n",
    "  output_df = input_df\n",
    "  #  Generate the final table with all columns being numeric.\n",
    "  for col_pos, col_name in enumerate(cate_col_names):\n",
    "    col_dict_df = (\n",
    "      cached_dict_df.filter(col('column_id') == col_pos)\n",
    "        .drop('column_id')\n",
    "        .withColumnRenamed('data', col_name)\n",
    "    )\n",
    "    output_df = (\n",
    "      output_df.join(broadcast(col_dict_df), col_name, 'left')\n",
    "        .drop(col_name)\n",
    "        .withColumnRenamed('id', col_name)\n",
    "    )\n",
    "  return output_df     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Function\n",
    "\n",
    "In this function:\n",
    "1. Parse date in Performance data by calling _parse_dates (parsed_perf)\n",
    "2. Create deliqency dataframe(perf_deliqency) form Performance data by calling _create_perf_deliquency\n",
    "3. Create cleaned acquisition dataframe(cleaned_acq) from Acquisition data by calling _create_acquisition\n",
    "4. Join deliqency dataframe(perf_deliqency) and cleaned acquisition dataframe(cleaned_acq), get clean_df\n",
    "5. Cast String column to Numeric in clean_df by calling _cast_string_columns_to_numeric, get casted_clean_df\n",
    "6. Return casted_clean_df as final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mortgage(spark, perf, acq):\n",
    "  parsed_perf = _parse_dates(perf)\n",
    "  perf_deliqency = _create_perf_deliquency(spark, parsed_perf)\n",
    "  cleaned_acq = _create_acquisition(spark, acq)\n",
    "  clean_df = perf_deliqency.join(cleaned_acq, ['loan_id', 'quarter'], 'inner').drop('quarter')\n",
    "  casted_clean_df = (\n",
    "    _cast_string_columns_to_numeric(spark, clean_df)\n",
    "      .select(all_col_names)\n",
    "      .withColumn(label_col_name, when(col(label_col_name) > 0, col(label_col_name)).otherwise(0))\n",
    "      .fillna(float(0))\n",
    "  )\n",
    "  return casted_clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knobs for running the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should raw csv input be used or input persisted to Parquet \n",
    "read_from_csv = False\n",
    "# if not read_from_csv, include conversion to Parquet in this run?\n",
    "convert_csv_to_parquet = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Execute SQL and ML on GPU ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "accelerate_on_gpu = True"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled', accelerate_on_gpu)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerate_on_gpu:\n",
    "  spark.conf.set('spark.connect.ml.backend.classes', 'com.nvidia.rapids.ml.Plugin')\n",
    "else:\n",
    "  spark.conf.unset('spark.connect.ml.backend.classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ETL Pipeline\n",
    "\n",
    "#### Read Raw Data and Run ETL Process, Save the Result\n",
    "\n",
    "##### Convert CSV to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_from_csv:\n",
    "  mortgage_csv = read_raw_csv(spark, f'{global_data_dir}/mortgage.input.csv')\n",
    "elif convert_csv_to_parquet:\n",
    "  read_raw_csv(spark, f'{global_data_dir}/mortgage.input.csv')\\\n",
    "    .write.parquet(f'{global_data_dir}/mortgage_input.pq', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ETL from Parquet or raw CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortgage = mortgage_csv if read_from_csv else spark.read.parquet(f'{global_data_dir}/mortgage_input.pq')\n",
    "acq = extract_acq_columns(mortgage)\n",
    "perf = extract_perf_columns(mortgage)\n",
    "# run main function to process data\n",
    "preprocessed = run_mortgage(spark, perf, acq)\n",
    "# save processed data\n",
    "\n",
    "start = time.time()\n",
    "preprocessed.write.parquet(f'{global_data_dir}/mortgage_preprocessed.pq' , mode='overwrite')\n",
    "end = time.time()\n",
    "\n",
    "etl_dur = end - start\n",
    "print(f'ETL takes {etl_dur}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Pipeline\n",
    "\n",
    "#### The ML modeling phase of the example uses the `spark.ml` Pipeline API to carry out the following steps on a random subsample of the ETL output:\n",
    "  - use `spark.ml FeatureHasher` to map the int type columns in the ETL output to a 2^15 dimensional sparse feature vector with a non-zero entry in each location corresponding to hash value of each input column value + column name.\n",
    "  - use `spark.ml VectorAssembler` to combine the output of `FeatureHasher` with the original float type columns into a single `VectorUDT` type feature vector\n",
    "  - train a model using `LogisticRegression` to predict the multi-class (4 class values) label \"delinquency_12\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etlDf = spark.read.parquet(f'{global_data_dir}/mortgage_preprocessed.pq')\n",
    "etlDf = etlDf.sample(fraction=0.1, seed=1234)\n",
    "etlDf.describe().filter(col('summary') == 'mean').show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etlDf = etlDf.withColumn('loc',(etlDf.msa*1000+etlDf.zip).cast('int')).drop('zip' ,'msa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col_name = 'delinquency_12'\n",
    "schema = etlDf.schema\n",
    "raw_features = [ x for x in schema.fields if x.name != label_col_name ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [f.name for f in raw_features if f.dataType == IntegerType()]\n",
    "numerical_cols = [f.name for f in raw_features if f.name not in categorical_cols]\n",
    "hasher = FeatureHasher(inputCols=categorical_cols, outputCol='hashed_categorical', \n",
    "                       categoricalCols=categorical_cols, numFeatures=(1 << 15))\n",
    "va = VectorAssembler().setInputCols(numerical_cols + [hasher.getOutputCol()]).setOutputCol('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic =  ( \n",
    "  LogisticRegression()\n",
    "    .setMaxIter(200)\n",
    "    .setRegParam(0.00002)\n",
    "    .setElasticNetParam(0.1)\n",
    "    .setTol(1.0e-12)\n",
    "    .setFeaturesCol('features')\n",
    "    .setLabelCol(label_col_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train, df_test] = etlDf.randomSplit([0.8, 0.2], seed=1234)\n",
    "pipeline = Pipeline().setStages([hasher, va, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# gpu lr, gpu etl, gpu transform, 200 iters, double precision, elasticnet=0.1, featurehasher, 0.1 sample, multiclass, float64\n",
    "pipeline_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(df_test)\n",
    "predictions.sample(0.1).show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setMetricName('logLoss').setLabelCol(label_col_name)\n",
    "eval_res = evaluator.evaluate(predictions)\n",
    "end = time.time()\n",
    "print(f'Evaluation result: {eval_res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dur = end - start\n",
    "print(f'ML takes {ml_dur}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save current run times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve across container restarts\n",
    "\n",
    "local_gpu_times_file = f'{global_data_dir}/gpu_times.csv'\n",
    "local_cpu_times_file = f'{global_data_dir}/cpu_times.csv'\n",
    "\n",
    "run_times = pd.Series({'etl' : etl_dur, 'ml' : ml_dur})\n",
    "run_times.to_csv(local_gpu_times_file if accelerate_on_gpu else local_cpu_times_file, index=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(local_cpu_times_file) and os.path.exists(local_gpu_times_file):\n",
    "  cpu_times = pd.read_csv(local_cpu_times_file, header=None, index_col=0)\n",
    "  gpu_times = pd.read_csv(local_gpu_times_file, header=None, index_col=0)\n",
    "  gpu_speedup = cpu_times / gpu_times\n",
    "  gpu_speedup.plot(kind='bar', \n",
    "    title='GPU Acceleration Factor (> 1.0 is good)', \n",
    "    color='#76B900', \n",
    "    legend=False)\n",
    "  cpu_times = cpu_times[1].rename('cpu')\n",
    "  gpu_times = gpu_times[1].rename('gpu')\n",
    "  times = pd.DataFrame([cpu_times, gpu_times]).transpose()\n",
    "  times.plot(kind='bar', \n",
    "    title = 'ETL and ML elapsed times for CPU and GPU (lower is better)', \n",
    "    color=['blue', '#76B900'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
