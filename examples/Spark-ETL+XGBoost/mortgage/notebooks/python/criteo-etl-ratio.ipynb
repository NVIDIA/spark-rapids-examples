{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean encoding in Spark SQL on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "from operator import itemgetter\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "from pyspark import broadcast, SparkConf\n",
    "from pyspark.sql import Row, SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_id_with_window(df):\n",
    "    windowed = Window.partitionBy('column_id').orderBy(desc('count'))\n",
    "    return (df\n",
    "            .withColumn('id', row_number().over(windowed))\n",
    "            .withColumnRenamed('count', 'model_count'))\n",
    "\n",
    "\n",
    "def assign_low_mem_partial_ids(df):\n",
    "    # To avoid some scaling issues with a simple window operation, we use a more complex method\n",
    "    # to compute the same thing, but in a more distributed spark specific way\n",
    "    df = df.orderBy(asc('column_id'), desc('count'))\n",
    "    # The monotonically_increasing_id is the partition id in the top 31 bits and the rest\n",
    "    # is an increasing count of the rows within that partition.  So we split it into two parts,\n",
    "    # the partion id part_id and the count mono_id\n",
    "    df = df.withColumn('part_id', spark_partition_id())\n",
    "    return df.withColumn('mono_id', monotonically_increasing_id() - shiftLeft(col('part_id'), 33))\n",
    "\n",
    "\n",
    "def assign_low_mem_final_ids(df):\n",
    "    # Now we can find the minimum and maximum mono_ids within a given column/partition pair\n",
    "    sub_model = df.groupBy('column_id', 'part_id').agg(max('mono_id').alias('top'), min('mono_id').alias('bottom'))\n",
    "    sub_model = sub_model.withColumn('diff', col('top') - col('bottom') + 1)\n",
    "    sub_model = sub_model.drop('top')\n",
    "    # This window function is over aggregated column/partition pair table. It will do a running sum of the rows\n",
    "    # within that column\n",
    "    windowed = Window.partitionBy('column_id').orderBy('part_id').rowsBetween(Window.unboundedPreceding, -1)\n",
    "    sub_model = sub_model.withColumn('running_sum', sum('diff').over(windowed)).na.fill(0, [\"running_sum\"])\n",
    "\n",
    "    joined = df.withColumnRenamed('column_id', 'i_column_id')\n",
    "    joined = joined.withColumnRenamed('part_id', 'i_part_id')\n",
    "    joined = joined.withColumnRenamed('count', 'model_count')\n",
    "\n",
    "    # Then we can join the original input with the pair it is a part of\n",
    "    joined = joined.join(sub_model, (col('i_column_id') == col('column_id')) & (col('part_id') == col('i_part_id')))\n",
    "\n",
    "    # So with all that we can subtract bottom from mono_id makeing it start at 0 for each partition\n",
    "    # and then add in the running_sum so the id is contiguous and unique for the entire column. + 1 to make it match the 1 based indexing\n",
    "    # for row_number\n",
    "    ret = joined.select(col('column_id'),\n",
    "                        col('data'),\n",
    "                        (col('mono_id') - col('bottom') + col('running_sum') + 1).cast(IntegerType()).alias('id'),\n",
    "                        col('model_count'))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_models(combined_model):\n",
    "    for i in CAT_COLS:\n",
    "        model = (combined_model\n",
    "            .filter('column_id == %d' % (i - CAT_COLS[0]))\n",
    "            .drop('column_id'))\n",
    "        yield i, model\n",
    "\n",
    "\n",
    "def col_of_rand_long():\n",
    "    return (rand() * (1 << 52)).cast(LongType())\n",
    "\n",
    "def skewed_join(df, model, col_name, cutoff):\n",
    "    # Most versions of spark don't have a good way\n",
    "    # to deal with a skewed join out of the box.\n",
    "    # Some do and if you want to replace this with\n",
    "    # one of those that would be great.\n",
    "    \n",
    "    # Because we have statistics about the skewedness\n",
    "    # that we can used we divide the model up into two parts\n",
    "    # one part is the highly skewed part and we do a\n",
    "    # broadcast join for that part, but keep the result in\n",
    "    # a separate column\n",
    "    b_model = broadcast(model.filter(col('model_count') >= cutoff)\n",
    "            .withColumnRenamed('data', col_name)\n",
    "            .drop('model_count'))\n",
    "    \n",
    "    df = (df\n",
    "            .join(b_model, col_name, how='left')\n",
    "            .withColumnRenamed('id', 'id_tmp'))\n",
    "    \n",
    "    # We also need to spread the skewed data that matched\n",
    "    # evenly.  We will use a source of randomness for this\n",
    "    # but use a -1 for anything that still needs to be matched\n",
    "    if 'ordinal' in df.columns:\n",
    "        rand_column = col('ordinal')\n",
    "    else:\n",
    "        rand_column = col_of_rand_long()\n",
    "\n",
    "    df = df.withColumn('join_rand',\n",
    "            # null values are not in the model, they are filtered out\n",
    "            # but can be a source of skewedness so include them in\n",
    "            # the even distribution\n",
    "            when(col('id_tmp').isNotNull() | col(col_name).isNull(), rand_column)\n",
    "            .otherwise(lit(-1)))\n",
    "    \n",
    "    # Null out the string data that already matched to save memory\n",
    "    df = df.withColumn(col_name,\n",
    "            when(col('id_tmp').isNotNull(), None)\n",
    "            .otherwise(col(col_name)))\n",
    "    \n",
    "    # Now do the second join, which will be a non broadcast join.\n",
    "    # Sadly spark is too smart for its own good and will optimize out\n",
    "    # joining on a column it knows will always be a constant value.\n",
    "    # So we have to make a convoluted version of assigning a -1 to the\n",
    "    # randomness column for the model itself to work around that.\n",
    "    nb_model = (model\n",
    "            .withColumn('join_rand', when(col('model_count') < cutoff, lit(-1)).otherwise(lit(-2)))\n",
    "            .filter(col('model_count') < cutoff)\n",
    "            .withColumnRenamed('data', col_name)\n",
    "            .drop('model_count'))\n",
    "    \n",
    "    df = (df\n",
    "            .join(nb_model, ['join_rand', col_name], how='left')\n",
    "            .drop(col_name, 'join_rand')\n",
    "            # Pick either join result as an answer\n",
    "            .withColumn(col_name, coalesce(col('id'), col('id_tmp')))\n",
    "            .drop('id', 'id_tmp'))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_models(df, models, broadcast_model = False, skew_broadcast_pct = 1.0):\n",
    "    # sort the models so broadcast joins come first. This is\n",
    "    # so we reduce the amount of shuffle data sooner than later\n",
    "    # If we parsed the string hex values to ints early on this would\n",
    "    # not make a difference.\n",
    "    models = sorted(models, key=itemgetter(3), reverse=True)\n",
    "    for i, model, original_rows, would_broadcast in models:\n",
    "        col_name = '_c%d' % i\n",
    "        if not (would_broadcast or broadcast_model):\n",
    "            # The data is highly skewed so we need to offset that\n",
    "            cutoff = int(original_rows * skew_broadcast_pct/100.0)\n",
    "            df = skewed_join(df, model, col_name, cutoff)\n",
    "        else:\n",
    "            # broadcast joins can handle skewed data so no need to\n",
    "            # do anything special\n",
    "            model = (model.drop('model_count')\n",
    "                          .withColumnRenamed('data', col_name))\n",
    "            model = broadcast(model) if broadcast_model else model\n",
    "            df = (df\n",
    "                .join(model, col_name, how='left')\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed('id', col_name))\n",
    "    return df.fillna(0, ['_c%d' % i for i in CAT_COLS])\n",
    "\n",
    "\n",
    "def transform_log(df, transform_log = False):\n",
    "    cols = ['_c%d' % i for i in INT_COLS]\n",
    "    if transform_log:\n",
    "        for col_name in cols:\n",
    "            df = df.withColumn(col_name, log(df[col_name] + 3))\n",
    "    return df.fillna(0, cols)\n",
    "\n",
    "def would_broadcast(spark, str_path):\n",
    "    sc = spark.sparkContext\n",
    "    config = sc._jsc.hadoopConfiguration()\n",
    "    path = sc._jvm.org.apache.hadoop.fs.Path(str_path)\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(config)\n",
    "    stat = fs.listFiles(path, True)\n",
    "    sum = 0\n",
    "    while stat.hasNext():\n",
    "       sum = sum + stat.next().getLen()\n",
    "    sql_conf = sc._jvm.org.apache.spark.sql.internal.SQLConf()\n",
    "    cutoff = sql_conf.autoBroadcastJoinThreshold() * sql_conf.fileCompressionFactor()\n",
    "    return sum <= cutoff\n",
    "\n",
    "def delete_data_source(spark, path):\n",
    "    sc = spark.sparkContext\n",
    "    config = sc._jsc.hadoopConfiguration()\n",
    "    path = sc._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    sc._jvm.org.apache.hadoop.fs.FileSystem.get(config).delete(path, True)\n",
    "    \n",
    "def load_raw(spark, folder, day_range):\n",
    "    label_fields = [StructField('_c%d' % LABEL_COL, IntegerType())]\n",
    "    int_fields = [StructField('_c%d' % i, IntegerType()) for i in INT_COLS]\n",
    "    str_fields = [StructField('_c%d' % i, StringType()) for i in CAT_COLS]\n",
    "\n",
    "    schema = StructType(label_fields + int_fields + str_fields)\n",
    "    paths = [os.path.join(folder, 'day_%d' % i) for i in day_range]\n",
    "    return (spark\n",
    "        .read\n",
    "        .schema(schema)\n",
    "        .option('sep', '\\t')\n",
    "        .csv(paths))\n",
    "\n",
    "def rand_ordinal(df):\n",
    "    # create a random long from the double precision float.  \n",
    "    # The fraction part of a double is 52 bits, so we try to capture as much\n",
    "    # of that as possible\n",
    "    return df.withColumn('ordinal', col_of_rand_long())\n",
    "\n",
    "def day_from_ordinal(df, num_days):\n",
    "    return df.withColumn('day', (col('ordinal') % num_days).cast(IntegerType()))\n",
    "\n",
    "def day_from_input_file(df):\n",
    "    return df.withColumn('day', substring_index(input_file_name(), '_', -1).cast(IntegerType()))\n",
    "\n",
    "def psudo_sort_by_day_plus(spark, df, num_days):\n",
    "    # Sort is very expensive because it needs to calculate the partitions\n",
    "    # which in our case may involve rereading all of the data.  In some cases\n",
    "    # we can avoid this by repartitioning the data and sorting within a single partition\n",
    "    shuffle_parts = int(spark.conf.get('spark.sql.shuffle.partitions'))\n",
    "    extra_parts = int(shuffle_parts/num_days)\n",
    "    if extra_parts <= 0:\n",
    "        df = df.repartition('day')\n",
    "    else:\n",
    "        #We want to spread out the computation to about the same amount as shuffle_parts\n",
    "        divided = (col('ordinal') / num_days).cast(LongType())\n",
    "        extra_ident = divided % extra_parts\n",
    "        df = df.repartition(col('day'), extra_ident)\n",
    "    return df.sortWithinPartitions('day', 'ordinal')\n",
    "\n",
    "\n",
    "def load_combined_model(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'combined.parquet')\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "\n",
    "def save_combined_model(df, model_folder, mode=None):\n",
    "    path = os.path.join(model_folder, 'combined.parquet')\n",
    "    df.write.parquet(path, mode=mode)\n",
    "\n",
    "\n",
    "def delete_combined_model(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'combined.parquet')\n",
    "    delete_data_source(spark, path)\n",
    "\n",
    "\n",
    "def load_low_mem_partial_ids(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'partial_ids.parquet')\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "\n",
    "def save_low_mem_partial_ids(df, model_folder, mode=None):\n",
    "    path = os.path.join(model_folder, 'partial_ids.parquet')\n",
    "    df.write.parquet(path, mode=mode)\n",
    "\n",
    "\n",
    "def delete_low_mem_partial_ids(spark, model_folder):\n",
    "    path = os.path.join(model_folder, 'partial_ids.parquet')\n",
    "    delete_data_source(spark, path)\n",
    "\n",
    "\n",
    "def load_column_models(spark, model_folder, count_required):\n",
    "    for i in CAT_COLS:\n",
    "        path = os.path.join(model_folder, '%d.parquet' % i)\n",
    "        df = spark.read.parquet(path)\n",
    "        if count_required:\n",
    "            values = df.agg(sum('model_count').alias('sum'), count('*').alias('size')).collect()\n",
    "        else:\n",
    "            values = df.agg(sum('model_count').alias('sum')).collect()\n",
    "        yield i, df, values[0], would_broadcast(spark, path)\n",
    "\n",
    "def save_column_models(column_models, model_folder, mode=None):\n",
    "    for i, model in column_models:\n",
    "        path = os.path.join(model_folder, '%d.parquet' % i)\n",
    "        model.write.parquet(path, mode=mode)\n",
    "\n",
    "\n",
    "def save_model_size(model_size, path, write_mode):\n",
    "    if os.path.exists(path) and write_mode == 'errorifexists':\n",
    "        print('Error: model size file %s exists' % path)\n",
    "        sys.exit(1)\n",
    "\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
    "    with open(path, 'w') as fp:\n",
    "        json.dump(model_size, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_benchmark = {}\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def _timed(step):\n",
    "    start = time()\n",
    "    yield\n",
    "    end = time()\n",
    "    _benchmark[step] = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use ratio instead of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = 0\n",
    "INT_COLS = list(range(1, 14))\n",
    "CAT_COLS = list(range(14, 40))\n",
    "LABEL_COL_NAME=\"_c0\"\n",
    "def get_column_counts_with_frequency_limit(df, frequency_limit):\n",
    "    cols = ['_c%d' % i for i in CAT_COLS]\n",
    "    \n",
    "    df = (df.select(posexplode(array(*cols)), LABEL_COL_NAME)\n",
    "          .withColumnRenamed('pos', 'column_id')\n",
    "          .withColumnRenamed('col', 'data')\n",
    "          .filter('data is not null')\n",
    "          .groupBy('column_id', 'data')\n",
    "          .agg(sum(\"_c0\").alias(\"num_pos\"),\n",
    "               count('*').alias(\"count\"))\n",
    "          .withColumn('ratio',(col('num_pos') / col('count')).cast('float'))\n",
    "          .drop('num_pos'))\n",
    "    \n",
    "    default_limit = int(frequency_limit)\n",
    "    df = df.filter((col('count') > default_limit) | (col('count') == default_limit))\n",
    "    df = df.drop('count').withColumnRenamed('ratio', 'count')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate models\n",
      "====================================================================================================\n",
      "{'generate models': 11.397159337997437}\n",
      "transform\n",
      "writing final output\n",
      "====================================================================================================\n",
      "{'generate models': 11.397159337997437, 'transform': 4.923393726348877}\n"
     ]
    }
   ],
   "source": [
    "def _main(spark, mode):\n",
    "    write_mode = \"overwrite\"\n",
    "    # You need to update them to your real paths!\n",
    "    dataRoot = os.getenv(\"DATA_ROOT\", \"/data\")\n",
    "    input_folder = dataRoot + \"/criteo/model_test/input\"\n",
    "    model_folder = dataRoot + \"/criteo/model_test/model\"\n",
    "    output_folder = dataRoot + \"/criteo/model_test/output\"\n",
    "    \n",
    "    # this range is the input data range, like day_0, day_1, .... , start with 1.\n",
    "    # we use 5 days datasets as an example, user can change from 1 to 23 days datasets instead.\n",
    "    day_range = 5\n",
    "    dict_build_shuffle_parallel_per_day = 2\n",
    "    apply_shuffle_parallel_per_day = 25\n",
    "    \n",
    "    origin_df = load_raw(spark, input_folder, range(day_range))\n",
    "    \n",
    "    if mode == 'generate models':\n",
    "        spark.conf.set('spark.sql.shuffle.partitions', day_range*dict_build_shuffle_parallel_per_day)\n",
    "        with _timed('generate models'):\n",
    "            print('generate models')\n",
    "            col_counts = get_column_counts_with_frequency_limit(origin_df, 100)\n",
    "            # in low memory mode we have to save an intermediate result\n",
    "            # because if we try to do it in one query spark ends up assigning the\n",
    "            # partial ids in two different locations that are not guaranteed to line up\n",
    "            # this prevents that from happening by assigning the partial ids\n",
    "            # and then writeing them out.\n",
    "            save_low_mem_partial_ids(\n",
    "                    assign_low_mem_partial_ids(col_counts),\n",
    "                    model_folder,\n",
    "                    write_mode)\n",
    "            save_combined_model(\n",
    "                    assign_low_mem_final_ids(load_low_mem_partial_ids(spark, model_folder)),\n",
    "                    model_folder,\n",
    "                    write_mode)\n",
    "            save_column_models(\n",
    "                get_column_models(load_combined_model(spark, model_folder)),\n",
    "                model_folder,\n",
    "                write_mode)\n",
    "\n",
    "        \n",
    "    if mode == 'transform':\n",
    "        spark.conf.set('spark.sql.shuffle.partitions', day_range*apply_shuffle_parallel_per_day)\n",
    "        with _timed('transform'):\n",
    "            print(\"transform\")\n",
    "            origin_df = origin_df.withColumn('ordinal', monotonically_increasing_id())\n",
    "\n",
    "            models = list(load_column_models(spark, model_folder, False))\n",
    "            models = [(i, df, agg.sum, flag) for i, df, agg, flag in models]\n",
    "          \n",
    "            df = apply_models(origin_df,models,False,1.0)\n",
    "            df = transform_log(df, False)\n",
    "            df = df.orderBy('ordinal')\n",
    "            df = df.drop('ordinal')\n",
    "            df = df.drop('day')\n",
    "\n",
    "            print(\"writing final output\")\n",
    "            df.write.parquet(\n",
    "                output_folder,\n",
    "                mode=write_mode)\n",
    "\n",
    "    print('=' * 100)\n",
    "    print(_benchmark)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    conf = SparkConf().setAppName('ETL')\\\n",
    "            .set('spark.executor.memory', '20g')\\\n",
    "            .set('spark.executor.resource.gpu.amount', 1)\\\n",
    "            .set('spark.executor.cores', 2)\\\n",
    "            .set('spark.cores.max', 2)\\\n",
    "            .set('spark.task.cpus', 1)\\\n",
    "            .set('spark.task.resource.gpu.amount', 0.5)\\\n",
    "            .set('spark.rapids.sql.concurrentGpuTasks', 1)\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    # generate model for cat columns\n",
    "    _main(spark, \"generate models\")\n",
    "    # transform data with model and save to new parquet\n",
    "    _main(spark, \"transform\")\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
