{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage CrossValidation with GPU accelerating on XGBoost\n",
    "\n",
    "In this notebook, we will show you how to levarage GPU to accelerate mortgage CrossValidation of XGBoost to find out the best model given a group of parameters.\n",
    "\n",
    "## Import classes\n",
    "First we need load some common classes that both GPU version and CPU version will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml.dmlc.xgboost4j.scala.spark.{XGBoostClassificationModel, XGBoostClassifier}\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder,CrossValidator}\n",
    "import org.apache.spark.sql.types.{FloatType, IntegerType, StructField, StructType, DoubleType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is new to xgboost-spark users is **rapids.CrossValidator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// You need to update them to your real paths!\n",
    "val dataRoot = sys.env.getOrElse(\"DATA_ROOT\", \"/data\")\n",
    "val trainParquetPath=dataRoot + \"/mortgage/train\"\n",
    "val evalParquetPath=dataRoot + \"/mortgage/eval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val labelColName = \"delinquency_12\"\n",
    "val schema = StructType(List(\n",
    "    StructField(\"orig_channel\", FloatType),\n",
    "    StructField(\"first_home_buyer\", FloatType),\n",
    "    StructField(\"loan_purpose\", FloatType),\n",
    "    StructField(\"property_type\", FloatType),\n",
    "    StructField(\"occupancy_status\", FloatType),\n",
    "    StructField(\"property_state\", FloatType),\n",
    "    StructField(\"product_type\", FloatType),\n",
    "    StructField(\"relocation_mortgage_indicator\", FloatType),\n",
    "    StructField(\"seller_name\", FloatType),\n",
    "    StructField(\"mod_flag\", FloatType),\n",
    "    StructField(\"orig_interest_rate\", FloatType),\n",
    "    StructField(\"orig_upb\", DoubleType),\n",
    "    StructField(\"orig_loan_term\", IntegerType),\n",
    "    StructField(\"orig_ltv\", FloatType),\n",
    "    StructField(\"orig_cltv\", FloatType),\n",
    "    StructField(\"num_borrowers\", FloatType),\n",
    "    StructField(\"dti\", FloatType),\n",
    "    StructField(\"borrower_credit_score\", FloatType),\n",
    "    StructField(\"num_units\", IntegerType),\n",
    "    StructField(\"zip\", IntegerType),\n",
    "    StructField(\"mortgage_insurance_percent\", FloatType),\n",
    "    StructField(\"current_loan_delinquency_status\", IntegerType),\n",
    "    StructField(\"current_actual_upb\", FloatType),\n",
    "    StructField(\"interest_rate\", FloatType),\n",
    "    StructField(\"loan_age\", FloatType),\n",
    "    StructField(\"msa\", FloatType),\n",
    "    StructField(\"non_interest_bearing_upb\", FloatType),\n",
    "    StructField(labelColName, IntegerType)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new spark session and load data\n",
    "we must create a new spark session to continue all spark operations.\n",
    "\n",
    "NOTE: in this notebook, we have uploaded dependency jars when installing toree kernel. If we don't upload them at installation time, we can also upload in notebook by [%AddJar magic](https://toree.incubator.apache.org/docs/current/user/faq/). However, there's one restriction for `%AddJar`: the jar uploaded can only be available when `AddJar` is called after a new spark session is created. We must use it as below:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"mortgage-gpu-cv\").getOrCreate\n",
    "%AddJar file:/data/libs/xgboost4j-spark-gpu_2.12-XXX.jar\n",
    "%AddJar file:/data/libs/xgboost4j-gpu_2.12-XXX.jar\n",
    "// ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@51af6ff3\n",
       "trainDs = [orig_channel: double, first_home_buyer: double ... 26 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[orig_channel: double, first_home_buyer: double ... 26 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"mortgage-gpu-cv\").getOrCreate()\n",
    "val trainDs = spark.read.parquet(trainParquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out features to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureNames = Array(orig_channel, first_home_buyer, loan_purpose, property_type, occupancy_status, property_state, product_type, relocation_mortgage_indicator, seller_name, mod_flag, orig_interest_rate, orig_upb, orig_loan_term, orig_ltv, orig_cltv, num_borrowers, dti, borrower_credit_score, num_units, zip, mortgage_insurance_percent, current_loan_delinquency_status, current_actual_upb, interest_rate, loan_age, msa, non_interest_bearing_upb)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(orig_channel, first_home_buyer, loan_purpose, property_type, occupancy_status, property_state, product_type, relocation_mortgage_indicator, seller_name, mod_flag, orig_interest_rate, orig_upb, orig_loan_term, orig_ltv, orig_cltv, num_borrowers, dti, borrower_credit_score, num_units, zip, mortgage_insurance_percent, current_loan_delinquency_status, current_actual_upb, interest_rate, loan_age, msa, non_interest_bearing_upb)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureNames = schema.filter(_.name != labelColName).map(_.name).toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifierParam = Map(objective -> binary:logistic, num_round -> 100, num_workers -> 1, tree_method -> gpu_hist)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(objective -> binary:logistic, num_round -> 100, num_workers -> 1, tree_method -> gpu_hist)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifierParam = Map(\n",
    "    \"objective\" -> \"binary:logistic\",\n",
    "    \"num_round\" -> 100,\n",
    "    \"num_workers\" -> 1,\n",
    "    \"tree_method\" -> \"gpu_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier = xgbc_ae8896ab2b67\n",
       "paramGrid = \n",
       "evaluator = MulticlassClassificationEvaluator: uid=mcEval_ebda5b6cea6c, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n",
       "cv = cv_cb7d8efe9ab5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\txgbc_ae8896ab2b67-eta: 0.2,\n",
       "\txgbc_ae8896ab2b67-maxDepth: 3\n",
       "}, {\n",
       "\txgbc_ae8896ab2b67-eta: 0.2,\n",
       "\txgbc_ae8896ab2b67-maxDepth: 10\n",
       "}, {\n",
       "\txgbc_ae8896ab2b67-eta: 0.6,\n",
       "\txgbc_ae8896ab2b67-maxDepth: 3\n",
       "}, {\n",
       "\txgbc_ae8896ab2b67-eta: 0.6,\n",
       "\txgbc_ae8896ab2b67-maxDepth: 10\n",
       "})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cv_cb7d8efe9ab5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifier = new XGBoostClassifier(classifierParam)\n",
    "    .setLabelCol(labelColName)\n",
    "    .setFeaturesCol(featureNames)\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(classifier.maxDepth, Array(3, 10))\n",
    "    .addGrid(classifier.eta, Array(0.2, 0.6))\n",
    "    .build()\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(labelColName)\n",
    "val cv = new CrossValidator()\n",
    "    .setEstimator(classifier)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setNumFolds(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=41609, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=45469, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=52795, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=53483, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=58067, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=43717, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=36075, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=53851, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=42227, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=46587, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=51295, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=54695, DMLC_NUM_WORKER=1}\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.210, DMLC_TRACKER_PORT=54019, DMLC_NUM_WORKER=1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model = xgbc_ae8896ab2b67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "xgbc_ae8896ab2b67"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = cv.fit(trainDs).bestModel.asInstanceOf[XGBoostClassificationModel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tranform with best model trained by CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformDs = [orig_channel: double, first_home_buyer: double ... 26 more fields]\n",
       "df = [orig_channel: double, first_home_buyer: double ... 29 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+----------+\n",
      "|delinquency_12|       rawPrediction|         probability|prediction|\n",
      "+--------------+--------------------+--------------------+----------+\n",
      "|             0|[17.3849449157714...|[0.99999997182821...|       0.0|\n",
      "|             0|[16.6074829101562...|[0.99999993869981...|       0.0|\n",
      "|             0|[16.0062618255615...|[0.99999988816731...|       0.0|\n",
      "|             0|[16.7623615264892...|[0.99999994749521...|       0.0|\n",
      "|             0|[15.1363153457641...|[0.99999973307967...|       0.0|\n",
      "+--------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[orig_channel: double, first_home_buyer: double ... 29 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transformDs = spark.read.parquet(evalParquetPath)\n",
    "val df = model.transform(transformDs).cache()\n",
    "df.drop(featureNames: _*).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = MulticlassClassificationEvaluator: uid=mcEval_d880c25944f1, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n",
       "accuracy = 1.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(labelColName)\n",
    "val accuracy = evaluator.evaluate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XGBoost4j-Spark - Scala",
   "language": "scala",
   "name": "XGBoost4j-Spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
