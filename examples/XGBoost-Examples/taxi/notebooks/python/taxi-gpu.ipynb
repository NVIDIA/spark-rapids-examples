{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to XGBoost Spark3.0 with GPU\n",
    "\n",
    "Taxi is an example of xgboost regressor. This notebook will show you how to load data, train the xgboost model and use this model to predict \"fare_amount\" of your taxi trip.\n",
    "\n",
    "A few libraries required for this notebook:\n",
    "  1. NumPy\n",
    "  2. cudf jar\n",
    "  3. xgboost4j jar\n",
    "  4. xgboost4j-spark jar\n",
    "  5. rapids-4-spark.jar  \n",
    "\n",
    "This notebook also illustrates the ease of porting a sample CPU based Spark xgboost4j code into GPU. There is only one change required for running Spark XGBoost on GPU. That is replacing the API `setFeaturesCol(feature)` on CPU with the new API `setFeaturesCols(features)`. This also eliminates the need for vectorization (assembling multiple feature columns in to one column) since we can read multiple columns.\n",
    "\n",
    "Note: For PySpark based XGBoost, please refer to the [Spark-RAPIDS-examples 22.04 branch](https://github.com/NVIDIA/spark-rapids-examples/tree/branch-22.04) that\n",
    "uses [NVIDIAâ€™s Spark XGBoost version](https://repo1.maven.org/maven2/com/nvidia/xgboost4j-spark_3.0/1.4.2-0.3.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.dmlc.xgboost4j.scala.spark import XGBoostRegressionModel, XGBoostRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, IntegerType, StructField, StructType\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides CPU version requires two extra libraries.\n",
    "```Python\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Session and Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "reader = spark.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Data Schema and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'fare_amount'\n",
    "schema = StructType([\n",
    "    StructField('vendor_id', FloatType()),\n",
    "    StructField('passenger_count', FloatType()),\n",
    "    StructField('trip_distance', FloatType()),\n",
    "    StructField('pickup_longitude', FloatType()),\n",
    "    StructField('pickup_latitude', FloatType()),\n",
    "    StructField('rate_code', FloatType()),\n",
    "    StructField('store_and_fwd', FloatType()),\n",
    "    StructField('dropoff_longitude', FloatType()),\n",
    "    StructField('dropoff_latitude', FloatType()),\n",
    "    StructField(label, FloatType()),\n",
    "    StructField('hour', FloatType()),\n",
    "    StructField('year', IntegerType()),\n",
    "    StructField('month', IntegerType()),\n",
    "    StructField('day', FloatType()),\n",
    "    StructField('day_of_week', FloatType()),\n",
    "    StructField('is_weekend', FloatType()),\n",
    "])\n",
    "features = [ x.name for x in schema if x.name != label ]\n",
    "\n",
    "# You need to update them to your real paths!\n",
    "dataRoot = os.getenv(\"DATA_ROOT\", \"/data\")\n",
    "train_data = reader.schema(schema).option('header', True).csv(dataRoot + '/taxi/csv/train')\n",
    "trans_data  = reader.schema(schema).option('header', True).csv(dataRoot + '/taxi/csv/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on CPU version, vectorization is required before fitting data to regressor, which means you need to assemble all feature columns into one column.\n",
    "\n",
    "```Python\n",
    "def vectorize(data_frame):\n",
    "    to_floats = [ col(x.name).cast(FloatType()) for x in data_frame.schema ]\n",
    "    return (VectorAssembler()\n",
    "        .setInputCols(features)\n",
    "        .setOutputCol('features')\n",
    "        .transform(data_frame.select(to_floats))\n",
    "        .select(col('features'), col(label)))\n",
    "\n",
    "train_data = vectorize(train_data)\n",
    "trans_data = vectorize(trans_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \n",
    "    'eta': 0.05,\n",
    "    'treeMethod': 'gpu_hist',\n",
    "    'maxDepth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'gamma': 1.0,\n",
    "    'numRound': 100,\n",
    "    'numWorkers': 1,\n",
    "}\n",
    "regressor = XGBoostRegressor(**params).setLabelCol(label).setFeaturesCols(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CPU version regressor provides the API `setFeaturesCol` which only accepts a single column name, so vectorization for multiple feature columns is required.\n",
    "```Python\n",
    "regressor = XGBoostRegressor(**params).setLabelCol(label).setFeaturesCol('features')\n",
    "```\n",
    "\n",
    "The parameter `num_workers` should be set to the number of GPUs in Spark cluster for GPU version, while for CPU version it is usually equal to the number of the CPU cores.\n",
    "\n",
    "Concerning the tree method, GPU version only supports `gpu_hist` currently, while `hist` is designed and used here for CPU training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Data with Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training takes 17.73 seconds\n"
     ]
    }
   ],
   "source": [
    "def with_benchmark(phrase, action):\n",
    "    start = time()\n",
    "    result = action()\n",
    "    end = time()\n",
    "    print('{} takes {} seconds'.format(phrase, round(end - start, 2)))\n",
    "    return result\n",
    "model = with_benchmark('Training', lambda: regressor.fit(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Reload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(dataRoot + '/new-model-path')\n",
    "loaded_model = XGBoostRegressionModel().load(dataRoot + '/new-model-path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation and Show Result Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation takes 2.55 seconds\n",
      "+------------+---------------+-------------+-----------+------------------+\n",
      "|   vendor_id|passenger_count|trip_distance|fare_amount|        prediction|\n",
      "+------------+---------------+-------------+-----------+------------------+\n",
      "|1.55973043E9|            1.0|          1.1|        6.2| 5.670516490936279|\n",
      "|1.55973043E9|            4.0|          2.7|        9.4|10.054250717163086|\n",
      "|1.55973043E9|            1.0|          1.5|        6.1|  7.01417350769043|\n",
      "|1.55973043E9|            1.0|          4.1|       12.6|14.309316635131836|\n",
      "|1.55973043E9|            1.0|          4.6|       13.4|13.990922927856445|\n",
      "+------------+---------------+-------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform():\n",
    "    result = loaded_model.transform(trans_data).cache()\n",
    "    result.foreachPartition(lambda _: None)\n",
    "    return result\n",
    "result = with_benchmark('Transformation', transform)\n",
    "result.select('vendor_id', 'passenger_count', 'trip_distance', label, 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on CPU version: You cannot `select` the feature columns after vectorization. So please use `result.show(5)` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation takes 0.45 seconds\n",
      "RMSE is 3.3195416959403032\n"
     ]
    }
   ],
   "source": [
    "accuracy = with_benchmark(\n",
    "    'Evaluation',\n",
    "    lambda: RegressionEvaluator().setLabelCol(label).evaluate(result))\n",
    "print('RMSE is ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
