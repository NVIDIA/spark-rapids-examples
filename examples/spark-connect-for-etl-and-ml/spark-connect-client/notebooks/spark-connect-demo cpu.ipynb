{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Spark Connect - SQL/DF ETL and MLlib on Mortgage Dataset (Spark 4.0+)\n",
    "\n",
    "Based on the Data and AI Summit 2025 session: [GPU Accelerated Spark Connect](https://www.databricks.com/dataaisummit/session/gpu-accelerated-spark-connect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, FeatureHasher\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark via Spark Connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Connect session id: 0711ae8f-945f-4287-9652-96b8d63f540a\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Create GPU-accelerated Spark session using Spark Connect 4.0+\n",
    "spark = (\n",
    "  SparkSession.builder\n",
    "    .remote('sc://spark-connect-server')\n",
    "    .appName('GPU-Accelerated-ETL-ML-Demo') \n",
    "    .getOrCreate()\n",
    ")\n",
    "print(f'Spark Connect session id: {spark.session_id}')\n",
    "print(f'Spark version: {spark.version}')\n",
    "# workaround for repeated executions\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|mod10|  count(1)|\n",
      "+-----+----------+\n",
      "|    0|3435973837|\n",
      "|    1|3435973837|\n",
      "|    2|3435973837|\n",
      "|    3|3435973837|\n",
      "|    4|3435973837|\n",
      "|    5|3435973837|\n",
      "|    6|3435973837|\n",
      "|    7|3435973837|\n",
      "|    8|3435973836|\n",
      "|    9|3435973836|\n",
      "+-----+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['mod10 ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['mod10], ['mod10, unresolvedalias('count(*))]\n",
      "   +- 'Project [unresolvedstarwithcolumns(mod10, '`%`('id, 10), Some(List({})))]\n",
      "      +- Range (0, 34359738368, step=1, splits=Some(64))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mod10: bigint, count(1): bigint\n",
      "Sort [mod10#77336L ASC NULLS FIRST], true\n",
      "+- Aggregate [mod10#77336L], [mod10#77336L, count(1) AS count(1)#77338L]\n",
      "   +- Project [id#70399L, (id#70399L % cast(10 as bigint)) AS mod10#77336L]\n",
      "      +- Range (0, 34359738368, step=1, splits=Some(64))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [mod10#77336L ASC NULLS FIRST], true\n",
      "+- Aggregate [mod10#77336L], [mod10#77336L, count(1) AS count(1)#77338L]\n",
      "   +- Project [(id#70399L % 10) AS mod10#77336L]\n",
      "      +- Range (0, 34359738368, step=1, splits=Some(64))\n",
      "\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow false\n",
      "+- GpuSort [mod10#77336L ASC NULLS FIRST], true, com.nvidia.spark.rapids.OutOfCoreSort$@5f6846c4\n",
      "   +- GpuShuffleCoalesce 536870912\n",
      "      +- GpuColumnarExchange gpurangepartitioning(mod10#77336L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=48077]\n",
      "         +- GpuHashAggregate (keys=[mod10#77336L], functions=[gpucount(1, false)], output=[mod10#77336L, count(1)#77338L]) \n",
      "            +- GpuShuffleCoalesce 536870912\n",
      "               +- GpuColumnarExchange gpuhashpartitioning(mod10#77336L, 200, Murmur3Mode), ENSURE_REQUIREMENTS, [plan_id=48060]\n",
      "                  +- GpuHashAggregate (keys=[mod10#77336L], functions=[partial_gpucount(1, false)], output=[mod10#77336L, count#77362L]) \n",
      "                     +- GpuProject [(id#70399L % 10) AS mod10#77336L], true\n",
      "                        +- GpuRange (0, 34359738368, step=1, splits=64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "  spark.range(2 ** 35)\n",
    "    .withColumn('mod10', col('id') % lit(10))\n",
    "    .groupBy('mod10').agg(count('*'))\n",
    "    .orderBy('mod10')\n",
    ")\n",
    "df.show()\n",
    "# workaround to get a plan with GpuOverrides applied by disabling adaptive execution\n",
    "spark.conf.set('spark.sql.adaptive.enabled', False)\n",
    "df.explain(mode='extended')\n",
    "spark.conf.set('spark.sql.adaptive.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Should GPU Be Used from the next cell on?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_accelerate_on_gpu = False\n",
    "accelerate_on_gpu = input(f\"USE GPU? (y/n) (default: {default_accelerate_on_gpu})\")\n",
    "if not accelerate_on_gpu:\n",
    "  accelerate_on_gpu = default_accelerate_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled', accelerate_on_gpu)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerate_on_gpu:\n",
    "  spark.conf.set('spark.connect.ml.backend.classes', 'com.nvidia.rapids.ml.Plugin')\n",
    "else:\n",
    "  spark.conf.unset('spark.connect.ml.backend.classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize references to the same bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+--------------+\n",
      "|from_seller_name                                      |to_seller_name|\n",
      "+------------------------------------------------------+--------------+\n",
      "|WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015|Wells Fargo   |\n",
      "|WELLS FARGO BANK,  NA                                 |Wells Fargo   |\n",
      "|WELLS FARGO BANK, N.A.                                |Wells Fargo   |\n",
      "|WELLS FARGO BANK, NA                                  |Wells Fargo   |\n",
      "+------------------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/home/jovyan/work/name_mapping.csv', 'r') as name_mapping_file:\n",
    "  nm_reader = csv.reader(name_mapping_file,)\n",
    "  name_mapping = [r for r in nm_reader]\n",
    "name_mapping_df = spark.createDataFrame(name_mapping, ['from_seller_name', 'to_seller_name'])\n",
    "\n",
    "(\n",
    "  name_mapping_df\n",
    "    .where(col('to_seller_name') == 'Wells Fargo' )\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String columns\n",
    "cate_col_names = [\n",
    "  'orig_channel',\n",
    "  'first_home_buyer',\n",
    "  'loan_purpose',\n",
    "  'property_type',\n",
    "  'occupancy_status',\n",
    "  'property_state',\n",
    "  'product_type',\n",
    "  'relocation_mortgage_indicator',\n",
    "  'seller_name',\n",
    "  'mod_flag'\n",
    "]\n",
    "# Numeric columns\n",
    "label_col_name = 'delinquency_12'\n",
    "numeric_col_names = [\n",
    "  'orig_interest_rate',\n",
    "  'orig_upb',\n",
    "  'orig_loan_term',\n",
    "  'orig_ltv',\n",
    "  'orig_cltv',\n",
    "  'num_borrowers',\n",
    "  'dti',\n",
    "  'borrower_credit_score',\n",
    "  'num_units',\n",
    "  'zip',\n",
    "  'mortgage_insurance_percent',\n",
    "  'current_loan_delinquency_status',\n",
    "  'current_actual_upb',\n",
    "  'interest_rate',\n",
    "  'loan_age',\n",
    "  'msa',\n",
    "  'non_interest_bearing_upb',\n",
    "  label_col_name\n",
    "]\n",
    "all_col_names = cate_col_names + numeric_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read raw columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_csv(spark, path):\n",
    "  def _get_quarter_from_csv_file_name():\n",
    "    return substring_index(substring_index(input_file_name(), '.', 1), '/', -1)\n",
    "\n",
    "  with open('/home/jovyan/work/csv_raw_schema.ddl', 'r') as f:\n",
    "    _csv_raw_schema_str = f.read()\n",
    "  \n",
    "  return (\n",
    "    spark.read\n",
    "    .format('csv') \n",
    "    .option('nullValue', '') \n",
    "    .option('header', False) \n",
    "    .option('delimiter', '|') \n",
    "    .schema(_csv_raw_schema_str) \n",
    "    .load(path) \n",
    "    .withColumn('quarter', _get_quarter_from_csv_file_name())\n",
    "  )\n",
    "\n",
    "def extract_perf_columns(rawDf):\n",
    "  perfDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    date_format(to_date(col('monthly_reporting_period'),'MMyyyy'), 'MM/dd/yyyy').alias('monthly_reporting_period'),\n",
    "    upper(col('servicer')).alias('servicer'),\n",
    "    col('interest_rate'),\n",
    "    col('current_actual_upb'),\n",
    "    col('loan_age'),\n",
    "    col('remaining_months_to_legal_maturity'),\n",
    "    col('adj_remaining_months_to_maturity'),\n",
    "    date_format(to_date(col('maturity_date'),'MMyyyy'), 'MM/yyyy').alias('maturity_date'),\n",
    "    col('msa'),\n",
    "    col('current_loan_delinquency_status'),\n",
    "    col('mod_flag'),\n",
    "    col('zero_balance_code'),\n",
    "    date_format(to_date(col('zero_balance_effective_date'),'MMyyyy'), 'MM/yyyy').alias('zero_balance_effective_date'),\n",
    "    date_format(to_date(col('last_paid_installment_date'),'MMyyyy'), 'MM/dd/yyyy').alias('last_paid_installment_date'),\n",
    "    date_format(to_date(col('foreclosed_after'),'MMyyyy'), 'MM/dd/yyyy').alias('foreclosed_after'),\n",
    "    date_format(to_date(col('disposition_date'),'MMyyyy'), 'MM/dd/yyyy').alias('disposition_date'),\n",
    "    col('foreclosure_costs'),\n",
    "    col('prop_preservation_and_repair_costs'),\n",
    "    col('asset_recovery_costs'),\n",
    "    col('misc_holding_expenses'),\n",
    "    col('holding_taxes'),\n",
    "    col('net_sale_proceeds'),\n",
    "    col('credit_enhancement_proceeds'),\n",
    "    col('repurchase_make_whole_proceeds'),\n",
    "    col('other_foreclosure_proceeds'),\n",
    "    col('non_interest_bearing_upb'),\n",
    "    col('principal_forgiveness_upb'),\n",
    "    col('repurchase_make_whole_proceeds_flag'),\n",
    "    col('foreclosure_principal_write_off_amount'),\n",
    "    col('servicing_activity_indicator'),\n",
    "    col('quarter')\n",
    "  )\n",
    "  return perfDf.select('*').filter('current_actual_upb != 0.0')\n",
    "\n",
    "def extract_acq_columns(rawDf):\n",
    "  acqDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    col('orig_channel'),\n",
    "    upper(col('seller_name')).alias('seller_name'),\n",
    "    col('orig_interest_rate'),\n",
    "    col('orig_upb'),\n",
    "    col('orig_loan_term'),\n",
    "    date_format(to_date(col('orig_date'),'MMyyyy'), 'MM/yyyy').alias('orig_date'),\n",
    "    date_format(to_date(col('first_pay_date'),'MMyyyy'), 'MM/yyyy').alias('first_pay_date'),\n",
    "    col('orig_ltv'),\n",
    "    col('orig_cltv'),\n",
    "    col('num_borrowers'),\n",
    "    col('dti'),\n",
    "    col('borrower_credit_score'),\n",
    "    col('first_home_buyer'),\n",
    "    col('loan_purpose'),\n",
    "    col('property_type'),\n",
    "    col('num_units'),\n",
    "    col('occupancy_status'),\n",
    "    col('property_state'),\n",
    "    col('zip'),\n",
    "    col('mortgage_insurance_percent'),\n",
    "    col('product_type'),\n",
    "    col('coborrow_credit_score'),\n",
    "    col('mortgage_insurance_type'),\n",
    "    col('relocation_mortgage_indicator'),\n",
    "    dense_rank().over(Window.partitionBy('loan_id').orderBy(to_date(col('monthly_reporting_period'),'MMyyyy'))).alias('rank'),\n",
    "    col('quarter')\n",
    "  )\n",
    "\n",
    "  return acqDf.select('*').filter(col('rank')==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to parse date in Performance data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_dates(perf):\n",
    "  return (\n",
    "    perf.withColumn('monthly_reporting_period', to_date(col('monthly_reporting_period'), 'MM/dd/yyyy')) \n",
    "      .withColumn('monthly_reporting_period_month', month(col('monthly_reporting_period'))) \n",
    "      .withColumn('monthly_reporting_period_year', year(col('monthly_reporting_period'))) \n",
    "      .withColumn('monthly_reporting_period_day', dayofmonth(col('monthly_reporting_period'))) \n",
    "      .withColumn('last_paid_installment_date', to_date(col('last_paid_installment_date'), 'MM/dd/yyyy')) \n",
    "      .withColumn('foreclosed_after', to_date(col('foreclosed_after'), 'MM/dd/yyyy')) \n",
    "      .withColumn('disposition_date', to_date(col('disposition_date'), 'MM/dd/yyyy')) \n",
    "      .withColumn('maturity_date', to_date(col('maturity_date'), 'MM/yyyy')) \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create deliquency data frame from Performance data.  \n",
    "\n",
    "The computed `delinquency_12` column denotes whether a loan will become delinquent by 3, 6, or 9 months, \n",
    "or not delinquent, within the next 12 month period.   \n",
    "\n",
    "It will be the target label for ML multi-class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_perf_deliquency(spark, perf):\n",
    "  aggDF = (\n",
    "    perf\n",
    "      .select(\n",
    "        col('quarter'),\n",
    "        col('loan_id'),\n",
    "        col('current_loan_delinquency_status'),\n",
    "        when(col('current_loan_delinquency_status') >= 1, col('monthly_reporting_period')).alias('delinquency_30'),\n",
    "        when(col('current_loan_delinquency_status') >= 3, col('monthly_reporting_period')).alias('delinquency_90'),\n",
    "        when(col('current_loan_delinquency_status') >= 6, col('monthly_reporting_period')).alias('delinquency_180')\n",
    "      ).groupBy('quarter', 'loan_id')\n",
    "       .agg(\n",
    "         max('current_loan_delinquency_status').alias('delinquency_12'),\n",
    "         min('delinquency_30').alias('delinquency_30'),\n",
    "         min('delinquency_90').alias('delinquency_90'),\n",
    "         min('delinquency_180').alias('delinquency_180')\n",
    "       ).select(\n",
    "         col('quarter'),\n",
    "         col('loan_id'),\n",
    "         (col('delinquency_12') >= 1).alias('ever_30'),\n",
    "         (col('delinquency_12') >= 3).alias('ever_90'),\n",
    "         (col('delinquency_12') >= 6).alias('ever_180'),\n",
    "         col('delinquency_30'),\n",
    "         col('delinquency_90'),\n",
    "         col('delinquency_180')\n",
    "       )\n",
    "  )\n",
    "  #aggDF.printSchema()\n",
    "  joinedDf = (\n",
    "    perf\n",
    "      .withColumnRenamed('monthly_reporting_period', 'timestamp')\n",
    "      .withColumnRenamed('monthly_reporting_period_month', 'timestamp_month') \n",
    "      .withColumnRenamed('monthly_reporting_period_year', 'timestamp_year') \n",
    "      .withColumnRenamed('current_loan_delinquency_status', 'delinquency_12') \n",
    "      .withColumnRenamed('current_actual_upb', 'upb_12') \n",
    "      .select('quarter', 'loan_id', 'timestamp', 'delinquency_12', 'upb_12', 'timestamp_month', 'timestamp_year') \n",
    "      .join(aggDF, ['loan_id', 'quarter'], 'left_outer')\n",
    "  )\n",
    "  # calculate the 12 month delinquency and upb values\n",
    "  months = 12\n",
    "  monthArray = [lit(x) for x in range(0, 12)]\n",
    "  \n",
    "  testDf = ( \n",
    "    joinedDf\n",
    "      .withColumn('month_y', explode(array(monthArray)))\n",
    "      .select(\n",
    "        col('quarter'),\n",
    "        floor(((col('timestamp_year') * 12 + col('timestamp_month')) - 24000) / months).alias('josh_mody'),\n",
    "        floor(((col('timestamp_year') * 12 + col('timestamp_month')) - 24000 - col('month_y')) / months).alias('josh_mody_n'),\n",
    "        col('ever_30'),\n",
    "        col('ever_90'),\n",
    "        col('ever_180'),\n",
    "        col('delinquency_30'),\n",
    "        col('delinquency_90'),\n",
    "        col('delinquency_180'),\n",
    "        col('loan_id'),\n",
    "        col('month_y'),\n",
    "        col('delinquency_12'),\n",
    "        col('upb_12')\n",
    "      ).groupBy('quarter', 'loan_id', 'josh_mody_n', 'ever_30', 'ever_90', 'ever_180', 'delinquency_30', 'delinquency_90', 'delinquency_180', 'month_y')\n",
    "    .agg(max('delinquency_12').alias('delinquency_12'), min('upb_12').alias('upb_12')) \n",
    "    .withColumn('timestamp_year', floor((lit(24000) + (col('josh_mody_n') * lit(months)) + (col('month_y') - 1)) / lit(12))) \n",
    "    .selectExpr('*', f'pmod(24000 + (josh_mody_n * {months}) + month_y, 12) as timestamp_month_tmp') \n",
    "    .withColumn('timestamp_month', when(col('timestamp_month_tmp') == lit(0), lit(12)).otherwise(col('timestamp_month_tmp'))) \n",
    "    .withColumn('delinquency_12', ((col('delinquency_12') > 9).cast('int') + (col('delinquency_12') > 6).cast('int') + (col('delinquency_12') > 3).cast('int') + (col('upb_12') == 0).cast('int')).alias('delinquency_12')) \n",
    "    .drop('timestamp_month_tmp', 'josh_mody_n', 'month_y')\n",
    "  )\n",
    "\n",
    "  return (\n",
    "    perf\n",
    "      .withColumnRenamed('monthly_reporting_period_month', 'timestamp_month')\n",
    "      .withColumnRenamed('monthly_reporting_period_year', 'timestamp_year')\n",
    "      .join(testDf, ['quarter', 'loan_id', 'timestamp_year', 'timestamp_month'], 'left')\n",
    "      .drop('timestamp_year', 'timestamp_month')\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create acquisition data frame from Acquisition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_acquisition(spark, acq):\n",
    "  return (\n",
    "    acq.join(name_mapping_df, col('seller_name') == col('from_seller_name'), 'left')\n",
    "      .drop('from_seller_name') \n",
    "      .withColumn('old_name', col('seller_name')) \n",
    "      .withColumn('seller_name', coalesce(col('to_seller_name'), col('seller_name'))) \n",
    "      .drop('to_seller_name') \n",
    "      .withColumn('orig_date', to_date(col('orig_date'), 'MM/yyyy')) \n",
    "      .withColumn('first_pay_date', to_date(col('first_pay_date'), 'MM/yyyy')) \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Casting Process\n",
    "\n",
    "\n",
    "This part is casting String column to Numeric one. \n",
    "Example:\n",
    "```\n",
    "col_1\n",
    " \"a\"\n",
    " \"b\"\n",
    " \"c\"\n",
    " \"a\"\n",
    "# After String ====> Numeric\n",
    "col_1\n",
    " 0\n",
    " 1\n",
    " 2\n",
    " 0\n",
    "```  \n",
    "\n",
    "### Define function to get column dictionary\n",
    "\n",
    "Example\n",
    "\n",
    "```\n",
    "col1 = [row(data=\"a\",id=0), row(data=\"b\",id=1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_dictionary(etl_df, col_names):\n",
    "  cnt_table = (\n",
    "    etl_df.select(posexplode(array([col(i) for i in col_names])))\n",
    "      .withColumnRenamed('pos', 'column_id')\n",
    "      .withColumnRenamed('col', 'data')\n",
    "      .filter('data is not null')\n",
    "      .groupBy('column_id', 'data')\n",
    "      .count()\n",
    "  )\n",
    "  windowed = Window.partitionBy('column_id').orderBy(desc('count'))\n",
    "  return cnt_table.withColumn('id', row_number().over(windowed)).drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to convert string columns to numeric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cast_string_columns_to_numeric(spark, input_df):\n",
    "  cached_dict_df = _gen_dictionary(input_df, cate_col_names).cache()\n",
    "  output_df = input_df\n",
    "  #  Generate the final table with all columns being numeric.\n",
    "  for col_pos, col_name in enumerate(cate_col_names):\n",
    "    col_dict_df = (\n",
    "      cached_dict_df.filter(col('column_id') == col_pos)\n",
    "        .drop('column_id')\n",
    "        .withColumnRenamed('data', col_name)\n",
    "    )\n",
    "    output_df = (\n",
    "      output_df.join(broadcast(col_dict_df), col_name, 'left')\n",
    "        .drop(col_name)\n",
    "        .withColumnRenamed('id', col_name)\n",
    "    )\n",
    "  return output_df     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Function\n",
    "\n",
    "In this function:\n",
    "1. Parse date in Performance data by calling _parse_dates (parsed_perf)\n",
    "2. Create deliqency dataframe(perf_deliqency) form Performance data by calling _create_perf_deliquency\n",
    "3. Create cleaned acquisition dataframe(cleaned_acq) from Acquisition data by calling _create_acquisition\n",
    "4. Join deliqency dataframe(perf_deliqency) and cleaned acquisition dataframe(cleaned_acq), get clean_df\n",
    "5. Cast String column to Numeric in clean_df by calling _cast_string_columns_to_numeric, get casted_clean_df\n",
    "6. Return casted_clean_df as final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mortgage(spark, perf, acq):\n",
    "  parsed_perf = _parse_dates(perf)\n",
    "  perf_deliqency = _create_perf_deliquency(spark, parsed_perf)\n",
    "  cleaned_acq = _create_acquisition(spark, acq)\n",
    "  clean_df = perf_deliqency.join(cleaned_acq, ['loan_id', 'quarter'], 'inner').drop('quarter')\n",
    "  casted_clean_df = (\n",
    "    _cast_string_columns_to_numeric(spark, clean_df)\n",
    "      .select(all_col_names)\n",
    "      .withColumn(label_col_name, when(col(label_col_name) > 0, col(label_col_name)).otherwise(0))\n",
    "      .fillna(float(0))\n",
    "  )\n",
    "  return casted_clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ETL Pipeline\n",
    "\n",
    "#### Read Raw Data and Run ETL Process, Save the Result\n",
    "\n",
    "##### Convert CSV to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out if already converted\n",
    "mortgage_csv = read_raw_csv(spark, '/data/mortgage.input.csv')\n",
    "mortgage_csv.write.parquet('/opt/spark/work-dir/mortgage.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ETL from Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# save processed data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mpreprocessed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/opt/spark/work-dir/mortgage-preprocessed.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m etl_dur \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/readwriter.py:755\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m--> 755\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/readwriter.py:679\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m--> 679\u001b[0m _, _, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback(ei)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1148\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[1;32m   1147\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[0;32m-> 1148\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1560\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, self_destruct)\u001b[0m\n\u001b[1;32m   1557\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[0;32m-> 1560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[1;32m   1561\u001b[0m         req, observations, progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[1;32m   1562\u001b[0m     ):\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1564\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1535\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, progress)\u001b[0m\n\u001b[1;32m   1533\u001b[0m         progress\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_operation(req\u001b[38;5;241m.\u001b[39moperation_id)\n\u001b[0;32m-> 1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1523\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, progress)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_reattachable_execute:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;66;03m# Don't use retryHandler - own retry handling is inside.\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m     generator \u001b[38;5;241m=\u001b[39m ExecutePlanResponseReattachableIterator(\n\u001b[1;32m   1521\u001b[0m         req, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stub, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrying, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_builder\u001b[38;5;241m.\u001b[39mmetadata()\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[0;32m-> 1523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1524\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m<frozen _collections_abc>:330\u001b[0m, in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:138\u001b[0m, in \u001b[0;36mExecutePlanResponseReattachableIterator.send\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pb2\u001b[38;5;241m.\u001b[39mExecutePlanResponse:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# will trigger reattach in case the stream completed without result_complete\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m    141\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:162\u001b[0m, in \u001b[0;36mExecutePlanResponseReattachableIterator._has_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:261\u001b[0m, in \u001b[0;36mExecutePlanResponseReattachableIterator._call_iter\u001b[0;34m(self, iter_fun)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stub\u001b[38;5;241m.\u001b[39mReattachExecute(\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_reattach_execute_request(), metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m    257\u001b[0m         )\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miter_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    263\u001b[0m     status \u001b[38;5;241m=\u001b[39m rpc_status\u001b[38;5;241m.\u001b[39mfrom_call(cast(grpc\u001b[38;5;241m.\u001b[39mCall, e))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:163\u001b[0m, in \u001b[0;36mExecutePlanResponseReattachableIterator._has_next.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_iter(\n\u001b[0;32m--> 163\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         )\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/grpc/_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/grpc/_channel.py:963\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_response_ready\u001b[39m():\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    959\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mOperationType\u001b[38;5;241m.\u001b[39mreceive_message \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdue\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    961\u001b[0m     )\n\u001b[0;32m--> 963\u001b[0m \u001b[43m_common\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_response_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/grpc/_common.py:156\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_complete_fn():\n\u001b[0;32m--> 156\u001b[0m         \u001b[43m_wait_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXIMUM_WAIT_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspin_cb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m timeout\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/grpc/_common.py:116\u001b[0m, in \u001b[0;36m_wait_once\u001b[0;34m(wait_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_once\u001b[39m(\n\u001b[1;32m    112\u001b[0m     wait_fn: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    113\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    114\u001b[0m     spin_cb: Optional[Callable[[], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m    115\u001b[0m ):\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mwait_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spin_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         spin_cb()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mortgage_pq = spark.read.parquet('/opt/spark/work-dir/mortgage.parquet')\n",
    "acq = extract_acq_columns(mortgage_pq)\n",
    "perf = extract_perf_columns(mortgage_pq)\n",
    "# run main function to process data\n",
    "preprocessed = run_mortgage(spark, perf, acq)\n",
    "# save processed data\n",
    "start = time.time()\n",
    "preprocessed.write.parquet('/opt/spark/work-dir/mortgage-preprocessed.parquet', mode='overwrite')\n",
    "end = time.time()\n",
    "etl_dur = end - start\n",
    "print(f'ETL takes {etl_dur}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ML\n",
    "\n",
    "#### The ML modeling phase of the example uses the `spark.ml` Pipeline API to carry out the following steps on a random subsample of the ETL output:\n",
    "  - use `spark.ml FeatureHasher` to map the int type columns in the ETL output to a 2^15 dimensional sparse feature vector with a non-zero entry in each location corresponding to hash value of each input column value + column name.\n",
    "  - use `spark.ml VectorAssembler` to combine the output of `FeatureHasher` with the original float type columns into a single `VectorUDT` type feature vector\n",
    "  - train a model using `LogisticRegression` to predict the multi-class (4 class values) label \"delinquency_12\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etlDf = spark.read.parquet('/opt/spark/work-dir/mortgage-preprocessed.parquet')\n",
    "etlDf.describe().show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etlDf = etlDf.sample(fraction=0.1, seed=1234)\n",
    "etlDf.describe().show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etlDf = etlDf.withColumn('loc',(etlDf.msa*1000+etlDf.zip).cast('int')).drop('zip' ,'msa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_col_name = 'delinquency_12'\n",
    "schema = etlDf.schema\n",
    "raw_features = [ x for x in schema.fields if x.name != label_col_name ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categorical_cols = [f.name for f in raw_features if f.dataType == IntegerType()]\n",
    "numerical_cols = [f.name for f in raw_features if f.name not in categorical_cols]\n",
    "hasher = FeatureHasher(inputCols=categorical_cols, outputCol='hashed_categorical', \n",
    "                       categoricalCols=categorical_cols, numFeatures=(1 << 15))\n",
    "va = VectorAssembler().setInputCols(numerical_cols + [hasher.getOutputCol()]).setOutputCol('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic =  ( \n",
    "  LogisticRegression()\n",
    "    .setMaxIter(200)\n",
    "    .setRegParam(0.00002)\n",
    "    .setElasticNetParam(0.1)\n",
    "    .setTol(1.0e-12)\n",
    "    .setFeaturesCol('features')\n",
    "    .setLabelCol(label_col_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train, df_test] = etlDf.randomSplit([0.8, 0.2], seed=1234)\n",
    "pipeline = Pipeline().setStages([hasher, va, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# gpu lr, gpu etl, gpu transform, 200 iters, double precision, elasticnet=0.1, featurehasher, 0.1 sample, multiclass, float64\n",
    "pipeline_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(df_test)\n",
    "predictions.sample(0.1).show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setMetricName('logLoss').setLabelCol(label_col_name)\n",
    "eval_res = evaluator.evaluate(predictions)\n",
    "end = time.time()\n",
    "print(f'Evaluation result: {eval_res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dur = end - start\n",
    "print(f'ML takes {ml_dur}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save current run times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times = pd.Series({'etl' : etl_dur, 'ml' : ml_dur})\n",
    "run_times.to_csv('/opt/spark/work-dir/gpu_times.csv' if accelerate_on_gpu else '/opt/spark/work-dir/cpu_times.csv', index=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/opt/spark/work-dir/cpu_times.csv') and os.path.exists('/opt/spark/work-dir/gpu_times.csv'):\n",
    "  cpu_times = pd.read_csv('/opt/spark/work-dir/cpu_times.csv', header=None, index_col=0)\n",
    "  gpu_times = pd.read_csv('/opt/spark/work-dir/gpu_times.csv', header=None, index_col=0)\n",
    "  gpu_speedup = cpu_times / gpu_times\n",
    "  gpu_speedup.plot(kind='bar', \n",
    "    title='GPU Acceleration Factor', \n",
    "    color='#76B900', \n",
    "    legend=False)\n",
    "  cpu_times = cpu_times[1].rename('cpu')\n",
    "  gpu_times = gpu_times[1].rename('gpu')\n",
    "  times = pd.DataFrame([cpu_times, gpu_times]).transpose()\n",
    "  times.plot(kind='bar', \n",
    "    title = 'ETL and ML elapsed times for CPU and GPU (lower is better)', \n",
    "    color=['blue', '#76B900'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
