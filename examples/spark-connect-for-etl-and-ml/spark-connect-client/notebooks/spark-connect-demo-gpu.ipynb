{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Spark Connect - SQL/DF ETL and MLlib on Mortgage Dataset (Spark 4.0+)\n",
    "\n",
    "Based on the Data and AI Summit 2025 session: [GPU Accelerated Spark Connect](https://www.databricks.com/dataaisummit/session/gpu-accelerated-spark-connect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, FeatureHasher\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark via Spark Connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Connect session id: 34215b55-5050-4830-9df7-dad6acf399ab\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Create GPU-accelerated Spark session using Spark Connect 4.0+\n",
    "spark = (\n",
    "  SparkSession.builder\n",
    "    .remote('sc://spark-connect-server')\n",
    "    .appName('GPU-Accelerated-ETL-ML-Demo') \n",
    "    .getOrCreate()\n",
    ")\n",
    "print(f'Spark Connect session id: {spark.session_id}')\n",
    "print(f'Spark version: {spark.version}')\n",
    "# workaround for repeated executions\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|mod10|  count(1)|\n",
      "+-----+----------+\n",
      "|    0|3435973837|\n",
      "|    1|3435973837|\n",
      "|    2|3435973837|\n",
      "|    3|3435973837|\n",
      "|    4|3435973837|\n",
      "|    5|3435973837|\n",
      "|    6|3435973837|\n",
      "|    7|3435973837|\n",
      "|    8|3435973836|\n",
      "|    9|3435973836|\n",
      "+-----+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['mod10 ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['mod10], ['mod10, unresolvedalias('count(*))]\n",
      "   +- 'Project [unresolvedstarwithcolumns(mod10, '`%`('id, 10), Some(List({})))]\n",
      "      +- Range (0, 34359738368, step=1, splits=Some(64))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mod10: bigint, count(1): bigint\n",
      "Sort [mod10#70020L ASC NULLS FIRST], true\n",
      "+- Aggregate [mod10#70020L], [mod10#70020L, count(1) AS count(1)#70022L]\n",
      "   +- Project [id#69970L, (id#69970L % cast(10 as bigint)) AS mod10#70020L]\n",
      "      +- Range (0, 34359738368, step=1, splits=Some(64))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [mod10#70020L ASC NULLS FIRST], true\n",
      "+- Aggregate [mod10#70020L], [mod10#70020L, count(1) AS count(1)#70022L]\n",
      "   +- Project [(id#69970L % 10) AS mod10#70020L]\n",
      "      +- Range (0, 34359738368, step=1, splits=Some(64))\n",
      "\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow false\n",
      "+- GpuSort [mod10#70020L ASC NULLS FIRST], true, com.nvidia.spark.rapids.OutOfCoreSort$@5f6846c4\n",
      "   +- GpuShuffleCoalesce 536870912\n",
      "      +- GpuColumnarExchange gpurangepartitioning(mod10#70020L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=44516]\n",
      "         +- GpuHashAggregate (keys=[mod10#70020L], functions=[gpucount(1, false)], output=[mod10#70020L, count(1)#70022L]) \n",
      "            +- GpuShuffleCoalesce 536870912\n",
      "               +- GpuColumnarExchange gpuhashpartitioning(mod10#70020L, 200, Murmur3Mode), ENSURE_REQUIREMENTS, [plan_id=44499]\n",
      "                  +- GpuHashAggregate (keys=[mod10#70020L], functions=[partial_gpucount(1, false)], output=[mod10#70020L, count#70024L]) \n",
      "                     +- GpuProject [(id#69970L % 10) AS mod10#70020L], true\n",
      "                        +- GpuRange (0, 34359738368, step=1, splits=64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "  spark.range(2 ** 35)\n",
    "    .withColumn('mod10', col('id') % lit(10))\n",
    "    .groupBy('mod10').agg(count('*'))\n",
    "    .orderBy('mod10')\n",
    ")\n",
    "df.show()\n",
    "# workaround to get a plan with GpuOverrides applied by disabling adaptive execution\n",
    "spark.conf.set('spark.sql.adaptive.enabled', False)\n",
    "df.explain(mode='extended')\n",
    "spark.conf.set('spark.sql.adaptive.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Should GPU Be Used from the next cell on?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_accelerate_on_gpu = True\n",
    "accelerate_on_gpu = input(f\"USE GPU? (y/n) (default: {default_accelerate_on_gpu})\")\n",
    "if not accelerate_on_gpu:\n",
    "  accelerate_on_gpu = default_accelerate_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled', accelerate_on_gpu)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerate_on_gpu:\n",
    "  spark.conf.set('spark.connect.ml.backend.classes', 'com.nvidia.rapids.ml.Plugin')\n",
    "else:\n",
    "  spark.conf.unset('spark.connect.ml.backend.classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize references to the same bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+--------------+\n",
      "|from_seller_name                                      |to_seller_name|\n",
      "+------------------------------------------------------+--------------+\n",
      "|WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015|Wells Fargo   |\n",
      "|WELLS FARGO BANK,  NA                                 |Wells Fargo   |\n",
      "|WELLS FARGO BANK, N.A.                                |Wells Fargo   |\n",
      "|WELLS FARGO BANK, NA                                  |Wells Fargo   |\n",
      "+------------------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/home/jovyan/work/name_mapping.csv', 'r') as name_mapping_file:\n",
    "  nm_reader = csv.reader(name_mapping_file,)\n",
    "  name_mapping = [r for r in nm_reader]\n",
    "name_mapping_df = spark.createDataFrame(name_mapping, ['from_seller_name', 'to_seller_name'])\n",
    "\n",
    "(\n",
    "  name_mapping_df\n",
    "    .where(col('to_seller_name') == 'Wells Fargo' )\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String columns\n",
    "cate_col_names = [\n",
    "  'orig_channel',\n",
    "  'first_home_buyer',\n",
    "  'loan_purpose',\n",
    "  'property_type',\n",
    "  'occupancy_status',\n",
    "  'property_state',\n",
    "  'product_type',\n",
    "  'relocation_mortgage_indicator',\n",
    "  'seller_name',\n",
    "  'mod_flag'\n",
    "]\n",
    "# Numeric columns\n",
    "label_col_name = 'delinquency_12'\n",
    "numeric_col_names = [\n",
    "  'orig_interest_rate',\n",
    "  'orig_upb',\n",
    "  'orig_loan_term',\n",
    "  'orig_ltv',\n",
    "  'orig_cltv',\n",
    "  'num_borrowers',\n",
    "  'dti',\n",
    "  'borrower_credit_score',\n",
    "  'num_units',\n",
    "  'zip',\n",
    "  'mortgage_insurance_percent',\n",
    "  'current_loan_delinquency_status',\n",
    "  'current_actual_upb',\n",
    "  'interest_rate',\n",
    "  'loan_age',\n",
    "  'msa',\n",
    "  'non_interest_bearing_upb',\n",
    "  label_col_name\n",
    "]\n",
    "all_col_names = cate_col_names + numeric_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read raw columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_csv(spark, path):\n",
    "  def _get_quarter_from_csv_file_name():\n",
    "    return substring_index(substring_index(input_file_name(), '.', 1), '/', -1)\n",
    "\n",
    "  with open('/home/jovyan/work/csv_raw_schema.ddl', 'r') as f:\n",
    "    _csv_raw_schema_str = f.read()\n",
    "  \n",
    "  return (\n",
    "    spark.read\n",
    "    .format('csv') \n",
    "    .option('nullValue', '') \n",
    "    .option('header', False) \n",
    "    .option('delimiter', '|') \n",
    "    .schema(_csv_raw_schema_str) \n",
    "    .load(path) \n",
    "    .withColumn('quarter', _get_quarter_from_csv_file_name())\n",
    "  )\n",
    "\n",
    "def extract_perf_columns(rawDf):\n",
    "  perfDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    date_format(to_date(col('monthly_reporting_period'),'MMyyyy'), 'MM/dd/yyyy').alias('monthly_reporting_period'),\n",
    "    upper(col('servicer')).alias('servicer'),\n",
    "    col('interest_rate'),\n",
    "    col('current_actual_upb'),\n",
    "    col('loan_age'),\n",
    "    col('remaining_months_to_legal_maturity'),\n",
    "    col('adj_remaining_months_to_maturity'),\n",
    "    date_format(to_date(col('maturity_date'),'MMyyyy'), 'MM/yyyy').alias('maturity_date'),\n",
    "    col('msa'),\n",
    "    col('current_loan_delinquency_status'),\n",
    "    col('mod_flag'),\n",
    "    col('zero_balance_code'),\n",
    "    date_format(to_date(col('zero_balance_effective_date'),'MMyyyy'), 'MM/yyyy').alias('zero_balance_effective_date'),\n",
    "    date_format(to_date(col('last_paid_installment_date'),'MMyyyy'), 'MM/dd/yyyy').alias('last_paid_installment_date'),\n",
    "    date_format(to_date(col('foreclosed_after'),'MMyyyy'), 'MM/dd/yyyy').alias('foreclosed_after'),\n",
    "    date_format(to_date(col('disposition_date'),'MMyyyy'), 'MM/dd/yyyy').alias('disposition_date'),\n",
    "    col('foreclosure_costs'),\n",
    "    col('prop_preservation_and_repair_costs'),\n",
    "    col('asset_recovery_costs'),\n",
    "    col('misc_holding_expenses'),\n",
    "    col('holding_taxes'),\n",
    "    col('net_sale_proceeds'),\n",
    "    col('credit_enhancement_proceeds'),\n",
    "    col('repurchase_make_whole_proceeds'),\n",
    "    col('other_foreclosure_proceeds'),\n",
    "    col('non_interest_bearing_upb'),\n",
    "    col('principal_forgiveness_upb'),\n",
    "    col('repurchase_make_whole_proceeds_flag'),\n",
    "    col('foreclosure_principal_write_off_amount'),\n",
    "    col('servicing_activity_indicator'),\n",
    "    col('quarter')\n",
    "  )\n",
    "  return perfDf.select('*').filter('current_actual_upb != 0.0')\n",
    "\n",
    "def extract_acq_columns(rawDf):\n",
    "  acqDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    col('orig_channel'),\n",
    "    upper(col('seller_name')).alias('seller_name'),\n",
    "    col('orig_interest_rate'),\n",
    "    col('orig_upb'),\n",
    "    col('orig_loan_term'),\n",
    "    date_format(to_date(col('orig_date'),'MMyyyy'), 'MM/yyyy').alias('orig_date'),\n",
    "    date_format(to_date(col('first_pay_date'),'MMyyyy'), 'MM/yyyy').alias('first_pay_date'),\n",
    "    col('orig_ltv'),\n",
    "    col('orig_cltv'),\n",
    "    col('num_borrowers'),\n",
    "    col('dti'),\n",
    "    col('borrower_credit_score'),\n",
    "    col('first_home_buyer'),\n",
    "    col('loan_purpose'),\n",
    "    col('property_type'),\n",
    "    col('num_units'),\n",
    "    col('occupancy_status'),\n",
    "    col('property_state'),\n",
    "    col('zip'),\n",
    "    col('mortgage_insurance_percent'),\n",
    "    col('product_type'),\n",
    "    col('coborrow_credit_score'),\n",
    "    col('mortgage_insurance_type'),\n",
    "    col('relocation_mortgage_indicator'),\n",
    "    dense_rank().over(Window.partitionBy('loan_id').orderBy(to_date(col('monthly_reporting_period'),'MMyyyy'))).alias('rank'),\n",
    "    col('quarter')\n",
    "  )\n",
    "\n",
    "  return acqDf.select('*').filter(col('rank')==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to parse date in Performance data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_dates(perf):\n",
    "  return (\n",
    "    perf.withColumn('monthly_reporting_period', to_date(col('monthly_reporting_period'), 'MM/dd/yyyy')) \n",
    "      .withColumn('monthly_reporting_period_month', month(col('monthly_reporting_period'))) \n",
    "      .withColumn('monthly_reporting_period_year', year(col('monthly_reporting_period'))) \n",
    "      .withColumn('monthly_reporting_period_day', dayofmonth(col('monthly_reporting_period'))) \n",
    "      .withColumn('last_paid_installment_date', to_date(col('last_paid_installment_date'), 'MM/dd/yyyy')) \n",
    "      .withColumn('foreclosed_after', to_date(col('foreclosed_after'), 'MM/dd/yyyy')) \n",
    "      .withColumn('disposition_date', to_date(col('disposition_date'), 'MM/dd/yyyy')) \n",
    "      .withColumn('maturity_date', to_date(col('maturity_date'), 'MM/yyyy')) \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create deliquency data frame from Performance data.  \n",
    "\n",
    "The computed `delinquency_12` column denotes whether a loan will become delinquent by 3, 6, or 9 months, \n",
    "or not delinquent, within the next 12 month period.   \n",
    "\n",
    "It will be the target label for ML multi-class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_perf_deliquency(spark, perf):\n",
    "  aggDF = (\n",
    "    perf\n",
    "      .select(\n",
    "        col('quarter'),\n",
    "        col('loan_id'),\n",
    "        col('current_loan_delinquency_status'),\n",
    "        when(col('current_loan_delinquency_status') >= 1, col('monthly_reporting_period')).alias('delinquency_30'),\n",
    "        when(col('current_loan_delinquency_status') >= 3, col('monthly_reporting_period')).alias('delinquency_90'),\n",
    "        when(col('current_loan_delinquency_status') >= 6, col('monthly_reporting_period')).alias('delinquency_180')\n",
    "      ).groupBy('quarter', 'loan_id')\n",
    "       .agg(\n",
    "         max('current_loan_delinquency_status').alias('delinquency_12'),\n",
    "         min('delinquency_30').alias('delinquency_30'),\n",
    "         min('delinquency_90').alias('delinquency_90'),\n",
    "         min('delinquency_180').alias('delinquency_180')\n",
    "       ).select(\n",
    "         col('quarter'),\n",
    "         col('loan_id'),\n",
    "         (col('delinquency_12') >= 1).alias('ever_30'),\n",
    "         (col('delinquency_12') >= 3).alias('ever_90'),\n",
    "         (col('delinquency_12') >= 6).alias('ever_180'),\n",
    "         col('delinquency_30'),\n",
    "         col('delinquency_90'),\n",
    "         col('delinquency_180')\n",
    "       )\n",
    "  )\n",
    "  #aggDF.printSchema()\n",
    "  joinedDf = (\n",
    "    perf\n",
    "      .withColumnRenamed('monthly_reporting_period', 'timestamp')\n",
    "      .withColumnRenamed('monthly_reporting_period_month', 'timestamp_month') \n",
    "      .withColumnRenamed('monthly_reporting_period_year', 'timestamp_year') \n",
    "      .withColumnRenamed('current_loan_delinquency_status', 'delinquency_12') \n",
    "      .withColumnRenamed('current_actual_upb', 'upb_12') \n",
    "      .select('quarter', 'loan_id', 'timestamp', 'delinquency_12', 'upb_12', 'timestamp_month', 'timestamp_year') \n",
    "      .join(aggDF, ['loan_id', 'quarter'], 'left_outer')\n",
    "  )\n",
    "  # calculate the 12 month delinquency and upb values\n",
    "  months = 12\n",
    "  monthArray = [lit(x) for x in range(0, 12)]\n",
    "  \n",
    "  testDf = ( \n",
    "    joinedDf\n",
    "      .withColumn('month_y', explode(array(monthArray)))\n",
    "      .select(\n",
    "        col('quarter'),\n",
    "        floor(((col('timestamp_year') * 12 + col('timestamp_month')) - 24000) / months).alias('josh_mody'),\n",
    "        floor(((col('timestamp_year') * 12 + col('timestamp_month')) - 24000 - col('month_y')) / months).alias('josh_mody_n'),\n",
    "        col('ever_30'),\n",
    "        col('ever_90'),\n",
    "        col('ever_180'),\n",
    "        col('delinquency_30'),\n",
    "        col('delinquency_90'),\n",
    "        col('delinquency_180'),\n",
    "        col('loan_id'),\n",
    "        col('month_y'),\n",
    "        col('delinquency_12'),\n",
    "        col('upb_12')\n",
    "      ).groupBy('quarter', 'loan_id', 'josh_mody_n', 'ever_30', 'ever_90', 'ever_180', 'delinquency_30', 'delinquency_90', 'delinquency_180', 'month_y')\n",
    "    .agg(max('delinquency_12').alias('delinquency_12'), min('upb_12').alias('upb_12')) \n",
    "    .withColumn('timestamp_year', floor((lit(24000) + (col('josh_mody_n') * lit(months)) + (col('month_y') - 1)) / lit(12))) \n",
    "    .selectExpr('*', f'pmod(24000 + (josh_mody_n * {months}) + month_y, 12) as timestamp_month_tmp') \n",
    "    .withColumn('timestamp_month', when(col('timestamp_month_tmp') == lit(0), lit(12)).otherwise(col('timestamp_month_tmp'))) \n",
    "    .withColumn('delinquency_12', ((col('delinquency_12') > 9).cast('int') + (col('delinquency_12') > 6).cast('int') + (col('delinquency_12') > 3).cast('int') + (col('upb_12') == 0).cast('int')).alias('delinquency_12')) \n",
    "    .drop('timestamp_month_tmp', 'josh_mody_n', 'month_y')\n",
    "  )\n",
    "\n",
    "  return (\n",
    "    perf\n",
    "      .withColumnRenamed('monthly_reporting_period_month', 'timestamp_month')\n",
    "      .withColumnRenamed('monthly_reporting_period_year', 'timestamp_year')\n",
    "      .join(testDf, ['quarter', 'loan_id', 'timestamp_year', 'timestamp_month'], 'left')\n",
    "      .drop('timestamp_year', 'timestamp_month')\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create acquisition data frame from Acquisition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_acquisition(spark, acq):\n",
    "  return (\n",
    "    acq.join(name_mapping_df, col('seller_name') == col('from_seller_name'), 'left')\n",
    "      .drop('from_seller_name') \n",
    "      .withColumn('old_name', col('seller_name')) \n",
    "      .withColumn('seller_name', coalesce(col('to_seller_name'), col('seller_name'))) \n",
    "      .drop('to_seller_name') \n",
    "      .withColumn('orig_date', to_date(col('orig_date'), 'MM/yyyy')) \n",
    "      .withColumn('first_pay_date', to_date(col('first_pay_date'), 'MM/yyyy')) \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Casting Process\n",
    "\n",
    "\n",
    "This part is casting String column to Numeric one. \n",
    "Example:\n",
    "```\n",
    "col_1\n",
    " \"a\"\n",
    " \"b\"\n",
    " \"c\"\n",
    " \"a\"\n",
    "# After String ====> Numeric\n",
    "col_1\n",
    " 0\n",
    " 1\n",
    " 2\n",
    " 0\n",
    "```  \n",
    "\n",
    "### Define function to get column dictionary\n",
    "\n",
    "Example\n",
    "\n",
    "```\n",
    "col1 = [row(data=\"a\",id=0), row(data=\"b\",id=1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_dictionary(etl_df, col_names):\n",
    "  cnt_table = (\n",
    "    etl_df.select(posexplode(array([col(i) for i in col_names])))\n",
    "      .withColumnRenamed('pos', 'column_id')\n",
    "      .withColumnRenamed('col', 'data')\n",
    "      .filter('data is not null')\n",
    "      .groupBy('column_id', 'data')\n",
    "      .count()\n",
    "  )\n",
    "  windowed = Window.partitionBy('column_id').orderBy(desc('count'))\n",
    "  return cnt_table.withColumn('id', row_number().over(windowed)).drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to convert string columns to numeric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cast_string_columns_to_numeric(spark, input_df):\n",
    "  cached_dict_df = _gen_dictionary(input_df, cate_col_names).cache()\n",
    "  output_df = input_df\n",
    "  #  Generate the final table with all columns being numeric.\n",
    "  for col_pos, col_name in enumerate(cate_col_names):\n",
    "    col_dict_df = (\n",
    "      cached_dict_df.filter(col('column_id') == col_pos)\n",
    "        .drop('column_id')\n",
    "        .withColumnRenamed('data', col_name)\n",
    "    )\n",
    "    output_df = (\n",
    "      output_df.join(broadcast(col_dict_df), col_name, 'left')\n",
    "        .drop(col_name)\n",
    "        .withColumnRenamed('id', col_name)\n",
    "    )\n",
    "  return output_df     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Function\n",
    "\n",
    "In this function:\n",
    "1. Parse date in Performance data by calling _parse_dates (parsed_perf)\n",
    "2. Create deliqency dataframe(perf_deliqency) form Performance data by calling _create_perf_deliquency\n",
    "3. Create cleaned acquisition dataframe(cleaned_acq) from Acquisition data by calling _create_acquisition\n",
    "4. Join deliqency dataframe(perf_deliqency) and cleaned acquisition dataframe(cleaned_acq), get clean_df\n",
    "5. Cast String column to Numeric in clean_df by calling _cast_string_columns_to_numeric, get casted_clean_df\n",
    "6. Return casted_clean_df as final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mortgage(spark, perf, acq):\n",
    "  parsed_perf = _parse_dates(perf)\n",
    "  perf_deliqency = _create_perf_deliquency(spark, parsed_perf)\n",
    "  cleaned_acq = _create_acquisition(spark, acq)\n",
    "  clean_df = perf_deliqency.join(cleaned_acq, ['loan_id', 'quarter'], 'inner').drop('quarter')\n",
    "  casted_clean_df = (\n",
    "    _cast_string_columns_to_numeric(spark, clean_df)\n",
    "      .select(all_col_names)\n",
    "      .withColumn(label_col_name, when(col(label_col_name) > 0, col(label_col_name)).otherwise(0))\n",
    "      .fillna(float(0))\n",
    "  )\n",
    "  return casted_clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ETL Pipeline\n",
    "\n",
    "#### Read Raw Data and Run ETL Process, Save the Result\n",
    "\n",
    "##### Convert CSV to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out if already converted\n",
    "mortgage_csv = read_raw_csv(spark, '/data/mortgage.input.csv')\n",
    "mortgage_csv.write.parquet('/opt/spark/work-dir/mortgage.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ETL from Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkException",
     "evalue": "Job aborted due to stage failure: Task 49 in stage 322.0 failed 4 times, most recent failure: Lost task 49.3 in stage 322.0 (TID 13474) (172.18.0.3 executor 0): java.io.FileNotFoundException: File file:/opt/spark/work-dir/mortgage.parquet/part-00031-af0913e7-3477-4e94-a02a-392d05dcfb1a-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBufUsingHadoop(GpuParquetScan.scala:560)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$readFooterBuffer$2(GpuParquetScan.scala:555)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBuffer(GpuParquetScan.scala:555)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$getFooterBuffer$5(GpuParquetScan.scala:536)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.getFooterBuffer(GpuParquetScan.scala:534)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readAndFilterFooter(GpuParquetScan.scala:622)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$filterBlocks$1(GpuParquetScan.scala:705)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.filterBlocks(GpuParquetScan.scala:691)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.com$nvidia$spark$rapids$parquet$GpuParquetMultiFilePartitionReaderFactory$$filterBlocksForCoalescingReader(GpuParquetScan.scala:1187)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$8(GpuParquetScan.scala:1264)\n\tat scala.collection.ArrayOps$.map$extension(ArrayOps.scala:936)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$4(GpuParquetScan.scala:1262)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$2(GpuParquetScan.scala:1245)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.buildBaseColumnarReaderForCoalescing(GpuParquetScan.scala:1245)\n\tat com.nvidia.spark.rapids.MultiFilePartitionReaderFactoryBase.createColumnarReader(GpuMultiFileReader.scala:273)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.advanceToNextIter(GpuDataSourceRDD.scala:96)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.hasNext(GpuDataSourceRDD.scala:80)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.$anonfun$hasNext$1(GpuFileSourceScanExec.scala:463)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:316)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.hasNext(GpuFileSourceScanExec.scala:462)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1(GpuMetrics.scala:364)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1$adapted(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.hasNext(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.getHasOnDeck(GpuCoalesceBatches.scala:311)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:328)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.$anonfun$hasNext$4(GpuAggregateExec.scala:2085)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.hasNext(GpuAggregateExec.scala:2085)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.partNextBatch(GpuShuffleExchangeExecBase.scala:391)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.hasNext(GpuShuffleExchangeExecBase.scala:414)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2955)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2492)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\nCaused by: java.io.FileNotFoundException: File file:/opt/spark/work-dir/mortgage.parquet/part-00031-af0913e7-3477-4e94-a02a-392d05dcfb1a-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBufUsingHadoop(GpuParquetScan.scala:560)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$readFooterBuffer$2(GpuParquetScan.scala:555)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBuffer(GpuParquetScan.scala:555)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$getFooterBuffer$5(GpuParquetScan.scala:536)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.getFooterBuffer(GpuParquetScan.scala:534)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readAndFilterFooter(GpuParquetScan.scala:622)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$filterBlocks$1(GpuParquetScan.scala:705)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.filterBlocks(GpuParquetScan.scala:691)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.com$nvidia$spark$rapids$parquet$GpuParquetMultiFilePartitionReaderFactory$$filterBlocksForCoalescingReader(GpuParquetScan.scala:1187)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$8(GpuParquetScan.scala:1264)\n\tat scala.collection.ArrayOps$.map$extension(ArrayOps.scala:936)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$4(GpuParquetScan.scala:1262)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$2(GpuParquetScan.scala:1245)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.buildBaseColumnarReaderForCoalescing(GpuParquetScan.scala:1245)\n\tat com.nvidia.spark.rapids.MultiFilePartitionReaderFactoryBase.createColumnarReader(GpuMultiFileReader.scala:273)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.advanceToNextIter(GpuDataSourceRDD.scala:96)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.hasNext(GpuDataSourceRDD.scala:80)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.$anonfun$hasNext$1(GpuFileSourceScanExec.scala:463)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:316)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.hasNext(GpuFileSourceScanExec.scala:462)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1(GpuMetrics.scala:364)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1$adapted(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.hasNext(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.getHasOnDeck(GpuCoalesceBatches.scala:311)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:328)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.$anonfun$hasNext$4(GpuAggregateExec.scala:2085)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.hasNext(GpuAggregateExec.scala:2085)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.partNextBatch(GpuShuffleExchangeExecBase.scala:391)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.hasNext(GpuShuffleExchangeExecBase.scala:414)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# save processed data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mpreprocessed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/opt/spark/work-dir/mortgage-preprocessed.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m etl_dur \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/readwriter.py:755\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m--> 755\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/readwriter.py:679\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m--> 679\u001b[0m _, _, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback(ei)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1148\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[1;32m   1147\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[0;32m-> 1148\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1560\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, self_destruct)\u001b[0m\n\u001b[1;32m   1557\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[0;32m-> 1560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[1;32m   1561\u001b[0m         req, observations, progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[1;32m   1562\u001b[0m     ):\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1564\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1537\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, progress)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1811\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 1811\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1882\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrorClass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_HANDLE.SESSION_CHANGED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1880\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1882\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   1883\u001b[0m                 info,\n\u001b[1;32m   1884\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   1885\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   1886\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   1887\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSparkException\u001b[0m: Job aborted due to stage failure: Task 49 in stage 322.0 failed 4 times, most recent failure: Lost task 49.3 in stage 322.0 (TID 13474) (172.18.0.3 executor 0): java.io.FileNotFoundException: File file:/opt/spark/work-dir/mortgage.parquet/part-00031-af0913e7-3477-4e94-a02a-392d05dcfb1a-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBufUsingHadoop(GpuParquetScan.scala:560)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$readFooterBuffer$2(GpuParquetScan.scala:555)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBuffer(GpuParquetScan.scala:555)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$getFooterBuffer$5(GpuParquetScan.scala:536)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.getFooterBuffer(GpuParquetScan.scala:534)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readAndFilterFooter(GpuParquetScan.scala:622)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$filterBlocks$1(GpuParquetScan.scala:705)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.filterBlocks(GpuParquetScan.scala:691)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.com$nvidia$spark$rapids$parquet$GpuParquetMultiFilePartitionReaderFactory$$filterBlocksForCoalescingReader(GpuParquetScan.scala:1187)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$8(GpuParquetScan.scala:1264)\n\tat scala.collection.ArrayOps$.map$extension(ArrayOps.scala:936)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$4(GpuParquetScan.scala:1262)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$2(GpuParquetScan.scala:1245)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.buildBaseColumnarReaderForCoalescing(GpuParquetScan.scala:1245)\n\tat com.nvidia.spark.rapids.MultiFilePartitionReaderFactoryBase.createColumnarReader(GpuMultiFileReader.scala:273)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.advanceToNextIter(GpuDataSourceRDD.scala:96)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.hasNext(GpuDataSourceRDD.scala:80)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.$anonfun$hasNext$1(GpuFileSourceScanExec.scala:463)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:316)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.hasNext(GpuFileSourceScanExec.scala:462)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1(GpuMetrics.scala:364)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1$adapted(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.hasNext(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.getHasOnDeck(GpuCoalesceBatches.scala:311)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:328)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.$anonfun$hasNext$4(GpuAggregateExec.scala:2085)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.hasNext(GpuAggregateExec.scala:2085)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.partNextBatch(GpuShuffleExchangeExecBase.scala:391)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.hasNext(GpuShuffleExchangeExecBase.scala:414)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2955)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2492)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\nCaused by: java.io.FileNotFoundException: File file:/opt/spark/work-dir/mortgage.parquet/part-00031-af0913e7-3477-4e94-a02a-392d05dcfb1a-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBufUsingHadoop(GpuParquetScan.scala:560)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$readFooterBuffer$2(GpuParquetScan.scala:555)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readFooterBuffer(GpuParquetScan.scala:555)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$getFooterBuffer$5(GpuParquetScan.scala:536)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.getFooterBuffer(GpuParquetScan.scala:534)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.readAndFilterFooter(GpuParquetScan.scala:622)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.$anonfun$filterBlocks$1(GpuParquetScan.scala:705)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetFileFilterHandler.filterBlocks(GpuParquetScan.scala:691)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.com$nvidia$spark$rapids$parquet$GpuParquetMultiFilePartitionReaderFactory$$filterBlocksForCoalescingReader(GpuParquetScan.scala:1187)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$8(GpuParquetScan.scala:1264)\n\tat scala.collection.ArrayOps$.map$extension(ArrayOps.scala:936)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$4(GpuParquetScan.scala:1262)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCoalescing$2(GpuParquetScan.scala:1245)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:321)\n\tat com.nvidia.spark.rapids.parquet.GpuParquetMultiFilePartitionReaderFactory.buildBaseColumnarReaderForCoalescing(GpuParquetScan.scala:1245)\n\tat com.nvidia.spark.rapids.MultiFilePartitionReaderFactoryBase.createColumnarReader(GpuMultiFileReader.scala:273)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.advanceToNextIter(GpuDataSourceRDD.scala:96)\n\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.hasNext(GpuDataSourceRDD.scala:80)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.$anonfun$hasNext$1(GpuFileSourceScanExec.scala:463)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat com.nvidia.spark.rapids.GpuMetric.ns(GpuMetrics.scala:316)\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.hasNext(GpuFileSourceScanExec.scala:462)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1(GpuMetrics.scala:364)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1$adapted(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)\n\tat com.nvidia.spark.rapids.CollectTimeIterator.hasNext(GpuMetrics.scala:363)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.getHasOnDeck(GpuCoalesceBatches.scala:311)\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:328)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:256)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.$anonfun$hasNext$4(GpuAggregateExec.scala:2085)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat com.nvidia.spark.rapids.DynamicGpuPartialAggregateIterator.hasNext(GpuAggregateExec.scala:2085)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.partNextBatch(GpuShuffleExchangeExecBase.scala:391)\n\tat org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.hasNext(GpuShuffleExchangeExecBase.scala:414)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
     ]
    }
   ],
   "source": [
    "mortgage_pq = spark.read.parquet('/opt/spark/work-dir/mortgage.parquet')\n",
    "acq = extract_acq_columns(mortgage_pq)\n",
    "perf = extract_perf_columns(mortgage_pq)\n",
    "# run main function to process data\n",
    "preprocessed = run_mortgage(spark, perf, acq)\n",
    "# save processed data\n",
    "start = time.time()\n",
    "preprocessed.write.parquet('/opt/spark/work-dir/mortgage-preprocessed.parquet', mode='overwrite')\n",
    "end = time.time()\n",
    "etl_dur = end - start\n",
    "print(f'ETL takes {etl_dur}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ML\n",
    "\n",
    "#### The ML modeling phase of the example uses the `spark.ml` Pipeline API to carry out the following steps on a random subsample of the ETL output:\n",
    "  - use `spark.ml FeatureHasher` to map the int type columns in the ETL output to a 2^15 dimensional sparse feature vector with a non-zero entry in each location corresponding to hash value of each input column value + column name.\n",
    "  - use `spark.ml VectorAssembler` to combine the output of `FeatureHasher` with the original float type columns into a single `VectorUDT` type feature vector\n",
    "  - train a model using `LogisticRegression` to predict the multi-class (4 class values) label \"delinquency_12\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " summary                         | count                \n",
      " orig_channel                    | 36662419             \n",
      " first_home_buyer                | 36662419             \n",
      " loan_purpose                    | 36662419             \n",
      " property_type                   | 36662419             \n",
      " occupancy_status                | 36662419             \n",
      " property_state                  | 36662419             \n",
      " product_type                    | 36662419             \n",
      " relocation_mortgage_indicator   | 36662419             \n",
      " seller_name                     | 36662419             \n",
      " mod_flag                        | 36662419             \n",
      " orig_interest_rate              | 36662419             \n",
      " orig_upb                        | 36662419             \n",
      " orig_loan_term                  | 36662419             \n",
      " orig_ltv                        | 36662419             \n",
      " orig_cltv                       | 36662419             \n",
      " num_borrowers                   | 36662419             \n",
      " dti                             | 36662419             \n",
      " borrower_credit_score           | 36662419             \n",
      " num_units                       | 36662419             \n",
      " zip                             | 36662419             \n",
      " mortgage_insurance_percent      | 36662419             \n",
      " current_loan_delinquency_status | 36662419             \n",
      " current_actual_upb              | 36662419             \n",
      " interest_rate                   | 36662419             \n",
      " loan_age                        | 36662419             \n",
      " msa                             | 36662419             \n",
      " non_interest_bearing_upb        | 36662419             \n",
      " delinquency_12                  | 36662419             \n",
      "-RECORD 1-----------------------------------------------\n",
      " summary                         | mean                 \n",
      " orig_channel                    | 1.4486339540225102   \n",
      " first_home_buyer                | 1.2687550976928172   \n",
      " loan_purpose                    | 1.5949912906728823   \n",
      " property_type                   | 1.5205843618774855   \n",
      " occupancy_status                | 1.1339841214514514   \n",
      " property_state                  | 12.773911726883052   \n",
      " product_type                    | 1.0                  \n",
      " relocation_mortgage_indicator   | 1.0033522337955934   \n",
      " seller_name                     | 5.778820159138981    \n",
      " mod_flag                        | 1.0000581521912124   \n",
      " orig_interest_rate              | 4.397569945316484    \n",
      " orig_upb                        | 302767.03531209985   \n",
      " orig_loan_term                  | 331.67249095047436   \n",
      " orig_ltv                        | 71.73010215174291    \n",
      " orig_cltv                       | 71.96010353272106    \n",
      " num_borrowers                   | 1.4529852490093467   \n",
      " dti                             | 36.15650093901333    \n",
      " borrower_credit_score           | 751.408905969898     \n",
      " num_units                       | 1.024945380718059    \n",
      " zip                             | 545.7791044557098    \n",
      " mortgage_insurance_percent      | 7.468208903509613    \n",
      " current_loan_delinquency_status | 0.020152734602700385 \n",
      " current_actual_upb              | 295994.1484082115    \n",
      " interest_rate                   | 4.397568867291599    \n",
      " loan_age                        | 7.634651930632291    \n",
      " msa                             | 27324.62900197611    \n",
      " non_interest_bearing_upb        | 0.7626683449338136   \n",
      " delinquency_12                  | 0.007889277573310151 \n",
      "-RECORD 2-----------------------------------------------\n",
      " summary                         | stddev               \n",
      " orig_channel                    | 0.695517883651418    \n",
      " first_home_buyer                | 0.4433123058492253   \n",
      " loan_purpose                    | 0.7443572663622811   \n",
      " property_type                   | 0.7405772533373938   \n",
      " occupancy_status                | 0.4165278797203917   \n",
      " property_state                  | 11.349508938548315   \n",
      " product_type                    | 0.0                  \n",
      " relocation_mortgage_indicator   | 0.05780135305770613  \n",
      " seller_name                     | 7.222388071547107    \n",
      " mod_flag                        | 0.00762553677593716  \n",
      " orig_interest_rate              | 1.2997226162451332   \n",
      " orig_upb                        | 162731.53405171176   \n",
      " orig_loan_term                  | 64.52672963260281    \n",
      " orig_ltv                        | 19.304109040284793   \n",
      " orig_cltv                       | 19.370952592444915   \n",
      " num_borrowers                   | 0.526998600850464    \n",
      " dti                             | 9.520914662105707    \n",
      " borrower_credit_score           | 50.73232538979521    \n",
      " num_units                       | 0.20721441010027106  \n",
      " zip                             | 291.83047036504087   \n",
      " mortgage_insurance_percent      | 12.44413494998935    \n",
      " current_loan_delinquency_status | 0.29548452419658244  \n",
      " current_actual_upb              | 160804.82046156697   \n",
      " interest_rate                   | 1.2997216561126097   \n",
      " loan_age                        | 5.437898041621951    \n",
      " msa                             | 13709.720496026797   \n",
      " non_interest_bearing_upb        | 240.03880708979494   \n",
      " delinquency_12                  | 0.1256826325407898   \n",
      "-RECORD 3-----------------------------------------------\n",
      " summary                         | min                  \n",
      " orig_channel                    | 1                    \n",
      " first_home_buyer                | 1                    \n",
      " loan_purpose                    | 1                    \n",
      " property_type                   | 1                    \n",
      " occupancy_status                | 1                    \n",
      " property_state                  | 1                    \n",
      " product_type                    | 1                    \n",
      " relocation_mortgage_indicator   | 1                    \n",
      " seller_name                     | 0                    \n",
      " mod_flag                        | 1                    \n",
      " orig_interest_rate              | 1.5                  \n",
      " orig_upb                        | 12000.0              \n",
      " orig_loan_term                  | 85                   \n",
      " orig_ltv                        | 2.0                  \n",
      " orig_cltv                       | 2.0                  \n",
      " num_borrowers                   | 1.0                  \n",
      " dti                             | 0.0                  \n",
      " borrower_credit_score           | 0.0                  \n",
      " num_units                       | 1                    \n",
      " zip                             | 0                    \n",
      " mortgage_insurance_percent      | 0.0                  \n",
      " current_loan_delinquency_status | 0                    \n",
      " current_actual_upb              | 0.01                 \n",
      " interest_rate                   | 1.5                  \n",
      " loan_age                        | -1.0                 \n",
      " msa                             | 0.0                  \n",
      " non_interest_bearing_upb        | 0.0                  \n",
      " delinquency_12                  | 0                    \n",
      "-RECORD 4-----------------------------------------------\n",
      " summary                         | max                  \n",
      " orig_channel                    | 3                    \n",
      " first_home_buyer                | 2                    \n",
      " loan_purpose                    | 3                    \n",
      " property_type                   | 5                    \n",
      " occupancy_status                | 3                    \n",
      " property_state                  | 54                   \n",
      " product_type                    | 1                    \n",
      " relocation_mortgage_indicator   | 2                    \n",
      " seller_name                     | 43                   \n",
      " mod_flag                        | 2                    \n",
      " orig_interest_rate              | 8.625                \n",
      " orig_upb                        | 2095000.0            \n",
      " orig_loan_term                  | 360                  \n",
      " orig_ltv                        | 97.0                 \n",
      " orig_cltv                       | 105.0                \n",
      " num_borrowers                   | 5.0                  \n",
      " dti                             | 64.0                 \n",
      " borrower_credit_score           | 840.0                \n",
      " num_units                       | 4                    \n",
      " zip                             | 999                  \n",
      " mortgage_insurance_percent      | 40.0                 \n",
      " current_loan_delinquency_status | 20                   \n",
      " current_actual_upb              | 2095000.0            \n",
      " interest_rate                   | 8.625                \n",
      " loan_age                        | 32.0                 \n",
      " msa                             | 49740.0              \n",
      " non_interest_bearing_upb        | 367936.17            \n",
      " delinquency_12                  | 3                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "etlDf = spark.read.parquet('/opt/spark/work-dir/mortgage-preprocessed.parquet')\n",
    "etlDf.describe().show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " summary                         | count                \n",
      " orig_channel                    | 3664340              \n",
      " first_home_buyer                | 3664340              \n",
      " loan_purpose                    | 3664340              \n",
      " property_type                   | 3664340              \n",
      " occupancy_status                | 3664340              \n",
      " property_state                  | 3664340              \n",
      " product_type                    | 3664340              \n",
      " relocation_mortgage_indicator   | 3664340              \n",
      " seller_name                     | 3664340              \n",
      " mod_flag                        | 3664340              \n",
      " orig_interest_rate              | 3664340              \n",
      " orig_upb                        | 3664340              \n",
      " orig_loan_term                  | 3664340              \n",
      " orig_ltv                        | 3664340              \n",
      " orig_cltv                       | 3664340              \n",
      " num_borrowers                   | 3664340              \n",
      " dti                             | 3664340              \n",
      " borrower_credit_score           | 3664340              \n",
      " num_units                       | 3664340              \n",
      " zip                             | 3664340              \n",
      " mortgage_insurance_percent      | 3664340              \n",
      " current_loan_delinquency_status | 3664340              \n",
      " current_actual_upb              | 3664340              \n",
      " interest_rate                   | 3664340              \n",
      " loan_age                        | 3664340              \n",
      " msa                             | 3664340              \n",
      " non_interest_bearing_upb        | 3664340              \n",
      " delinquency_12                  | 3664340              \n",
      "-RECORD 1-----------------------------------------------\n",
      " summary                         | mean                 \n",
      " orig_channel                    | 1.4486112096584924   \n",
      " first_home_buyer                | 1.2686985923795282   \n",
      " loan_purpose                    | 1.5951071680029691   \n",
      " property_type                   | 1.520276229825835    \n",
      " occupancy_status                | 1.1341622229378279   \n",
      " property_state                  | 12.77643122636       \n",
      " product_type                    | 1.0                  \n",
      " relocation_mortgage_indicator   | 1.0033809635568751   \n",
      " seller_name                     | 5.77411266421784     \n",
      " mod_flag                        | 1.0000564903911755   \n",
      " orig_interest_rate              | 4.397394351506684    \n",
      " orig_upb                        | 302716.90618228656   \n",
      " orig_loan_term                  | 331.6698379517185    \n",
      " orig_ltv                        | 71.72137929340617    \n",
      " orig_cltv                       | 71.95042599758756    \n",
      " num_borrowers                   | 1.452468657384413    \n",
      " dti                             | 36.16285251914397    \n",
      " borrower_credit_score           | 751.4370448702904    \n",
      " num_units                       | 1.025187619052817    \n",
      " zip                             | 545.8335326416217    \n",
      " mortgage_insurance_percent      | 7.46812632015588     \n",
      " current_loan_delinquency_status | 0.02003471293602668  \n",
      " current_actual_upb              | 295945.2144133268    \n",
      " interest_rate                   | 4.397393756856624    \n",
      " loan_age                        | 7.631722493000104    \n",
      " msa                             | 27335.085431482887   \n",
      " non_interest_bearing_upb        | 0.8411436848109073   \n",
      " delinquency_12                  | 0.00782951363683501  \n",
      "-RECORD 2-----------------------------------------------\n",
      " summary                         | stddev               \n",
      " orig_channel                    | 0.6956030324773513   \n",
      " first_home_buyer                | 0.4432828808533512   \n",
      " loan_purpose                    | 0.7444411544237499   \n",
      " property_type                   | 0.7398957114640072   \n",
      " occupancy_status                | 0.41679166015043273  \n",
      " property_state                  | 11.352436998190967   \n",
      " product_type                    | 0.0                  \n",
      " relocation_mortgage_indicator   | 0.05804768351837526  \n",
      " seller_name                     | 7.214926417987589    \n",
      " mod_flag                        | 0.007515797723900275 \n",
      " orig_interest_rate              | 1.2998514612281695   \n",
      " orig_upb                        | 162715.42197479168   \n",
      " orig_loan_term                  | 64.5328250458735     \n",
      " orig_ltv                        | 19.309691219479998   \n",
      " orig_cltv                       | 19.376300421775785   \n",
      " num_borrowers                   | 0.5267734522657201   \n",
      " dti                             | 9.519188200190765    \n",
      " borrower_credit_score           | 50.665327830021084   \n",
      " num_units                       | 0.20827096657497238  \n",
      " zip                             | 291.97787379781226   \n",
      " mortgage_insurance_percent      | 12.445753408821949   \n",
      " current_loan_delinquency_status | 0.29320917984481615  \n",
      " current_actual_upb              | 160794.61077613302   \n",
      " interest_rate                   | 1.2998503543089193   \n",
      " loan_age                        | 5.437254627471392    \n",
      " msa                             | 13710.135870388753   \n",
      " non_interest_bearing_upb        | 246.56868686960226   \n",
      " delinquency_12                  | 0.12486683480884546  \n",
      "-RECORD 3-----------------------------------------------\n",
      " summary                         | min                  \n",
      " orig_channel                    | 1                    \n",
      " first_home_buyer                | 1                    \n",
      " loan_purpose                    | 1                    \n",
      " property_type                   | 1                    \n",
      " occupancy_status                | 1                    \n",
      " property_state                  | 1                    \n",
      " product_type                    | 1                    \n",
      " relocation_mortgage_indicator   | 1                    \n",
      " seller_name                     | 0                    \n",
      " mod_flag                        | 1                    \n",
      " orig_interest_rate              | 1.625                \n",
      " orig_upb                        | 13000.0              \n",
      " orig_loan_term                  | 85                   \n",
      " orig_ltv                        | 2.0                  \n",
      " orig_cltv                       | 2.0                  \n",
      " num_borrowers                   | 1.0                  \n",
      " dti                             | 0.0                  \n",
      " borrower_credit_score           | 0.0                  \n",
      " num_units                       | 1                    \n",
      " zip                             | 0                    \n",
      " mortgage_insurance_percent      | 0.0                  \n",
      " current_loan_delinquency_status | 0                    \n",
      " current_actual_upb              | 0.01                 \n",
      " interest_rate                   | 1.625                \n",
      " loan_age                        | -1.0                 \n",
      " msa                             | 0.0                  \n",
      " non_interest_bearing_upb        | 0.0                  \n",
      " delinquency_12                  | 0                    \n",
      "-RECORD 4-----------------------------------------------\n",
      " summary                         | max                  \n",
      " orig_channel                    | 3                    \n",
      " first_home_buyer                | 2                    \n",
      " loan_purpose                    | 3                    \n",
      " property_type                   | 5                    \n",
      " occupancy_status                | 3                    \n",
      " property_state                  | 54                   \n",
      " product_type                    | 1                    \n",
      " relocation_mortgage_indicator   | 2                    \n",
      " seller_name                     | 43                   \n",
      " mod_flag                        | 2                    \n",
      " orig_interest_rate              | 8.625                \n",
      " orig_upb                        | 2095000.0            \n",
      " orig_loan_term                  | 360                  \n",
      " orig_ltv                        | 97.0                 \n",
      " orig_cltv                       | 105.0                \n",
      " num_borrowers                   | 5.0                  \n",
      " dti                             | 64.0                 \n",
      " borrower_credit_score           | 840.0                \n",
      " num_units                       | 4                    \n",
      " zip                             | 999                  \n",
      " mortgage_insurance_percent      | 40.0                 \n",
      " current_loan_delinquency_status | 20                   \n",
      " current_actual_upb              | 2085000.0            \n",
      " interest_rate                   | 8.625                \n",
      " loan_age                        | 32.0                 \n",
      " msa                             | 49740.0              \n",
      " non_interest_bearing_upb        | 201646.0             \n",
      " delinquency_12                  | 3                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "etlDf = etlDf.sample(fraction=0.1, seed=1234)\n",
    "etlDf.describe().show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etlDf = etlDf.withColumn('loc',(etlDf.msa*1000+etlDf.zip).cast('int')).drop('zip' ,'msa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_col_name = 'delinquency_12'\n",
    "schema = etlDf.schema\n",
    "raw_features = [ x for x in schema.fields if x.name != label_col_name ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categorical_cols = [f.name for f in raw_features if f.dataType == IntegerType()]\n",
    "numerical_cols = [f.name for f in raw_features if f.name not in categorical_cols]\n",
    "hasher = FeatureHasher(inputCols=categorical_cols, outputCol='hashed_categorical', \n",
    "                       categoricalCols=categorical_cols, numFeatures=(1 << 15))\n",
    "va = VectorAssembler().setInputCols(numerical_cols + [hasher.getOutputCol()]).setOutputCol('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic =  ( \n",
    "  LogisticRegression()\n",
    "    .setMaxIter(200)\n",
    "    .setRegParam(0.00002)\n",
    "    .setElasticNetParam(0.1)\n",
    "    .setTol(1.0e-12)\n",
    "    .setFeaturesCol('features')\n",
    "    .setLabelCol(label_col_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train, df_test] = etlDf.randomSplit([0.8, 0.2], seed=1234)\n",
    "pipeline = Pipeline().setStages([hasher, va, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# gpu lr, gpu etl, gpu transform, 200 iters, double precision, elasticnet=0.1, featurehasher, 0.1 sample, multiclass, float64\n",
    "pipeline_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " orig_channel                    | 1                                                                                                                                                                                                                                               \n",
      " first_home_buyer                | 1                                                                                                                                                                                                                                               \n",
      " loan_purpose                    | 1                                                                                                                                                                                                                                               \n",
      " property_type                   | 1                                                                                                                                                                                                                                               \n",
      " occupancy_status                | 1                                                                                                                                                                                                                                               \n",
      " property_state                  | 1                                                                                                                                                                                                                                               \n",
      " product_type                    | 1                                                                                                                                                                                                                                               \n",
      " relocation_mortgage_indicator   | 1                                                                                                                                                                                                                                               \n",
      " seller_name                     | 1                                                                                                                                                                                                                                               \n",
      " mod_flag                        | 1                                                                                                                                                                                                                                               \n",
      " orig_interest_rate              | 3.875                                                                                                                                                                                                                                           \n",
      " orig_upb                        | 616000.0                                                                                                                                                                                                                                        \n",
      " orig_loan_term                  | 360                                                                                                                                                                                                                                             \n",
      " orig_ltv                        | 95.0                                                                                                                                                                                                                                            \n",
      " orig_cltv                       | 95.0                                                                                                                                                                                                                                            \n",
      " num_borrowers                   | 2.0                                                                                                                                                                                                                                             \n",
      " dti                             | 47.0                                                                                                                                                                                                                                            \n",
      " borrower_credit_score           | 713.0                                                                                                                                                                                                                                           \n",
      " num_units                       | 1                                                                                                                                                                                                                                               \n",
      " mortgage_insurance_percent      | 30.0                                                                                                                                                                                                                                            \n",
      " current_loan_delinquency_status | 0                                                                                                                                                                                                                                               \n",
      " current_actual_upb              | 600732.76                                                                                                                                                                                                                                       \n",
      " interest_rate                   | 3.875                                                                                                                                                                                                                                           \n",
      " loan_age                        | 16.0                                                                                                                                                                                                                                            \n",
      " non_interest_bearing_upb        | 0.0                                                                                                                                                                                                                                             \n",
      " delinquency_12                  | 0                                                                                                                                                                                                                                               \n",
      " loc                             | 40140925                                                                                                                                                                                                                                        \n",
      " hashed_categorical              | (32768,[5884,10908,11409,12250,17072,17929,18817,19568,20220,21462,26270,30646,32406,32594],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                          \n",
      " features                        | (32780,[0,1,2,3,4,5,6,7,8,9,10,5896,10920,11421,12262,17084,17941,18829,19580,20232,21474,26282,30658,32418,32606],[3.875,616000.0,95.0,95.0,2.0,47.0,713.0,30.0,600732.76,3.875,16.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) \n",
      " rawPrediction                   | [5.957805415017059,-0.5238385231823801,-1.8873943310683559,-5.013679396990047]                                                                                                                                                                  \n",
      " probability                     | [0.9980636532697318,0.0015283261499836258,3.9086904788980726E-4,1.7151532394752872E-5]                                                                                                                                                          \n",
      " prediction                      | 0.0                                                                                                                                                                                                                                             \n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(df_test)\n",
    "predictions.sample(0.1).show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result: 0.018943926849266976\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setMetricName('logLoss').setLabelCol(label_col_name)\n",
    "eval_res = evaluator.evaluate(predictions)\n",
    "end = time.time()\n",
    "print(f'Evaluation result: {eval_res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML takes 46.97028374671936\n"
     ]
    }
   ],
   "source": [
    "ml_dur = end - start\n",
    "print(f'ML takes {ml_dur}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save current run times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times = pd.Series({'etl' : etl_dur, 'ml' : ml_dur})\n",
    "run_times.to_csv('/opt/spark/work-dir/gpu_times.csv' if accelerate_on_gpu else '/opt/spark/work-dir/cpu_times.csv', index=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/opt/spark/work-dir/cpu_times.csv') and os.path.exists('/opt/spark/work-dir/gpu_times.csv'):\n",
    "  cpu_times = pd.read_csv('/opt/spark/work-dir/cpu_times.csv', header=None, index_col=0)\n",
    "  gpu_times = pd.read_csv('/opt/spark/work-dir/gpu_times.csv', header=None, index_col=0)\n",
    "  gpu_speedup = cpu_times / gpu_times\n",
    "  gpu_speedup.plot(kind='bar', \n",
    "    title='GPU Acceleration Factor', \n",
    "    color='#76B900', \n",
    "    legend=False)\n",
    "  cpu_times = cpu_times[1].rename('cpu')\n",
    "  gpu_times = gpu_times[1].rename('gpu')\n",
    "  times = pd.DataFrame([cpu_times, gpu_times]).transpose()\n",
    "  times.plot(kind='bar', \n",
    "    title = 'ETL and ML elapsed times for CPU and GPU (lower is better)', \n",
    "    color=['blue', '#76B900'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
