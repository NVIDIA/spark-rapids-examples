{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Spark Connect Demo - ETL and ML Pipeline (Spark 4.0+)\n",
    "\n",
    "Based on the Data and AI Summit 2025 session: [GPU Accelerated Spark Connect](https://www.databricks.com/dataaisummit/session/gpu-accelerated-spark-connect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark via Spark Connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "# Create GPU-accelerated Spark session using Spark Connect 4.0+\n",
    "spark = (\n",
    "  SparkSession.builder\n",
    "    .remote('sc://spark-connect-server')\n",
    "    .appName('GPU-Accelerated-ETL-ML-Demo') \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f'Spark version: {spark.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Smoke Test GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|mod10|  count(1)|\n",
      "+-----+----------+\n",
      "|    0|3435973837|\n",
      "|    1|3435973837|\n",
      "|    2|3435973837|\n",
      "|    3|3435973837|\n",
      "|    4|3435973837|\n",
      "|    5|3435973837|\n",
      "|    6|3435973837|\n",
      "|    7|3435973837|\n",
      "|    8|3435973836|\n",
      "|    9|3435973836|\n",
      "+-----+----------+\n",
      "\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow (10)\n",
      "+- GpuSort (9)\n",
      "   +- GpuShuffleCoalesce (8)\n",
      "      +- GpuColumnarExchange (7)\n",
      "         +- GpuHashAggregate (6)\n",
      "            +- GpuShuffleCoalesce (5)\n",
      "               +- GpuColumnarExchange (4)\n",
      "                  +- GpuHashAggregate (3)\n",
      "                     +- GpuProject (2)\n",
      "                        +- GpuRange (1)\n",
      "\n",
      "\n",
      "(1) GpuRange\n",
      "Output [1]: [id#298L]\n",
      "Arguments: 0, 34359738368, 1, 64, [id#298L], 536870912\n",
      "\n",
      "(2) GpuProject\n",
      "Input [1]: [id#298L]\n",
      "Arguments: [(id#298L % 10) AS mod10#333L], true\n",
      "\n",
      "(3) GpuHashAggregate\n",
      "Input [1]: [mod10#333L]\n",
      "Keys [1]: [mod10#333L]\n",
      "Functions [1]: [partial_gpucount(1, false)]\n",
      "Aggregate Attributes [1]: [count#336L]\n",
      "Results [2]: [mod10#333L, count#337L]\n",
      "Lore: \n",
      "\n",
      "(4) GpuColumnarExchange\n",
      "Input [2]: [mod10#333L, count#337L]\n",
      "Arguments: gpuhashpartitioning(mod10#333L, 200, Murmur3Mode), ENSURE_REQUIREMENTS, [plan_id=1200]\n",
      "\n",
      "(5) GpuShuffleCoalesce\n",
      "Input [2]: [mod10#333L, count#337L]\n",
      "Arguments: 536870912\n",
      "\n",
      "(6) GpuHashAggregate\n",
      "Input [2]: [mod10#333L, count#337L]\n",
      "Keys [1]: [mod10#333L]\n",
      "Functions [1]: [gpucount(1, false)]\n",
      "Aggregate Attributes [1]: [count(1)#334L]\n",
      "Results [2]: [mod10#333L, count(1)#334L AS count(1)#335L]\n",
      "Lore: \n",
      "\n",
      "(7) GpuColumnarExchange\n",
      "Input [2]: [mod10#333L, count(1)#335L]\n",
      "Arguments: gpurangepartitioning(mod10#333L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1217]\n",
      "\n",
      "(8) GpuShuffleCoalesce\n",
      "Input [2]: [mod10#333L, count(1)#335L]\n",
      "Arguments: 536870912\n",
      "\n",
      "(9) GpuSort\n",
      "Input [2]: [mod10#333L, count(1)#335L]\n",
      "Arguments: [mod10#333L ASC NULLS FIRST], true, com.nvidia.spark.rapids.OutOfCoreSort$@17ac265b\n",
      "\n",
      "(10) GpuColumnarToRow\n",
      "Input [2]: [mod10#333L, count(1)#335L]\n",
      "Arguments: false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "  spark.range(2 ** 35)\n",
    "    .withColumn('mod10', col('id') % lit(10))\n",
    "    .groupBy('mod10').agg(count('*'))\n",
    "    .orderBy('mod10')\n",
    ")\n",
    "df.show()\n",
    "# workaround to get a plan with GpuOverrides applied by disabling adaptive execution\n",
    "spark.conf.set('spark.sql.adaptive.enabled', False)\n",
    "df.explain(mode='formatted')\n",
    "spark.conf.set('spark.sql.adaptive.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Should GPU Be Used from the next cell on?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate_on_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled', accelerate_on_gpu)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerate_on_gpu:\n",
    "  spark.conf.set('spark.connect.ml.backend.classes', 'com.nvidia.rapids.ml.Plugin')\n",
    "else:\n",
    "  spark.conf.unset('spark.connect.ml.backend.classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize references to the same bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+--------------+\n",
      "|from_seller_name                                      |to_seller_name|\n",
      "+------------------------------------------------------+--------------+\n",
      "|WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015|Wells Fargo   |\n",
      "|WELLS FARGO BANK,  NA                                 |Wells Fargo   |\n",
      "|WELLS FARGO BANK, N.A.                                |Wells Fargo   |\n",
      "|WELLS FARGO BANK, NA                                  |Wells Fargo   |\n",
      "+------------------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('work/name_mapping.csv', 'r') as name_mapping_file:\n",
    "  nm_reader = csv.reader(name_mapping_file,)\n",
    "  name_mapping = [r for r in nm_reader]\n",
    "name_mapping_df = spark.createDataFrame(name_mapping, ['from_seller_name', 'to_seller_name'])\n",
    "\n",
    "(\n",
    "  name_mapping_df\n",
    "    .where(col('to_seller_name') == 'Wells Fargo' )\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String columns\n",
    "cate_col_names = [\n",
    "  'orig_channel',\n",
    "  'first_home_buyer',\n",
    "  'loan_purpose',\n",
    "  'property_type',\n",
    "  'occupancy_status',\n",
    "  'property_state',\n",
    "  'product_type',\n",
    "  'relocation_mortgage_indicator',\n",
    "  'seller_name',\n",
    "  'mod_flag'\n",
    "]\n",
    "# Numeric columns\n",
    "label_col_name = 'delinquency_12'\n",
    "numeric_col_names = [\n",
    "  'orig_interest_rate',\n",
    "  'orig_upb',\n",
    "  'orig_loan_term',\n",
    "  'orig_ltv',\n",
    "  'orig_cltv',\n",
    "  'num_borrowers',\n",
    "  'dti',\n",
    "  'borrower_credit_score',\n",
    "  'num_units',\n",
    "  'zip',\n",
    "  'mortgage_insurance_percent',\n",
    "  'current_loan_delinquency_status',\n",
    "  'current_actual_upb',\n",
    "  'interest_rate',\n",
    "  'loan_age',\n",
    "  'msa',\n",
    "  'non_interest_bearing_upb',\n",
    "  label_col_name\n",
    "]\n",
    "all_col_names = cate_col_names + numeric_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read raw columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_csv(spark, path):\n",
    "  def _get_quarter_from_csv_file_name():\n",
    "    return substring_index(substring_index(input_file_name(), '.', 1), '/', -1)\n",
    "\n",
    "  with open('csv_raw_schema.ddl', 'r') as f:\n",
    "    _csv_raw_schema_str = f.read()\n",
    "  \n",
    "  return (\n",
    "    spark.read\n",
    "      .format('csv') \n",
    "      .option('nullValue', '') \n",
    "      .option('header', False) \n",
    "      .option('delimiter', '|') \n",
    "      .schema(_csv_raw_schema_str) \n",
    "      .load(path) \n",
    "      .withColumn('quarter', _get_quarter_from_csv_file_name())\n",
    "  )\n",
    "\n",
    "def extract_perf_columns(rawDf):\n",
    "  perfDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    date_format(to_date(col('monthly_reporting_period'),'MMyyyy'), 'MM/dd/yyyy').alias('monthly_reporting_period'),\n",
    "    upper(col('servicer')).alias('servicer'),\n",
    "    col('interest_rate'),\n",
    "    col('current_actual_upb'),\n",
    "    col('loan_age'),\n",
    "    col('remaining_months_to_legal_maturity'),\n",
    "    col('adj_remaining_months_to_maturity'),\n",
    "    date_format(to_date(col('maturity_date'),'MMyyyy'), 'MM/yyyy').alias('maturity_date'),\n",
    "    col('msa'),\n",
    "    col('current_loan_delinquency_status'),\n",
    "    col('mod_flag'),\n",
    "    col('zero_balance_code'),\n",
    "    date_format(to_date(col('zero_balance_effective_date'),'MMyyyy'), 'MM/yyyy').alias('zero_balance_effective_date'),\n",
    "    date_format(to_date(col('last_paid_installment_date'),'MMyyyy'), 'MM/dd/yyyy').alias('last_paid_installment_date'),\n",
    "    date_format(to_date(col('foreclosed_after'),'MMyyyy'), 'MM/dd/yyyy').alias('foreclosed_after'),\n",
    "    date_format(to_date(col('disposition_date'),'MMyyyy'), 'MM/dd/yyyy').alias('disposition_date'),\n",
    "    col('foreclosure_costs'),\n",
    "    col('prop_preservation_and_repair_costs'),\n",
    "    col('asset_recovery_costs'),\n",
    "    col('misc_holding_expenses'),\n",
    "    col('holding_taxes'),\n",
    "    col('net_sale_proceeds'),\n",
    "    col('credit_enhancement_proceeds'),\n",
    "    col('repurchase_make_whole_proceeds'),\n",
    "    col('other_foreclosure_proceeds'),\n",
    "    col('non_interest_bearing_upb'),\n",
    "    col('principal_forgiveness_upb'),\n",
    "    col('repurchase_make_whole_proceeds_flag'),\n",
    "    col('foreclosure_principal_write_off_amount'),\n",
    "    col('servicing_activity_indicator'),\n",
    "    col('quarter')\n",
    "  )\n",
    "  return perfDf.select('*').filter('current_actual_upb != 0.0')\n",
    "\n",
    "def extract_acq_columns(rawDf):\n",
    "  acqDf = rawDf.select(\n",
    "    col('loan_id'),\n",
    "    col('orig_channel'),\n",
    "    upper(col('seller_name')).alias('seller_name'),\n",
    "    col('orig_interest_rate'),\n",
    "    col('orig_upb'),\n",
    "    col('orig_loan_term'),\n",
    "    date_format(to_date(col('orig_date'),'MMyyyy'), 'MM/yyyy').alias('orig_date'),\n",
    "    date_format(to_date(col('first_pay_date'),'MMyyyy'), 'MM/yyyy').alias('first_pay_date'),\n",
    "    col('orig_ltv'),\n",
    "    col('orig_cltv'),\n",
    "    col('num_borrowers'),\n",
    "    col('dti'),\n",
    "    col('borrower_credit_score'),\n",
    "    col('first_home_buyer'),\n",
    "    col('loan_purpose'),\n",
    "    col('property_type'),\n",
    "    col('num_units'),\n",
    "    col('occupancy_status'),\n",
    "    col('property_state'),\n",
    "    col('zip'),\n",
    "    col('mortgage_insurance_percent'),\n",
    "    col('product_type'),\n",
    "    col('coborrow_credit_score'),\n",
    "    col('mortgage_insurance_type'),\n",
    "    col('relocation_mortgage_indicator'),\n",
    "    dense_rank().over(Window.partitionBy('loan_id').orderBy(to_date(col('monthly_reporting_period'),'MMyyyy'))).alias('rank'),\n",
    "    col('quarter')\n",
    "  )\n",
    "\n",
    "  return acqDf.select('*').filter(col('rank')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
