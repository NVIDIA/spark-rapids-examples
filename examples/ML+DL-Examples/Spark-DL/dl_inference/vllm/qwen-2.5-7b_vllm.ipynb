{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_efficientnet/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "# PySpark LLM Inference: Qwen-2.5 Text Summarization\n",
    "\n",
    "In this notebook, we demonstrate distributed batch inference with [Qwen-2.5](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct), using open weights on Huggingface.\n",
    "\n",
    "The Qwen-2.5-7b-instruct is an instruction-fine-tuned version of the Qwen-2.5-7b base model. We'll show how to use the model to perform text summarization.\n",
    "\n",
    "**Note:** Running this model on GPU with 16-bit precision requires **~16GB** of GPU RAM. Make sure your instances have sufficient GPU capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually enable Huggingface tokenizer parallelism to avoid disabling with PySpark parallelism.\n",
    "# See (https://github.com/huggingface/transformers/issues/5486) for more info. \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the cluster environment to handle any platform-specific configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_databricks = os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", False)\n",
    "on_dataproc = os.environ.get(\"DATAPROC_IMAGE_VERSION\", False)\n",
    "on_standalone = not (on_databricks or on_dataproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cloud environments, load the model to the distributed file system.\n",
    "if on_databricks:\n",
    "    models_dir = \"/dbfs/FileStore/spark-dl-models\"\n",
    "    dbutils.fs.mkdirs(\"/FileStore/spark-dl-models\")\n",
    "    model_path = f\"{models_dir}/qwen-2.5-7b\"\n",
    "elif on_dataproc:\n",
    "    models_dir = \"/mnt/gcs/spark-dl-models\"\n",
    "    os.mkdir(models_dir) if not os.path.exists(models_dir) else None\n",
    "    model_path = f\"{models_dir}/qwen-2.5-7b\"\n",
    "else:\n",
    "    model_path = os.path.abspath(\"qwen-2.5-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model from huggingface hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aa6eac07b34f298da36fdcc6571550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = snapshot_download(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    local_dir=model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: Running locally\n",
    "\n",
    "**Note**: If the driver node does not have sufficient GPU capacity, proceed to the PySpark section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 14:59:27 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-19 14:59:30 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-19 14:59:30 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/rishic/spark-rapids-examples/examples/ML+DL-Examples/Spark-DL/dl_inference/vllm/qwen-2.5-7b', speculative_config=None, tokenizer='/home/rishic/spark-rapids-examples/examples/ML+DL-Examples/Spark-DL/dl_inference/vllm/qwen-2.5-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/rishic/spark-rapids-examples/examples/ML+DL-Examples/Spark-DL/dl_inference/vllm/qwen-2.5-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-19 14:59:31 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-19 14:59:31 model_runner.py:1110] Starting to load model /home/rishic/spark-rapids-examples/examples/ML+DL-Examples/Spark-DL/dl_inference/vllm/qwen-2.5-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873a7164a10748508824826aaba01c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 14:59:33 model_runner.py:1115] Loading model weights took 14.2487 GB\n",
      "INFO 03-19 14:59:37 worker.py:267] Memory profiling takes 3.93 seconds\n",
      "INFO 03-19 14:59:37 worker.py:267] the current vLLM instance can use total_gpu_memory (47.41GiB) x gpu_memory_utilization (0.80) = 37.93GiB\n",
      "INFO 03-19 14:59:37 worker.py:267] model weights take 14.25GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 19.27GiB.\n",
      "INFO 03-19 14:59:37 executor_base.py:111] # cuda blocks: 22548, # CPU blocks: 4681\n",
      "INFO 03-19 14:59:37 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 11.01x\n",
      "INFO 03-19 14:59:38 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:08<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 14:59:47 model_runner.py:1562] Graph capturing finished in 9 secs, took 0.23 GiB\n",
      "INFO 03-19 14:59:47 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 14.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=128)\n",
    "llm = LLM(model=model_path, gpu_memory_utilization=0.8)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a knowledgeable AI assistant that provides accurate answers to questions.\"\n",
    "}\n",
    "\n",
    "queries = [\n",
    "    \"What does CUDA stand for?\",\n",
    "    \"In one sentence, what's the difference between a CPU and a GPU?\",\n",
    "    \"What's the hottest planet in the solar system?\"\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    [\n",
    "        system_prompt,\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ] for query in queries\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    prompts,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.98it/s, est. speed input: 72.10 toks/s, output: 115.09 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(text, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What does CUDA stand for?\n",
      "A: CUDA stands for Compute Unified Device Architecture. It is a parallel computing platform and application programming interface (API) model created by NVIDIA. CUDA allows developers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing, which can significantly speed up the execution of computationally intensive applications.\n",
      "\n",
      "Q: In one sentence, what's the difference between a CPU and a GPU?\n",
      "A: A CPU (Central Processing Unit) is designed for general-purpose processing and managing the flow of information within a computer, while a GPU (Graphics Processing Unit) is optimized for handling large amounts of data in parallel, particularly for rendering images and graphics.\n",
      "\n",
      "Q: What's the hottest planet in the solar system?\n",
      "A: The hottest planet in the solar system is Venus. Despite Mercury being closer to the Sun, Venus has a thick atmosphere that traps heat in a runaway greenhouse effect, making it significantly hotter. The average surface temperature on Venus is about 462°C (864°F), which is hot enough to melt lead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q, o in zip(queries, outputs):\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {o.outputs[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, col, struct, length, lit, concat\n",
    "from pyspark.ml.functions import predict_batch_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "datasets.disable_progress_bars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Session\n",
    "\n",
    "For local standalone clusters, we'll connect to the cluster and create the Spark Session.  \n",
    "For CSP environments, Spark will either be preconfigured (Databricks) or we'll need to create the Spark Session (Dataproc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/19 14:59:51 WARN Utils: Your hostname, cb4ae00-lcedt resolves to a loopback address: 127.0.1.1; using 10.110.47.100 instead (on interface eno1)\n",
      "25/03/19 14:59:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/19 14:59:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    if on_standalone:\n",
    "        import socket\n",
    "        conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "        hostname = socket.gethostname()\n",
    "        conf.setMaster(f\"spark://{hostname}:7077\")\n",
    "        conf.set(\"spark.pyspark.python\", f\"{conda_env}/bin/python\")\n",
    "        conf.set(\"spark.pyspark.driver.python\", f\"{conda_env}/bin/python\")\n",
    "\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.task.maxFailures\", \"1\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.125\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"spark-dl-examples\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Preprocess DataFrame\n",
    "\n",
    "Load the first 500 samples of the [ML ArXiv dataset](https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers) from Huggingface and store in a Spark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_arxiv_dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\", split=\"train\", streaming=True)\n",
    "ml_arxiv_pds = pd.Series([sample[\"abstract\"] for sample in ml_arxiv_dataset.take(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(ml_arxiv_pds, schema=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                               value|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|  The problem of statistical learning is to construct a predictor of a random\\nvariable $Y$ as a ...|\n",
      "|  In a sensor network, in practice, the communication among sensors is subject\\nto:(1) errors or ...|\n",
      "|  The on-line shortest path problem is considered under various models of\\npartial monitoring. Gi...|\n",
      "|  Ordinal regression is an important type of learning, which has properties of\\nboth classificati...|\n",
      "|  This paper uncovers and explores the close relationship between Monte Carlo\\nOptimization of a ...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format each sample into the chat template, including a system prompt to guide generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are a knowledgeable AI assistant. Your job is to create a 1 sentence summary \n",
    "of a research abstract that captures the main objective, methodology, and key findings, using clear \n",
    "language while preserving technical accuracy and quantitative results.'''\n",
    "\n",
    "df = df.select(\n",
    "    concat(\n",
    "        lit(\"<|im_start|>system\\n\"),\n",
    "        lit(system_prompt),\n",
    "        lit(\"<|im_end|>\\n<|im_start|>user\\n\"),\n",
    "        col(\"value\"),\n",
    "        lit(\"<|im_end|>\\n<|im_start|>assistant\\n\")\n",
    "    ).alias(\"prompt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a knowledgeable AI assistant. Your job is to create a 1 sentence summary \n",
      "of a research abstract that captures the main objective, methodology, and key findings, using clear \n",
      "language while preserving technical accuracy and quantitative results.<|im_end|>\n",
      "<|im_start|>user\n",
      "  The problem of statistical learning is to construct a predictor of a random\n",
      "variable $Y$ as a function of a related random variable $X$ on the basis of an\n",
      "i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\n",
      "predictors are drawn from some specified class, and the goal is to approach\n",
      "asymptotically the performance (expected loss) of the best predictor in the\n",
      "class. We consider the setting in which one has perfect observation of the\n",
      "$X$-part of the sample, while the $Y$-part has to be communicated at some\n",
      "finite bit rate. The encoding of the $Y$-values is allowed to depend on the\n",
      "$X$-values. Under suitable regularity conditions on the admissible predictors,\n",
      "the underlying family of probability distributions and the loss function, we\n",
      "give an information-theoretic characterization of achievable predictor\n",
      "performance in terms of conditional distortion-rate functions. The ideas are\n",
      "illustrated on the example of nonparametric regression in Gaussian noise.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.take(1)[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"spark-dl-datasets/arxiv_abstracts\"\n",
    "if on_databricks:\n",
    "    dbutils.fs.mkdirs(\"/FileStore/spark-dl-datasets\")\n",
    "    data_path = \"dbfs:/FileStore/\" + data_path\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using vLLM Server\n",
    "In this section, we demonstrate integration with [vLLM Serving](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html), an open-source server with an OpenAI-compatible completions endpoint for LLMs.  \n",
    "\n",
    "The process looks like this:\n",
    "- Distribute a server startup task across the Spark cluster, instructing each node to launch a vLLM server process.\n",
    "- Define a vLLM inference function, which sends inference request to the local server on a given node.\n",
    "- Wrap the vLLM inference function in a predict_batch_udf to launch parallel inference requests using Spark.\n",
    "- Finally, distribute a shutdown signal to terminate the vLLM server processes on each node.\n",
    "\n",
    "<img src=\"../images/spark-server.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the helper class from server_utils.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"server_utils.py\")\n",
    "\n",
    "from server_utils import VLLMServerManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start vLLM servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VLLMServerManager` will handle the lifecycle of vLLM server instances across the Spark cluster:\n",
    "- Find available ports for HTTP\n",
    "- Deploy a server on each node via stage-level scheduling\n",
    "- Gracefully shutdown servers across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"qwen-2.5-7b\"\n",
    "# model_path = \"/home/rishic/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28\"\n",
    "\n",
    "server_manager = VLLMServerManager(model_name=model_name, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any of the supported [vLLM serve CLI arguments](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#vllm-serve) as key-word arguments when starting the servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 14:59:58,379 - INFO - Requesting stage-level resources: (cores=5, gpu=1.0)\n",
      "2025-03-19 14:59:58,381 - INFO - Starting 1 VLLM servers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cb4ae00-lcedt': (3668902, [7000])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_manager.start_servers(gpu_memory_utilization=0.8, task=\"generate\", wait_retries=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define client function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the hostname -> url mapping from the server manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_to_http_url = server_manager.host_to_http_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the vLLM inference function, which returns a predict function for batch inference through the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_fn(model_name, host_to_url):\n",
    "    import socket\n",
    "    import numpy as np\n",
    "    import requests\n",
    "\n",
    "    url = host_to_url[socket.gethostname()]\n",
    "    \n",
    "    def predict(inputs):\n",
    "        response = requests.post(\n",
    "            f\"{url}/v1/completions\",\n",
    "            json={\n",
    "                \"model\": model_name,\n",
    "                \"prompt\": inputs.tolist(),\n",
    "                \"max_tokens\": 128,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.8,\n",
    "                \"repetition_penalty\": 1.05,\n",
    "            }\n",
    "        )\n",
    "        return np.array([r[\"text\"] for r in response.json()[\"choices\"]])\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = predict_batch_udf(partial(vllm_fn, model_name=model_name, host_to_url=host_to_http_url),\n",
    "                             return_type=StringType(),\n",
    "                             batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame\n",
    "\n",
    "We'll parallelize over a small set of prompts for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(data_path).limit(128).repartition(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.6 ms, sys: 38.3 ms, total: 74.9 ms\n",
      "Wall time: 8.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "preds = df.withColumn(\"outputs\", generate(col(\"prompt\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.6 ms, sys: 31.9 ms, total: 73.5 ms\n",
      "Wall time: 7.78 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"outputs\", generate(col(\"prompt\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: <|im_start|>system\n",
      "You are a knowledgeable AI assistant. Your job is to create a 1 sentence summary \n",
      "of a research abstract that captures the main objective, methodology, and key findings, using clear \n",
      "language while preserving technical accuracy and quantitative results.<|im_end|>\n",
      "<|im_start|>user\n",
      "  This article applies Machine Learning techniques to solve Intrusion Detection\n",
      "problems within computer networks. Due to complex and dynamic nature of\n",
      "computer networks and hacking techniques, detecting malicious activities\n",
      "remains a challenging task for security experts, that is, currently available\n",
      "defense systems suffer from low detection capability and high number of false\n",
      "alarms. To overcome such performance limitations, we propose a novel Machine\n",
      "Learning algorithm, namely Boosted Subspace Probabilistic Neural Network\n",
      "(BSPNN), which integrates an adaptive boosting technique and a semi parametric\n",
      "neural network to obtain good tradeoff between accuracy and generality. As the\n",
      "result, learning bias and generalization variance can be significantly\n",
      "minimized. Substantial experiments on KDD 99 intrusion benchmark indicate that\n",
      "our model outperforms other state of the art learning algorithms, with\n",
      "significantly improved detection accuracy, minimal false alarms and relatively\n",
      "small computational complexity.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      " \n",
      "\n",
      "A: The study proposes a novel Boosted Subspace Probabilistic Neural Network (BSPNN) to enhance intrusion detection in computer networks, achieving superior detection accuracy and minimal false alarms compared to existing methods, while maintaining low computational complexity. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Q: {results[0].prompt} \\n\")\n",
    "print(f\"A: {results[0].outputs} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shut down server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 15:00:46,841 - INFO - Requesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 15:00:48,314 - INFO - Successfully stopped 1 VLLM servers.           \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_manager.stop_servers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not on_databricks: # on databricks, spark.stop() puts the cluster in a bad state\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-dl-vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
