{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6659b4-88da-4207-8d32-2674da5383a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PySpark DL Inference on Dataproc\n",
    "### Conditional generation with Huggingface\n",
    "\n",
    "In this notebook, we demonstrate distributed inference with the T5 transformer to perform sentence translation.  \n",
    "From: https://huggingface.co/docs/transformers/model_doc/t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8dae4a-3bfc-4430-b28a-7350db5efed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, col, struct\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93a1424-e483-4d37-a719-32fabee3f285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "datasets.disable_progress_bars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08c37a5-fb0c-45f6-8630-d2af67831641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load the IMBD Movie Reviews dataset from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ec30c9-365a-43c5-9c53-3497400ee548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "dataset = dataset.to_pandas().drop(columns=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/13 18:35:31 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/11/13 18:35:31 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/11/13 18:35:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/11/13 18:35:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/11/13 18:35:33 WARN RapidsPluginUtils: RAPIDS Accelerator 24.08.1 using cudf 24.08.0, private revision 9fac64da220ddd6bf5626bd7bd1dd74c08603eac\n",
      "24/11/13 18:35:33 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n"
     ]
    }
   ],
   "source": [
    "def initialize_spark():\n",
    "    rapids_jar = glob.glob(\"/usr/lib/spark/jars/rapids-4-spark*\")[-1]\n",
    "    python_path=\"/opt/conda/miniconda3/python3.11\"\n",
    "    conda_lib_path=\"/opt/conda/miniconda3/lib\"\n",
    "\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.125\")\n",
    "    conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    conf.set(\"spark.task.maxFailures\", \"1\")\n",
    "    conf.set(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "    conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "    conf.set(\"spark.rapids.ml.uvm.enabled\", \"true\")\n",
    "    conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_lib_path}:$LD_LIBRARY_PATH\") # PyTriton needs libpython3.11.so\n",
    "    conf.set(\"spark.executorEnv.PYTHONPATH\", f\"{rapids_jar}:{python_path}\")\n",
    "    conf.set(\"spark.rapids.memory.gpu.minAllocFraction\", \"0.0001\")\n",
    "    conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "    conf.set(\"spark.locality.wait\", \"0s\")\n",
    "    conf.set(\"spark.sql.cache.serializer\", \"com.nvidia.spark.ParquetCachedBatchSerializer\")\n",
    "    conf.set(\"spark.rapids.memory.gpu.pooling.enabled\", \"false\")\n",
    "    conf.set(\"spark.sql.execution.sortBeforeRepartition\", \"false\")\n",
    "    conf.set(\"spark.rapids.sql.format.parquet.reader.type\", \"MULTITHREADED\")\n",
    "    conf.set(\"spark.rapids.sql.format.parquet.multiThreadedRead.maxNumFilesParallel\", \"20\")\n",
    "    conf.set(\"spark.rapids.sql.multiThreadedRead.numThreads\", \"20\")\n",
    "    conf.set(\"spark.rapids.sql.python.gpu.enabled\", \"true\")\n",
    "    conf.set(\"spark.rapids.memory.pinnedPool.size\", \"2G\")\n",
    "    conf.set(\"spark.python.daemon.module\", \"rapids.daemon\")\n",
    "    conf.set(\"spark.rapids.sql.batchSizeBytes\", \"512m\")\n",
    "    conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    conf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\n",
    "    conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "    conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"20000\")\n",
    "    conf.set(\"spark.rapids.sql.explain\", \"NONE\")\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"spark-dl\").config(conf=conf).getOrCreate()\n",
    "    return spark\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    print(\"Initializing Spark session.\")\n",
    "    spark = initialize_spark()\n",
    "else:\n",
    "    print(\"Using existing Spark session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e4269da-d2b3-46a5-9309-38a1ba825a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30dab34d-8e4b-4f30-b7c2-3dff49da018b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('text', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(dataset).repartition(16)\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c33cc0-5dfb-449c-ae79-80972fb04405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd6d6d9-1c2c-4131-8df4-a3ef75c3fc57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/13 18:36:05 WARN TaskSetManager: Stage 3 contains a task of very large size (2020 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(text=\"Miles O'keefe stars as Ator, a loin-clothed hero who resembles a Chippendale's dancer. The Conan-wannabe must do battle with an evil guy in a Cher wig, and protect the Earth from the Geometric Nucleus, a sort of primitive atomic bomb. Watch closely for visible sunglasses and tire-tracks. Mystery Science Theater 3000 made fun of it under the title CAVE DWELLERS.\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a5b258-1634-441e-8b36-29777e54592d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/13 18:36:10 WARN TaskSetManager: Stage 5 contains a task of very large size (2020 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = \"/tmp/datasets/imdb_test\"\n",
    "df.write.mode(\"overwrite\").parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b909f4-5732-428b-ad61-9a6c5cf94df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load and preprocess DataFrame\n",
    "\n",
    "We'll take the first sentence from each sample as our target for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7e53d6-bbd0-48d2-a3be-36847275e2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text: pd.Series, prefix: str = \"\") -> pd.Series:\n",
    "    @pandas_udf(\"string\")\n",
    "    def _preprocess(text: pd.Series) -> pd.Series:\n",
    "        return pd.Series([prefix + s.split(\".\")[0] for s in text])\n",
    "    return _preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97eee1a4-9dc4-43b0-9578-6d7f8ff338bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                text|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|G&M started a the odd couple downstairs in Man About the House and went on to amusing the nation ...|\n",
      "|Simply miserable Lana Turner-Ezio Pinza vehicle. Pinza had a beautiful voice but he rarely uses i...|\n",
      "|this 2.5 hour diluted snore-fest appears to be one of the poorest excuses for an adaptation, ever...|\n",
      "|seriously, if i wanted to make a movie that makes zero sense, never will, and features lesbian sc...|\n",
      "|I'm usually quite tolerant of movies, and very easily entertained, however this movie was dreadfu...|\n",
      "|Worse than mediocre thriller about an abused wife who goes on the lam after she is linked circums...|\n",
      "|**Could be considered some mild spoilers, but no more than in anyone else's review of this film.*...|\n",
      "|I just saw this on a local independent station in the New York City area. The cast showed promise...|\n",
      "|I registered with IMDb.com just so i could comment about this movie. My god what a steaming pile ...|\n",
      "|The worst movie I've ever seen in my life. From the amateur directing to the porn-quality acting,...|\n",
      "|It doesn't surprise me that the makers of this hopeless movie couldn't find a UK distributor, and...|\n",
      "|I rented the DVD in a video store, as an alternative to reading the report. But it's pretty much ...|\n",
      "|This movie is so stupid that I want my $2.99 back that I paid for!! First this movie starts off w...|\n",
      "|After watching Avalon (which was decent only because of the very nice digital fx), and several an...|\n",
      "|This movie starts out as if it were a comedy. It almost appears that the actors are reading off o...|\n",
      "|Nice attempt to bring Shakespearian language alive in a post-apocalyptic setting, but the final r...|\n",
      "|I'd never thought that I would be caught saying this: But I think \"Dog the Bounty Hunter\" is more...|\n",
      "|Now, I love bad, old skifee movies as much as most people. And I understand that a budget is a bu...|\n",
      "|One of the commenter's is wrong. This is not the only Pat Patterson film and he didn't die two ye...|\n",
      "|The worst part of all is the poor scripting, leading to superficial acting.<br /><br />Dreyfuss' ...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(data_path).limit(1024).repartition(16)\n",
    "df.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa14304d-b409-4d07-99ef-9da7c7c76158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                               input|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                              translate English to French: 1983's \"Frightmare\" is an odd little film|\n",
      "|                                translate English to French: The film made no sense to me whatsoever|\n",
      "|translate English to French: I loved the first 15 minutes, and I loved some of the dialogue in th...|\n",
      "|                    translate English to French: Don't tell me this film was funny or a little funny|\n",
      "|                                translate English to French: I was really disappointed by this movie|\n",
      "|       translate English to French: It's about time for a female boxing flick, but this one ain't it|\n",
      "|translate English to French: I'm actually too drained to write this review -- bad movies always d...|\n",
      "|translate English to French: Hello it is I Derrick Cannon and I welcome you to the first ever Can...|\n",
      "|                                                  translate English to French: Don't waste your time|\n",
      "|translate English to French: The original title always struck me as a rather overblown definition...|\n",
      "|                            translate English to French: \"Three\" is a seriously dumb shipwreck movie|\n",
      "|                                            translate English to French: I love movies in this genre|\n",
      "|                translate English to French: The quote above just about says it all for \"Slipstream\"|\n",
      "|translate English to French: \"Fate\" leads Walter Sparrow to come in possession of a mysterious no...|\n",
      "|translate English to French: I felt I had to add a comment after seeing the breathless gushing of...|\n",
      "|translate English to French: Owen (David Krumholtz) and Chloe (Denise Richards) are a youngish co...|\n",
      "|                                            translate English to French: This film just doesn't work|\n",
      "|translate English to French: The over-riding problem with this film is that it can't possibly use...|\n",
      "|translate English to French: This film revival right march in a bad film industry and Saudi Arabi...|\n",
      "|                                                                     translate English to French: OK|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_df = df.select(preprocess(col(\"text\"), \"translate English to French: \").alias(\"input\")).cache()\n",
    "input_df.show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9cbdd2-1ca6-48e4-a549-792b3726525b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference using Spark DL API\n",
    "\n",
    "Distributed inference using the PySpark [predict_batch_udf](https://spark.apache.org/docs/3.4.0/api/python/reference/api/pyspark.ml.functions.predict_batch_udf.html#pyspark.ml.functions.predict_batch_udf):\n",
    "\n",
    "- predict_batch_fn uses PyTorch APIs to load the model and return a predict function which operates on numpy arrays \n",
    "- predict_batch_udf will convert the Spark DataFrame columns into numpy input batches for the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb81177-442d-42ab-b86d-d8792201b4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    from pyspark import TaskContext\n",
    "\n",
    "    print(f\"Initializing model on worker {TaskContext.get().partitionId()}.\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device} device.\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "    def predict(inputs):\n",
    "        flattened = np.squeeze(inputs).tolist()\n",
    "        inputs = tokenizer(flattened, \n",
    "                           padding=True,\n",
    "                           return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(input_ids=inputs[\"input_ids\"],\n",
    "                                 attention_mask=inputs[\"attention_mask\"],\n",
    "                                 max_length=128)\n",
    "        string_outputs = np.array([tokenizer.decode(o, skip_special_tokens=True) for o in outputs])\n",
    "        print(\"predict: {}\".format(len(flattened)))\n",
    "        return string_outputs\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20aab3a1-2284-4c07-9ce1-a20cf54d88f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=StringType(),\n",
    "                             batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d6f48e-09e7-4fc7-9d2f-1b68bc2976a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.314849376678467 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# first pass caches model/fn\n",
    "preds = input_df.withColumn(\"preds\", generate(struct(\"input\")))\n",
    "results = preds.collect()\n",
    "print(f\"{time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe2271d-0077-48f6-98b1-93524dd86447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.648898124694824 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "preds = input_df.withColumn(\"preds\", generate(\"input\"))\n",
    "results = preds.collect()\n",
    "print(f\"{time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77623711-a742-4262-8839-16fc3ddd1af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.734241485595703 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "preds = input_df.withColumn(\"preds\", generate(col(\"input\")))\n",
    "results = preds.collect()\n",
    "print(f\"{time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f339c654-52fd-4992-b054-188dfb260e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                             input|                                             preds|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|translate English to French: 1983's \"Frightmare...|Le film \"Frightmare\" de 1983 est un petit film ...|\n",
      "|translate English to French: The film made no s...|                 Le film n'a rien de sens pour moi|\n",
      "|translate English to French: I loved the first ...|J'ai aimé les 15 premières minutes et j'ai aimé...|\n",
      "|translate English to French: Don't tell me this...|Ne me dit pas que ce film était amusant ou un p...|\n",
      "|translate English to French: I was really disap...|                Je suis vraiment déçu par ce film.|\n",
      "|translate English to French: It's about time fo...|Il est temps de faire un féminisme de boxe, mai...|\n",
      "|translate English to French: I'm actually too d...|Je suis en fait trop entraîné pour rédiger cett...|\n",
      "|translate English to French: Hello it is I Derr...|Bonjour, c'est moi Derrick Cannon et je vous so...|\n",
      "|translate English to French: Don't waste your time|                      Ne gaspillez pas votre temps|\n",
      "|translate English to French: The original title...|Le titre original m’a toujours frappé comme une...|\n",
      "|translate English to French: \"Three\" is a serio...|       \"Trois\" est un film d'épave gravement dilué|\n",
      "|translate English to French: I love movies in t...|          Je suis en amour des films dans ce genre|\n",
      "|translate English to French: The quote above ju...|La citation ci-dessus stipule tout simplement «...|\n",
      "|translate English to French: \"Fate\" leads Walte...|« Fate » mène Walter Sparrow à acquérir un roma...|\n",
      "|translate English to French: I felt I had to ad...|J’ai eu l’impression d’ajouter un commentaire a...|\n",
      "|translate English to French: Owen (David Krumho...|Owen (David Krumholtz) et Chloe (Denise Richard...|\n",
      "|translate English to French: This film just doe...|                         Ce film ne fonctionne pas|\n",
      "|translate English to French: The over-riding pr...|Le problème majeur de ce film est qu'il ne peut...|\n",
      "|translate English to French: This film revival ...|Ce film revival right march in a bad film indus...|\n",
      "|                   translate English to French: OK|                                                OK|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79a6f3a-cc34-46a4-aadd-16870423fffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Triton Inference Server\n",
    "In this section, we demonstrate integration with the [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server), an open-source, GPU-accelerated serving solution for DL.  \n",
    "We use [PyTriton](https://github.com/triton-inference-server/pytriton), a Flask-like framework that handles client/server communication with the Triton server.  \n",
    "\n",
    "The process looks like this:\n",
    "- Distribute a PyTriton task across the Spark cluster, instructing each node to launch a Triton server process.\n",
    "- Define a Triton inference function, which binds to the local server on a given node and sends inference requests.\n",
    "- Wrap the Triton inference function in a predict_batch_udf to launch parallel inference requests using Spark.\n",
    "- Finally, distribute a shutdown signal to terminate the Triton server processes on each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e73757e-a451-4835-98e0-257ccf7a9025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b1cb49-3d8f-4eeb-937a-c0c334bd2947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def triton_server():\n",
    "    import signal\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "    from pytriton.decorators import batch\n",
    "    from pytriton.model_config import DynamicBatcher, ModelConfig, Tensor\n",
    "    from pytriton.triton import Triton\n",
    "    from pyspark import TaskContext\n",
    "\n",
    "    with Triton() as triton:\n",
    "        print(f\"SERVER: Initializing Conditional Generation model on worker {TaskContext.get().partitionId()}.\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "        \n",
    "        DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"SERVER: Using {DEVICE} device.\")\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        @batch\n",
    "        def _infer_fn(**inputs):\n",
    "            sentences = np.squeeze(inputs[\"text\"]).tolist()\n",
    "            print(f\"SERVER: Received batch of size {len(sentences)}\")\n",
    "            decoded_sentences = [s.decode(\"utf-8\") for s in sentences]\n",
    "            inputs = tokenizer(decoded_sentences,\n",
    "                            padding=True,\n",
    "                            return_tensors=\"pt\").to(DEVICE)\n",
    "            output_ids = model.generate(input_ids=inputs[\"input_ids\"],\n",
    "                                        attention_mask=inputs[\"attention_mask\"],\n",
    "                                        max_length=128)\n",
    "            outputs = np.array([[tokenizer.decode(o, skip_special_tokens=True)] for o in output_ids])\n",
    "            return {\n",
    "                \"translations\": outputs,\n",
    "            }\n",
    "\n",
    "        triton.bind(\n",
    "            model_name=\"ConditionalGeneration\",\n",
    "            infer_func=_infer_fn,\n",
    "            inputs=[\n",
    "                Tensor(name=\"text\", dtype=object, shape=(-1,)),\n",
    "            ],\n",
    "            outputs=[\n",
    "                Tensor(name=\"translations\", dtype=object, shape=(-1,)),\n",
    "            ],\n",
    "            config=ModelConfig(\n",
    "                max_batch_size=64,\n",
    "                batcher=DynamicBatcher(max_queue_delay_microseconds=5000),  # 5ms\n",
    "            ),\n",
    "            strict=True,\n",
    "        )\n",
    "\n",
    "        def stop_triton(signum, frame):\n",
    "            print(\"SERVER: Received SIGTERM. Stopping Triton server.\")\n",
    "            triton.stop()\n",
    "\n",
    "        signal.signal(signal.SIGTERM, stop_triton)\n",
    "\n",
    "        print(\"SERVER: Serving inference\")\n",
    "        triton.serve()\n",
    "\n",
    "def start_triton(url, model_name):\n",
    "    import socket\n",
    "    from multiprocessing import Process\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    hostname = socket.gethostname()\n",
    "    process = Process(target=triton_server)\n",
    "    process.start()\n",
    "\n",
    "    client = ModelClient(url, model_name)\n",
    "    ready = False\n",
    "    while not ready:\n",
    "        try:\n",
    "            client.wait_for_server(5)\n",
    "            ready = True\n",
    "        except Exception as e:\n",
    "            print(f\"Waiting for server to be ready: {e}\")\n",
    "    \n",
    "    return [(hostname, process.pid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf14846-15a3-4bc8-b0c5-ce71680d3550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Start Triton servers\n",
    "\n",
    "To ensure that only one Triton inference server is started per node, we use stage-level scheduling to delegate each task to a separate GPU.  \n",
    "Stage-level scheudling requires a value for `spark.executor.cores`, and requires that `spark.executor.resource.gpu.amount` <= 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c40df2-161b-483d-9d10-e462ecfb9fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _use_stage_level_scheduling(spark, rdd):\n",
    "\n",
    "    if spark.version < \"3.4.0\":\n",
    "        raise Exception(\"Stage-level scheduling is not supported in Spark < 3.4.0\")\n",
    "\n",
    "    executor_cores = spark.conf.get(\"spark.executor.cores\")\n",
    "    assert executor_cores is not None, \"spark.executor.cores is not set\"\n",
    "    executor_gpus = spark.conf.get(\"spark.executor.resource.gpu.amount\")\n",
    "    assert executor_gpus is not None and int(executor_gpus) <= 1, \"spark.executor.resource.gpu.amount must be set and <= 1\"\n",
    "\n",
    "    from pyspark.resource.profile import ResourceProfileBuilder\n",
    "    from pyspark.resource.requests import TaskResourceRequests\n",
    "\n",
    "    # each training task requires cpu cores > total executor cores/2 which can\n",
    "    # ensure each training task be sent to different executor.\n",
    "    #\n",
    "    # Please note that we can't set task_cores to the value which is smaller than total executor cores/2\n",
    "    # because only task_gpus can't ensure the tasks be sent to different executor even task_gpus=1.0\n",
    "    #\n",
    "    # If spark-rapids enabled. we don't allow other ETL task running alongside training task to avoid OOM\n",
    "    spark_plugins = spark.conf.get(\"spark.plugins\", \" \")\n",
    "    assert spark_plugins is not None\n",
    "    spark_rapids_sql_enabled = spark.conf.get(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    assert spark_rapids_sql_enabled is not None\n",
    "\n",
    "    task_cores = (\n",
    "        int(executor_cores)\n",
    "        if \"com.nvidia.spark.SQLPlugin\" in spark_plugins\n",
    "        and \"true\" == spark_rapids_sql_enabled.lower()\n",
    "        else (int(executor_cores) // 2) + 1\n",
    "    )\n",
    "    # task_gpus means how many slots per gpu address the task requires,\n",
    "    # it does mean how many gpus it would like to require, so it can be any value of (0, 0.5] or 1.\n",
    "    task_gpus = 1.0\n",
    "\n",
    "    treqs = TaskResourceRequests().cpus(task_cores).resource(\"gpu\", task_gpus)\n",
    "    rp = ResourceProfileBuilder().require(treqs).build\n",
    "\n",
    "    print(f\"Reqesting stage-level resources: (cores={task_cores}, gpu={task_gpus})\")\n",
    "\n",
    "    return rdd.withResources(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf1fafc-d9c9-4fd7-901d-da97cf4ff496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=8, gpu=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Start servers (2 node cluster)\n",
    "num_nodes = 2\n",
    "url = \"localhost\"\n",
    "model_name = \"ConditionalGeneration\"\n",
    "\n",
    "sc = spark.sparkContext\n",
    "nodeRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "nodeRDD = _use_stage_level_scheduling(spark, nodeRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289b08fa-7916-44ea-8fe5-28821451db6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pids = nodeRDD.barrier().mapPartitions(lambda _: start_triton(url, model_name)).collectAsMap()\n",
    "# print(\"Triton Server PIDs:\\n\", json.dumps(pids, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e203eb19-166d-4177-aa87-fd31b7e3c90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def triton_fn(url, model_name, init_timeout_s):\n",
    "    import numpy as np\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    print(f\"Connecting to Triton model {model_name} at {url}.\")\n",
    "\n",
    "    def infer_batch(inputs):\n",
    "        with ModelClient(url, model_name, init_timeout_s=init_timeout_s) as client:\n",
    "            flattened = np.squeeze(inputs).tolist() \n",
    "            # Encode batch\n",
    "            encoded_batch = [[text.encode(\"utf-8\")] for text in flattened]\n",
    "            encoded_batch_np = np.array(encoded_batch, dtype=np.bytes_)\n",
    "            # Run inference\n",
    "            result_data = client.infer_batch(encoded_batch_np)\n",
    "            result_data = np.squeeze(result_data[\"translations\"], -1)\n",
    "            return result_data\n",
    "        \n",
    "    return infer_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6b2a05-aea4-4e4d-a87d-0a6bd5ab554c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load and preprocess DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e83230-5178-4fec-bba2-0e69be40e68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text: pd.Series, prefix: str = \"\") -> pd.Series:\n",
    "    @pandas_udf(\"string\")\n",
    "    def _preprocess(text: pd.Series) -> pd.Series:\n",
    "        return pd.Series([prefix + s.split(\".\")[0] for s in text])\n",
    "    return _preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad299b0-34bb-4edb-b1e4-cd0c82bb7455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(data_path).limit(1024).repartition(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7934a6fc-57bc-4104-a52c-076351e77cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/13 18:38:46 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "input_df = df.select(preprocess(col(\"text\"), \"translate English to French: \").alias(\"input\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be692f4a-cf86-4cf4-9530-7c62e479cacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate = predict_batch_udf(partial(triton_fn, url=url, model_name=model_name, init_timeout_s=600),\n",
    "                             return_type=StringType(),\n",
    "                             input_tensor_shapes=[[1]],\n",
    "                             batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f6229ef-01c8-43c9-a259-c5df6a18d689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 3.57 ms, total: 26.5 ms\n",
      "Wall time: 17.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "preds = input_df.withColumn(\"preds\", generate(struct(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a543b4c-8b29-4f61-9773-2639bbc7f728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 4.63 ms, total: 20.5 ms\n",
      "Wall time: 15.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = input_df.withColumn(\"preds\", generate(\"input\"))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0cfc4e-ef0a-435e-9fdf-72b72b6def93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 ms, sys: 4.27 ms, total: 24.2 ms\n",
      "Wall time: 15.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = input_df.withColumn(\"preds\", generate(col(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d756e2e-8b60-43cb-b5f9-e27de11be24d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                             input|                                             preds|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|translate English to French: 1983's \"Frightmare...|Le film \"Frightmare\" de 1983 est un petit film ...|\n",
      "|translate English to French: The film made no s...|                 Le film n'a rien de sens pour moi|\n",
      "|translate English to French: I loved the first ...|J'ai aimé les 15 premières minutes et j'ai aimé...|\n",
      "|translate English to French: Don't tell me this...|Ne me dit pas que ce film était amusant ou un p...|\n",
      "|translate English to French: I was really disap...|                Je suis vraiment déçu par ce film.|\n",
      "|translate English to French: It's about time fo...|Il est temps de faire un féminisme de boxe, mai...|\n",
      "|translate English to French: I'm actually too d...|Je suis en fait trop entraîné pour rédiger cett...|\n",
      "|translate English to French: Hello it is I Derr...|Bonjour, c'est moi Derrick Cannon et je vous so...|\n",
      "|translate English to French: Don't waste your time|                      Ne gaspillez pas votre temps|\n",
      "|translate English to French: The original title...|Le titre original m’a toujours frappé comme une...|\n",
      "|translate English to French: \"Three\" is a serio...|       \"Trois\" est un film d'épave gravement dilué|\n",
      "|translate English to French: I love movies in t...|          Je suis en amour des films dans ce genre|\n",
      "|translate English to French: The quote above ju...|La citation ci-dessus stipule tout simplement «...|\n",
      "|translate English to French: \"Fate\" leads Walte...|« Fate » mène Walter Sparrow à acquérir un roma...|\n",
      "|translate English to French: I felt I had to ad...|J’ai eu l’impression d’ajouter un commentaire a...|\n",
      "|translate English to French: Owen (David Krumho...|Owen (David Krumholtz) et Chloe (Denise Richard...|\n",
      "|translate English to French: This film just doe...|                         Ce film ne fonctionne pas|\n",
      "|translate English to French: The over-riding pr...|Le problème majeur de ce film est qu'il ne peut...|\n",
      "|translate English to French: This film revival ...|Ce film revival right march in a bad film indus...|\n",
      "|                   translate English to French: OK|                                                OK|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ae68d4-57da-41d9-91b4-625ef9465d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Shut down servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fd4601-f6d5-4ddf-9b5e-d918ab0adf3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=8, gpu=1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(pids):\n",
    "    import os\n",
    "    import socket\n",
    "    import signal\n",
    "    import time \n",
    "    \n",
    "    hostname = socket.gethostname()\n",
    "    pid = pids.get(hostname, None)\n",
    "    assert pid is not None, \"Could not find pid for {hostname}\"\n",
    "    os.kill(pid, signal.SIGTERM)\n",
    "    time.sleep(7)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            os.kill(pid, 0)\n",
    "        except OSError:\n",
    "            return [True]\n",
    "        time.sleep(3)\n",
    "\n",
    "    return [False]\n",
    "\n",
    "shutdownRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "shutdownRDD = _use_stage_level_scheduling(spark, shutdownRDD)\n",
    "shutdownRDD.barrier().mapPartitions(lambda _: stop_triton(pids)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008c3e50-d321-4431-a9ab-919b35d1b042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 421988607303514,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-triton-db.ipynb",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
