{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792d95f9",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_efficientnet/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "# PySpark PyTorch Inference\n",
    "\n",
    "### Regression\n",
    "\n",
    "In this notebook, we will train an MLP to perform regression on the California housing dataset, and load it for distributed inference with Spark.  \n",
    "\n",
    "Based on: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-neural-network-for-regression-with-pytorch.md  \n",
    "\n",
    "We also demonstrate accelerated inference via Torch-TensorRT model compilation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75930360-c5ce-49ef-a69a-da88fa69a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de685f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('models') if not os.path.exists('models') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5bc0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754b174",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Each label corresponds to the average house value in units of 100,000, which we'll try to predict using the following features:  \n",
    "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bee64cf-a44a-4aff-82db-c64ee3a8b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8644e508-5e4c-4cdd-9ed1-9235887d9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X.astype(np.float32))\n",
    "            self.y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6b55c3-dc7b-4831-9943-83efd48091bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HousingDataset(X, y)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d868f39d-4695-4110-91d2-6f7a09d73b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.5241, -0.8454, -0.2556,  0.3256,  0.8226,  0.0041,  0.7623, -1.1132],\n",
       "         [-0.3852, -1.1632, -0.3439, -0.0999, -0.3060, -0.0110, -0.5767,  0.0198],\n",
       "         [ 0.0802,  0.5054, -0.0854, -0.0261,  0.1170,  0.0557, -0.8062,  0.7485],\n",
       "         [-0.7215, -0.5276, -0.2073, -0.1787, -0.6115, -0.0134,  1.3756, -0.8686],\n",
       "         [-0.9009, -0.4481, -0.4374, -0.1855, -0.1788,  0.0477,  1.0760, -0.8537],\n",
       "         [ 0.6685,  1.3794,  0.2930, -0.1939, -0.8066, -0.0586,  0.9402, -1.1731],\n",
       "         [-0.7873, -0.9249,  0.1057, -0.0384,  0.3025, -0.0482,  0.1817,  0.2794],\n",
       "         [ 0.2764,  1.8562, -1.3384, -0.3592, -0.3219,  1.2067,  1.0010, -1.4127],\n",
       "         [-0.2856, -1.7194, -0.1692,  0.0904,  1.0177, -0.0792, -0.6938,  1.1279],\n",
       "         [ 1.3093, -0.2097,  0.5685,  0.0202, -0.2477, -0.0660,  0.9121, -1.4127]]),\n",
       " tensor([1.8440, 2.4870, 1.5770, 1.0160, 0.6780, 3.1560, 0.7980, 2.2500, 1.2220,\n",
       "         5.0000])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e817b9a",
   "metadata": {},
   "source": [
    "### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a441b60-dca4-44d2-bc1c-aa7336d704bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15cff2b4-9d23-4d2b-808a-a5edb8eda135",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "mlp = MLP().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2db3f9-5db8-4b42-89ad-e77f23c4c1fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   201: 0.709\n",
      "Loss after mini-batch   401: 0.505\n",
      "Loss after mini-batch   601: 0.379\n",
      "Loss after mini-batch   801: 0.305\n",
      "Loss after mini-batch  1001: 0.269\n",
      "Loss after mini-batch  1201: 0.257\n",
      "Loss after mini-batch  1401: 0.227\n",
      "Loss after mini-batch  1601: 0.214\n",
      "Loss after mini-batch  1801: 0.223\n",
      "Loss after mini-batch  2001: 0.223\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   201: 0.214\n",
      "Loss after mini-batch   401: 0.206\n",
      "Loss after mini-batch   601: 0.200\n",
      "Loss after mini-batch   801: 0.199\n",
      "Loss after mini-batch  1001: 0.203\n",
      "Loss after mini-batch  1201: 0.197\n",
      "Loss after mini-batch  1401: 0.197\n",
      "Loss after mini-batch  1601: 0.197\n",
      "Loss after mini-batch  1801: 0.188\n",
      "Loss after mini-batch  2001: 0.193\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   201: 0.182\n",
      "Loss after mini-batch   401: 0.186\n",
      "Loss after mini-batch   601: 0.195\n",
      "Loss after mini-batch   801: 0.195\n",
      "Loss after mini-batch  1001: 0.190\n",
      "Loss after mini-batch  1201: 0.186\n",
      "Loss after mini-batch  1401: 0.185\n",
      "Loss after mini-batch  1601: 0.185\n",
      "Loss after mini-batch  1801: 0.190\n",
      "Loss after mini-batch  2001: 0.186\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   201: 0.179\n",
      "Loss after mini-batch   401: 0.182\n",
      "Loss after mini-batch   601: 0.187\n",
      "Loss after mini-batch   801: 0.183\n",
      "Loss after mini-batch  1001: 0.182\n",
      "Loss after mini-batch  1201: 0.187\n",
      "Loss after mini-batch  1401: 0.179\n",
      "Loss after mini-batch  1601: 0.183\n",
      "Loss after mini-batch  1801: 0.177\n",
      "Loss after mini-batch  2001: 0.184\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   201: 0.174\n",
      "Loss after mini-batch   401: 0.181\n",
      "Loss after mini-batch   601: 0.183\n",
      "Loss after mini-batch   801: 0.176\n",
      "Loss after mini-batch  1001: 0.179\n",
      "Loss after mini-batch  1201: 0.176\n",
      "Loss after mini-batch  1401: 0.183\n",
      "Loss after mini-batch  1601: 0.177\n",
      "Loss after mini-batch  1801: 0.183\n",
      "Loss after mini-batch  2001: 0.172\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 5):  # 5 epochs at maximum\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 200 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352539f5",
   "metadata": {},
   "source": [
    "### Save Model State Dict\n",
    "This saves the serialized object to disk using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b950a3ed-ffe1-477f-a84f-f71c85dbf9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to models/housing_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(mlp.state_dict(), \"models/housing_model.pt\")\n",
    "print(\"Saved PyTorch Model State to models/housing_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060fcca",
   "metadata": {},
   "source": [
    "### Save Model as TorchScript\n",
    "This saves an [intermediate representation of the compute graph](https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format), which does not require pickle (or even python). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fedb5d-c59e-4b0b-ba91-3dd15df1f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript Model to models/ts_housing_model.pt\n"
     ]
    }
   ],
   "source": [
    "scripted = torch.jit.script(mlp)\n",
    "scripted.save(\"models/ts_housing_model.pt\")\n",
    "print(\"Saved TorchScript Model to models/ts_housing_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101c0fe-65f1-411e-9192-e8a6b585ba0d",
   "metadata": {},
   "source": [
    "### Load and Test from Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7411b00f-88d2-40f5-b716-a26733c968ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_mlp = MLP().to(device)\n",
    "loaded_mlp.load_state_dict(torch.load(\"models/housing_model.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e226f449-2931-4492-9003-503cdc61f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d46af47e-db7e-42ee-9bd3-6e7d93850be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.0423],\n",
       "        [1.9258],\n",
       "        [2.8864],\n",
       "        [3.2128],\n",
       "        [2.8639],\n",
       "        [1.0359],\n",
       "        [2.8652],\n",
       "        [1.5528],\n",
       "        [1.7592],\n",
       "        [0.7497]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "loaded_mlp(testX.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ae2c0f-1da5-45a4-bf32-ed8b562d7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2030, 2.1590, 1.9400, 3.4310, 2.4480, 1.1410, 2.8780, 0.8310, 1.6530,\n",
       "        1.2380])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Labels:\")\n",
    "testY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd329d",
   "metadata": {},
   "source": [
    "### Load and Test from TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "422e317f-c9bd-4f76-9463-7af2935d401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp = torch.jit.load(\"models/ts_housing_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cda8ec8-644e-4888-bfa0-b79425ece7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0423, 1.9258, 2.8864, 3.2128, 2.8639, 1.0359, 2.8652, 1.5528, 1.7592,\n",
       "        0.7497], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "scripted_mlp(testX.to(device)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b64e4",
   "metadata": {},
   "source": [
    "### Compile using the Torch JIT Compiler\n",
    "This leverages the [Torch-TensorRT inference compiler](https://pytorch.org/TensorRT/) for accelerated inference on GPUs using the `torch.compile` JIT interface under the hood. The compiler stack returns a [boxed-function](http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/) that triggers compilation on the first call.  \n",
    "\n",
    "Modules compiled in this fashion are [not serializable with pickle](https://github.com/pytorch/pytorch/issues/101107#issuecomment-1542688089), so we cannot send the compiled model directly to Spark.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613f24e",
   "metadata": {},
   "source": [
    "(You may see a warning about modelopt quantization. This is safe to ignore, as [implicit quantization](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#intro-quantization) is deprecated in the latest TensorRT. See [this link](https://pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/vgg16_fp8_ptq.html) for a guide to explicit quantization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ffb27fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n"
     ]
    }
   ],
   "source": [
    "import torch_tensorrt as trt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c10f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: set the filename for the TensorRT timing cache\n",
    "timestamp = time.time()\n",
    "timing_cache = f\"/tmp/timing_cache-{timestamp}.bin\"\n",
    "with open(timing_cache, \"wb\") as f:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4aa2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_bs1 = torch.randn((10, 8), dtype=torch.float).to(\"cuda\")\n",
    "# This indicates dimension 0 of inputs_bs1 is dynamic with a range of values [1, 50]. No recompilation will happen when the batch size changes.\n",
    "torch._dynamo.mark_dynamic(inputs_bs1, 0, min=1, max=50)\n",
    "trt_model = trt.compile(\n",
    "    loaded_mlp,\n",
    "    ir=\"torch_compile\",\n",
    "    inputs=inputs_bs1,\n",
    "    enabled_precisions={torch.float},\n",
    "    timing_cache_path=timing_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5da8cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)\n",
      "INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.\n",
      "INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.f32: 7>}, debug=False, workspace_size=0, min_block_size=5, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False, timing_cache_path='/tmp/timing_cache-1738007491.6462495.bin')\n",
      "\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Node _param_constant1 of op type get_attr does not have metadata. This could sometimes lead to undefined behavior.\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Some nodes do not have metadata (shape and dtype information). This could lead to problems sometimes if the graph has PyTorch and TensorRT segments.\n",
      "INFO:torch_tensorrt.dynamo._compiler:Partitioning the graph via the fast partitioner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 581, GPU 2466 (MiB)\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init builder kernel library: CPU +1635, GPU +288, now: CPU 2363, GPU 2754 (MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/torch_tensorrt/dynamo/conversion/impl/activation/base.py:40: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  if input_val.dynamic_range is not None and dyn_range_fn is not None:\n",
      "\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.008361\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Detected 1 inputs and 1 output network tensors.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Host Persistent Memory: 22240\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Device Persistent Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Scratch Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Started assigning block shifts. This will take 10 steps to complete.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Algorithm ShiftNTopDown took 0.147039ms to assign 4 blocks to 10 nodes requiring 7168 bytes.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Activation Memory: 6656\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Weights Memory: 11648\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Engine generation completed in 1.60138 seconds.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 1 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 4106 MiB\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:01.606208\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 390420 bytes of Memory\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 26 bytes of code generator cache.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 52 timing cache entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0423],\n",
      "        [1.9258],\n",
      "        [2.8864],\n",
      "        [3.2128],\n",
      "        [2.8639],\n",
      "        [1.0359],\n",
      "        [2.8652],\n",
      "        [1.5528],\n",
      "        [1.7592],\n",
      "        [0.7497]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "stream = torch.cuda.Stream()\n",
    "with torch.no_grad(), torch.cuda.stream(stream):\n",
    "    testX = testX.to(device)\n",
    "    print(\"Predictions:\")\n",
    "    print(trt_model(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c55e07",
   "metadata": {},
   "source": [
    "### Compile using the Torch-TensorRT AOT Compiler\n",
    "Alternatively, use the Torch-TensorRT Dynamo backend for Ahead-of-Time (AOT) compilation to eagerly optimize the model in an explicit compilation phase. We first export the model to produce a traced graph representing the Tensor computation in an AOT fashion, which produces a `ExportedProgram` object which can be [serialized and reloaded](https://pytorch.org/TensorRT/user_guide/saving_models.html). We can then compile this IR using the Torch-TensorRT AOT compiler for inference.   \n",
    "\n",
    "[Read the docs](https://pytorch.org/TensorRT/user_guide/torch_tensorrt_explained.html) for more information on JIT vs AOT compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf36a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt.dynamo._compiler:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.f32: 7>}, debug=False, workspace_size=1073741824, min_block_size=5, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False, timing_cache_path='/tmp/timing_cache-1738007491.6462495.bin')\n",
      "\n",
      "INFO:torch_tensorrt.dynamo._compiler:Partitioning the graph via the fast partitioner\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 892, GPU 2468 (MiB)\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init builder kernel library: CPU +1633, GPU +286, now: CPU 2525, GPU 2754 (MiB)\n",
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/torch_tensorrt/dynamo/conversion/impl/activation/base.py:40: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  if input_val.dynamic_range is not None and dyn_range_fn is not None:\n",
      "\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.002784\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Detected 1 inputs and 1 output network tensors.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Host Persistent Memory: 18368\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Device Persistent Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Scratch Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Started assigning block shifts. This will take 6 steps to complete.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Algorithm ShiftNTopDown took 0.237457ms to assign 3 blocks to 6 nodes requiring 25088 bytes.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Activation Memory: 24576\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Weights Memory: 12292\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Engine generation completed in 1.33069 seconds.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 4131 MiB\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:01.332388\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 212892 bytes of Memory\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 26 bytes of code generator cache.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 100 timing cache entries\n"
     ]
    }
   ],
   "source": [
    "example_inputs = (torch.randn((10, 8), dtype=torch.float).to(\"cuda\"),)\n",
    "\n",
    "# Mark dim 1 (batch size) as dynamic\n",
    "batch = torch.export.Dim(\"batch\", min=1, max=64)\n",
    "# Produce traced graph in ExportedProgram format\n",
    "exp_program = torch.export.export(loaded_mlp, args=example_inputs, dynamic_shapes={\"x\": {0: batch}})\n",
    "# Compile the traced graph to produce an optimized module\n",
    "trt_gm = trt.dynamo.compile(exp_program,\n",
    "                            tuple(example_inputs),\n",
    "                            enabled_precisions={torch.float},\n",
    "                            timing_cache_path=timing_cache,\n",
    "                            workspace_size=1<<30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc4efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.export.exported_program.ExportedProgram'>\n",
      "<class 'torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl'>\n"
     ]
    }
   ],
   "source": [
    "print(type(exp_program))\n",
    "print(type(trt_gm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bcf9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[2.0425],\n",
      "        [1.9257],\n",
      "        [2.8869],\n",
      "        [3.2127],\n",
      "        [2.8640],\n",
      "        [1.0362],\n",
      "        [2.8658],\n",
      "        [1.5528],\n",
      "        [1.7586],\n",
      "        [0.7496]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "stream = torch.cuda.Stream()\n",
    "with torch.no_grad(), torch.cuda.stream(stream):\n",
    "    print(\"Predictions:\")\n",
    "    testX = testX.to(device)\n",
    "    print(trt_gm(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb957a",
   "metadata": {},
   "source": [
    "We can run the optimized module with a few different batch sizes (without recompilation!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49f72c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "torch.Size([10, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = (torch.randn((10, 8), dtype=torch.float).cuda(),)\n",
    "inputs_bs1 = (torch.randn((1, 8), dtype=torch.float).cuda(),)\n",
    "inputs_bs50 = (torch.randn((50, 8), dtype=torch.float).cuda(),)\n",
    "\n",
    "stream = torch.cuda.Stream()\n",
    "with torch.no_grad(), torch.cuda.stream(stream):\n",
    "    print(\"Output shapes:\")\n",
    "    print(trt_gm(*inputs).shape)\n",
    "    print(trt_gm(*inputs_bs1).shape)\n",
    "    print(trt_gm(*inputs_bs50).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fef57d",
   "metadata": {},
   "source": [
    "We can serialize the ExportedProgram (a traced graph representing the model's forward function) using `torch.export.save` to be recompiled at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "876fea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ExportedProgram to models/trt_housing_model.ep\n"
     ]
    }
   ],
   "source": [
    "torch.export.save(exp_program, \"models/trt_housing_model.ep\")\n",
    "print(\"Saved ExportedProgram to models/trt_housing_model.ep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13631d1f-2c71-4bee-afcb-bd3b55ec87c5",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb71dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.\n",
      "  from typing.io import BinaryIO  # type: ignore[import]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, struct, pandas_udf, array\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769c060",
   "metadata": {},
   "source": [
    "Check the cluster environment to handle any platform-specific Spark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7727b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_databricks = os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", False)\n",
    "on_dataproc = os.environ.get(\"DATAPROC_IMAGE_VERSION\", False)\n",
    "on_standalone = not (on_databricks or on_dataproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7d360",
   "metadata": {},
   "source": [
    "#### Create Spark Session\n",
    "\n",
    "For local standalone clusters, we'll connect to the cluster and create the Spark Session.  \n",
    "For CSP environments, Spark will either be preconfigured (Databricks) or we'll need to create the Spark Session (Dataproc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52e9dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/27 19:51:37 WARN Utils: Your hostname, cb4ae00-lcedt resolves to a loopback address: 127.0.1.1; using 10.110.47.100 instead (on interface eno1)\n",
      "25/01/27 19:51:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/27 19:51:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/27 19:51:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    if on_standalone:\n",
    "        import socket\n",
    "        conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "        hostname = socket.gethostname()\n",
    "        conf.setMaster(f\"spark://{hostname}:7077\")\n",
    "        conf.set(\"spark.pyspark.python\", f\"{conda_env}/bin/python\")\n",
    "        conf.set(\"spark.pyspark.driver.python\", f\"{conda_env}/bin/python\")\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_env}/lib:{conda_env}/lib/python3.11/site-packages/nvidia_pytriton.libs:$LD_LIBRARY_PATH\")\n",
    "    elif on_dataproc:\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conda_lib_path=\"/opt/conda/miniconda3/lib\"\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_lib_path}:$LD_LIBRARY_PATH\")\n",
    "        conf.set(\"spark.executor.instances\", \"4\") # dataproc defaults to 2\n",
    "\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.125\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "    \n",
    "spark = SparkSession.builder.appName(\"spark-dl-examples\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9937e-2c70-4d67-b95f-4d9d5ab17c12",
   "metadata": {},
   "source": [
    "### Create Spark DataFrame from Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf35da14-61a3-4e7b-9d4f-086bf5e931b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95148019-ea95-40e5-a529-fcdb9a06f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(housing.data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f82d957c-6747-4408-aac8-45305afbfe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(X, columns=housing.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "881afee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:229: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(s.dtype):\n",
      "\n",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+\n",
      "|      MedInc|    HouseAge|   AveRooms|   AveBedrms| Population|    AveOccup|  Latitude|   Longitude|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+\n",
      "|  0.20909257|  -1.1632254| 0.38946992|  0.04609274| -0.9806099| -0.07099328|0.61245227|-0.020113053|\n",
      "|-0.098627955|  0.34647804| 0.27216315|  -0.0129226| -0.6953838| -0.05380849| 1.0665938|  -1.2479742|\n",
      "| -0.66006273|   1.0616008|-0.55292207| -0.48945764|-0.13641118| 0.028952759| 1.1040496|  -1.3827378|\n",
      "|  0.08218294|   0.5848523|-0.13912922| -0.14707813|-0.19116047| -0.07136432|0.96827507|  -1.3028787|\n",
      "|   0.0784456|  -1.4810578| 0.57265776|  0.32067496|  1.0345173|-0.024157424| 1.4411427| -0.52423614|\n",
      "| -0.82318723| -0.36864465| 0.07829511|  -0.1808107|-0.67242444|-0.061470542| 1.9374212|  -1.0083897|\n",
      "|  0.59671736|   0.5848523| 0.19346413|  -0.1371872|-0.19645879| 0.009964322|0.96827507|  -1.2928978|\n",
      "|  -0.9612035|  -1.5605159|-0.56329846| 0.027148023|-0.71127874| -0.08471591| 0.5328614| -0.13990337|\n",
      "| -0.74344087|  -1.2426835| 0.27282518|   0.4037246| -0.9841421| -0.05610115| 1.2257773| -0.42940006|\n",
      "|   0.9784464|  -0.2891866| 0.24374022| -0.24670053| 0.28922042| -0.01102468| 1.1087307|  -1.2280084|\n",
      "|  -0.5070446|  -1.0043093|-0.78254056|0.0122275995|  2.8465424|-0.060435444| 0.8980464|  -1.2080427|\n",
      "| -0.18690155|   1.2205169|0.015323491|  0.12183313|-0.41015765|  0.04452552|  1.010412|  -1.3228445|\n",
      "|  -1.2551856|   1.6178073| -0.3341509|-0.060125165| -0.7554314| -0.08777025| 1.0291398|  -1.3477987|\n",
      "|   4.9607058|  -1.9578062|  1.4854684| -0.03948475|  2.1833694|0.0029250523|  1.024457|  -1.1581304|\n",
      "|  0.73652315|  -1.6399739|  0.7913185| -0.05238397|    1.67738|  0.01944797| 1.0993668|  -1.1331724|\n",
      "|   -0.505834|  0.18756187|-0.47093546| -0.24297306|-0.60619545| -0.10791535|  0.977639|  -1.2879055|\n",
      "| -0.88477343|-0.050812364| -0.6318951| -0.15244243| -0.5258376| -0.15618815| 0.9823201|  -1.2879055|\n",
      "| -0.42840376|   0.9821427| -0.2266495| -0.36083496| -0.6883194| -0.08552282| 0.5328614| -0.12493005|\n",
      "|   0.9369153|  -1.4810578|  0.6722208|-0.121177554|  0.3996021|  0.01291408| 1.1040496|  -1.1082181|\n",
      "| -0.80702734| -0.92485124|-0.26602685|  -0.1560743|  1.4398388| -0.09314839|0.55627036| -0.09498342|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"MedInc\",FloatType(),True),\n",
    "    StructField(\"HouseAge\",FloatType(),True),\n",
    "    StructField(\"AveRooms\",FloatType(),True),\n",
    "    StructField(\"AveBedrms\",FloatType(),True),\n",
    "    StructField(\"Population\",FloatType(),True),\n",
    "    StructField(\"AveOccup\",FloatType(),True),\n",
    "    StructField(\"Latitude\",FloatType(),True),\n",
    "    StructField(\"Longitude\",FloatType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(pdf, schema=schema).repartition(8)\n",
    "df.show(truncate=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b33d367-fbf9-4918-b755-5447125547c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('MedInc', FloatType(), True), StructField('HouseAge', FloatType(), True), StructField('AveRooms', FloatType(), True), StructField('AveBedrms', FloatType(), True), StructField('Population', FloatType(), True), StructField('AveOccup', FloatType(), True), StructField('Latitude', FloatType(), True), StructField('Longitude', FloatType(), True)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "751bff7a-b687-4184-b3fa-b5f5b46ef5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"spark-dl-datasets/california_housing\"\n",
    "if on_databricks:\n",
    "    dbutils.fs.mkdirs(\"/FileStore/spark-dl-datasets\")\n",
    "    data_path = \"dbfs:/FileStore/\" + data_path\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3cd75",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API\n",
    "\n",
    "Distributed inference using the PySpark [predict_batch_udf](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.functions.predict_batch_udf.html#pyspark.ml.functions.predict_batch_udf):\n",
    "\n",
    "- predict_batch_fn uses PyTorch APIs to load the model and return a predict function which operates on numpy arrays \n",
    "- predict_batch_udf will convert the Spark DataFrame columns into numpy input batches for the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e40c266-24de-454d-a776-f3716ba50e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b144c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d608e2f-66a8-44b5-9cde-5f7837bf4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get absolute path to model\n",
    "model_path = \"{}/models/trt_housing_model.ep\".format(os.getcwd())\n",
    "\n",
    "# For cloud environments, copy the model to the distributed file system.\n",
    "if on_databricks:\n",
    "    dbutils.fs.mkdirs(\"/FileStore/spark-dl-models\")\n",
    "    dbfs_model_path = \"/dbfs/FileStore/spark-dl-models/trt_housing_model.ep\"\n",
    "    shutil.copy(model_path, dbfs_model_path)\n",
    "    model_path = dbfs_model_path\n",
    "elif on_dataproc:\n",
    "    # GCS is mounted at /mnt/gcs by the init script\n",
    "    models_dir = \"/mnt/gcs/spark-dl/models\"\n",
    "    os.mkdir(models_dir) if not os.path.exists(models_dir) else None\n",
    "    gcs_model_path = models_dir + \"/trt_housing_model.ep\"\n",
    "    shutil.copy(model_path, gcs_model_path)\n",
    "    model_path = gcs_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd143e7",
   "metadata": {},
   "source": [
    "For inference on Spark, we'll load the ExportedProgram and compile the model with the Torch-TensorRT AOT compiler and cache on the executor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc400771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A resource warning may occur due to unclosed file descriptors used by TensorRT across multiple PySpark daemon processes.\n",
    "# These can be safely ignored as the resources will be cleaned up when the worker processes terminate.\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2f45f5d-c941-4197-a274-1eec2af3fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import torch\n",
    "    import torch_tensorrt as trt\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        raise ValueError(\"This function uses the TensorRT model which requires a GPU device\")\n",
    "\n",
    "    example_inputs = (torch.randn((50, 8), dtype=torch.float).to(\"cuda\"),)\n",
    "    exp_program = torch.export.load(model_path)\n",
    "    trt_gm = trt.dynamo.compile(exp_program,\n",
    "                            tuple(example_inputs),\n",
    "                            enabled_precisions={torch.float},\n",
    "                            timing_cache_path=timing_cache,\n",
    "                            workspace_size=1<<30)\n",
    "\n",
    "    print(\"Model compiled.\")\n",
    "    \n",
    "    def predict(inputs):\n",
    "        stream = torch.cuda.Stream()\n",
    "        with torch.no_grad(), torch.cuda.stream(stream), trt.logging.errors():\n",
    "            print(f\"Predict {inputs.shape}\")\n",
    "            torch_inputs = torch.from_numpy(inputs).to(device)\n",
    "            outputs = trt_gm(torch_inputs) # .flatten()\n",
    "            return outputs.detach().cpu().numpy()\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "220a00a4-e842-4f5d-a4b3-7693d09e2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=FloatType(),\n",
    "                             input_tensor_shapes=[[8]],\n",
    "                             batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f3bf287-8ffc-4456-8772-e97c418d6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.2 ms, sys: 4.38 ms, total: 33.6 ms\n",
      "Wall time: 8.23 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", regress(struct(*columns)))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cd23b71-296d-4ce7-b56c-567cc2eec79c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.7 ms, sys: 6.15 ms, total: 36.8 ms\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75d16bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 ms, sys: 4.6 ms, total: 31.9 ms\n",
      "Wall time: 276 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "764a40d8-25f7-425c-ba03-fe8c45f4b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+---------+\n",
      "|      MedInc|    HouseAge|   AveRooms|   AveBedrms| Population|    AveOccup|  Latitude|   Longitude|    preds|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+---------+\n",
      "|  0.20909257|  -1.1632254| 0.38946992|  0.04609274| -0.9806099| -0.07099328|0.61245227|-0.020113053|1.4441836|\n",
      "|-0.098627955|  0.34647804| 0.27216315|  -0.0129226| -0.6953838| -0.05380849| 1.0665938|  -1.2479742|1.7245855|\n",
      "| -0.66006273|   1.0616008|-0.55292207| -0.48945764|-0.13641118| 0.028952759| 1.1040496|  -1.3827378|1.3524103|\n",
      "|  0.08218294|   0.5848523|-0.13912922| -0.14707813|-0.19116047| -0.07136432|0.96827507|  -1.3028787|2.3009148|\n",
      "|   0.0784456|  -1.4810578| 0.57265776|  0.32067496|  1.0345173|-0.024157424| 1.4411427| -0.52423614| 1.272077|\n",
      "| -0.82318723| -0.36864465| 0.07829511|  -0.1808107|-0.67242444|-0.061470542| 1.9374212|  -1.0083897|0.6968852|\n",
      "|  0.59671736|   0.5848523| 0.19346413|  -0.1371872|-0.19645879| 0.009964322|0.96827507|  -1.2928978|2.6057932|\n",
      "|  -0.9612035|  -1.5605159|-0.56329846| 0.027148023|-0.71127874| -0.08471591| 0.5328614| -0.13990337|1.0139045|\n",
      "| -0.74344087|  -1.2426835| 0.27282518|   0.4037246| -0.9841421| -0.05610115| 1.2257773| -0.42940006|0.9931088|\n",
      "|   0.9784464|  -0.2891866| 0.24374022| -0.24670053| 0.28922042| -0.01102468| 1.1087307|  -1.2280084|  2.64501|\n",
      "|  -0.5070446|  -1.0043093|-0.78254056|0.0122275995|  2.8465424|-0.060435444| 0.8980464|  -1.2080427|2.1324868|\n",
      "| -0.18690155|   1.2205169|0.015323491|  0.12183313|-0.41015765|  0.04452552|  1.010412|  -1.3228445|1.9459988|\n",
      "|  -1.2551856|   1.6178073| -0.3341509|-0.060125165| -0.7554314| -0.08777025| 1.0291398|  -1.3477987|1.2558049|\n",
      "|   4.9607058|  -1.9578062|  1.4854684| -0.03948475|  2.1833694|0.0029250523|  1.024457|  -1.1581304|5.8959594|\n",
      "|  0.73652315|  -1.6399739|  0.7913185| -0.05238397|    1.67738|  0.01944797| 1.0993668|  -1.1331724|2.0677836|\n",
      "|   -0.505834|  0.18756187|-0.47093546| -0.24297306|-0.60619545| -0.10791535|  0.977639|  -1.2879055|1.7652202|\n",
      "| -0.88477343|-0.050812364| -0.6318951| -0.15244243| -0.5258376| -0.15618815| 0.9823201|  -1.2879055|1.5843444|\n",
      "| -0.42840376|   0.9821427| -0.2266495| -0.36083496| -0.6883194| -0.08552282| 0.5328614| -0.12493005|0.9752624|\n",
      "|   0.9369153|  -1.4810578|  0.6722208|-0.121177554|  0.3996021|  0.01291408| 1.1040496|  -1.1082181|2.2553492|\n",
      "| -0.80702734| -0.92485124|-0.26602685|  -0.1560743|  1.4398388| -0.09314839|0.55627036| -0.09498342|1.0328484|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0aa85f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will clear the engine cache (containing previously compiled TensorRT engines) and reset the CUDA Context.\n",
    "torch._dynamo.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53536808",
   "metadata": {},
   "source": [
    "## Using Triton Inference Server\n",
    "In this section, we demonstrate integration with the [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server), an open-source, GPU-accelerated serving solution for DL.  \n",
    "We use [PyTriton](https://github.com/triton-inference-server/pytriton), a Flask-like framework that handles client/server communication with the Triton server.  \n",
    "\n",
    "The process looks like this:\n",
    "- Distribute a PyTriton task across the Spark cluster, instructing each node to launch a Triton server process.\n",
    "- Define a Triton inference function, which contains a client that binds to the local server on a given node and sends inference requests.\n",
    "- Wrap the Triton inference function in a predict_batch_udf to launch parallel inference requests using Spark.\n",
    "- Finally, distribute a shutdown signal to terminate the Triton server processes on each node.\n",
    "\n",
    "<img src=\"../images/spark-pytriton.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9ab4cdf-8103-447e-9ac8-944e2e527239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77dc96",
   "metadata": {},
   "source": [
    "Import the utility functions from pytriton_utils.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ac83062",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"pytriton_utils.py\")\n",
    "\n",
    "from pytriton_utils import (\n",
    "    use_stage_level_scheduling,\n",
    "    find_ports,\n",
    "    start_triton,\n",
    "    stop_triton\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc5d81",
   "metadata": {},
   "source": [
    "Define the Triton Server function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6632636e-67a3-406c-832c-758aac4245fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_server(ports, model_path):\n",
    "    import time\n",
    "    import signal\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch_tensorrt as trt\n",
    "    from pytriton.decorators import batch\n",
    "    from pytriton.model_config import DynamicBatcher, ModelConfig, Tensor\n",
    "    from pytriton.triton import Triton, TritonConfig\n",
    "    from pyspark import TaskContext\n",
    "\n",
    "    print(f\"SERVER: Initializing model on worker {TaskContext.get().partitionId()}.\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    exp_program = torch.export.load(model_path)\n",
    "    example_inputs = (torch.randn((50, 8), dtype=torch.float).to(\"cuda\"),)\n",
    "    trt_gm = trt.dynamo.compile(exp_program,\n",
    "                            tuple(example_inputs),\n",
    "                            enabled_precisions={torch.float},\n",
    "                            workspace_size=1<<30)\n",
    "\n",
    "    print(\"SERVER: Compiled model.\")\n",
    "\n",
    "    @batch\n",
    "    def _infer_fn(**inputs):\n",
    "        features = inputs[\"features\"]\n",
    "        if len(inputs[\"features\"]) != 1:\n",
    "            features = np.squeeze(features)\n",
    "        stream = torch.cuda.Stream()\n",
    "        with torch.no_grad(), torch.cuda.stream(stream):\n",
    "            torch_inputs = torch.from_numpy(features).to(device)\n",
    "            outputs = trt_gm(torch_inputs)\n",
    "            return {\n",
    "                \"preds\": outputs.cpu().numpy(),\n",
    "            }\n",
    "\n",
    "    workspace_path = f\"/tmp/triton_{time.strftime('%m_%d_%M_%S')}\"\n",
    "    triton_conf = TritonConfig(http_port=ports[0], grpc_port=ports[1], metrics_port=ports[2])\n",
    "    with Triton(config=triton_conf, workspace=workspace_path) as triton:\n",
    "        triton.bind(\n",
    "            model_name=\"HousingModel\",\n",
    "            infer_func=_infer_fn,\n",
    "            inputs=[\n",
    "                Tensor(name=\"features\", dtype=np.float32, shape=(-1,)),\n",
    "            ],\n",
    "            outputs=[\n",
    "                Tensor(name=\"preds\", dtype=np.float32, shape=(-1,)),\n",
    "            ],\n",
    "            config=ModelConfig(\n",
    "                max_batch_size=50,\n",
    "                batcher=DynamicBatcher(max_queue_delay_microseconds=5000),  # 5ms\n",
    "            ),\n",
    "            strict=True,\n",
    "        )\n",
    "\n",
    "        def _stop_triton(signum, frame):\n",
    "            print(\"SERVER: Received SIGTERM. Stopping Triton server.\")\n",
    "            triton.stop()\n",
    "\n",
    "        signal.signal(signal.SIGTERM, _stop_triton)\n",
    "\n",
    "        print(\"SERVER: Serving inference\")\n",
    "        triton.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74121cd7",
   "metadata": {},
   "source": [
    "#### Start Triton servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcca988",
   "metadata": {},
   "source": [
    "**Specify the number of nodes in the cluster.**  \n",
    "Following the README, the example standalone cluster uses 1 node. The example Databricks/Dataproc cluster scripts use 4 nodes by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "160a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change based on cluster setup\n",
    "num_nodes = 1 if on_standalone else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba4f54",
   "metadata": {},
   "source": [
    "To ensure that only one Triton inference server is started per node, we use stage-level scheduling to delegate each task to a separate GPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bca2f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "nodeRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "nodeRDD = use_stage_level_scheduling(spark, nodeRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76c554",
   "metadata": {},
   "source": [
    "Triton occupies ports for HTTP requests, GRPC requests, and the metrics service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba954d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ports [7000, 7001, 7002]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HousingModel\"\n",
    "ports = find_ports()\n",
    "assert len(ports) == 3\n",
    "print(f\"Using ports {ports}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5358d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton Server PIDs:\n",
      " {\n",
      "    \"cb4ae00-lcedt\": 2964321\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pids = nodeRDD.barrier().mapPartitions(lambda _: start_triton(triton_server_fn=triton_server,\n",
    "                                                              ports=ports,\n",
    "                                                              model_name=model_name,\n",
    "                                                              model_path=model_path)).collectAsMap()\n",
    "print(\"Triton Server PIDs:\\n\", json.dumps(pids, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ac038",
   "metadata": {},
   "source": [
    "#### Define client function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3812e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"http://localhost:{ports[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ae91c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fn(url, model_name):\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    print(f\"Connecting to Triton model {model_name} at {url}.\")\n",
    "\n",
    "    def infer_batch(inputs):\n",
    "        with ModelClient(url, model_name, inference_timeout_s=240) as client:\n",
    "            result_data = client.infer_batch(inputs)\n",
    "            return result_data[\"preds\"]\n",
    "        \n",
    "    return infer_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8514e-01de-481f-86aa-75afd99bcc7c",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5eae04bc-75ca-421a-87c8-ac507ce1f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b350bd8e-9b8f-4511-9ddf-76d917b21b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3e64fda-117b-4810-a9a2-dd498239496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress = predict_batch_udf(partial(triton_fn, url=url, model_name=model_name),\n",
    "                               input_tensor_shapes=[[8]],\n",
    "                               return_type=FloatType(),\n",
    "                               batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a24149a5-3adc-4089-8769-13cf1e44547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 ms, sys: 8.1 ms, total: 29.2 ms\n",
      "Wall time: 1.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions = df.withColumn(\"preds\", regress(struct(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df2ce39f-30af-491a-8472-800fb1ce8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.7 ms, sys: 4.15 ms, total: 27.8 ms\n",
      "Wall time: 973 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca6f3eaa-9569-45d0-88bf-9aa0757e1ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 ms, sys: 6.47 ms, total: 40.7 ms\n",
      "Wall time: 1.49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b79c62c8-e1e8-4467-8aef-8939c31833b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "|      MedInc|    HouseAge|   AveRooms|   AveBedrms| Population|    AveOccup|  Latitude|   Longitude|     preds|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "|  0.20909257|  -1.1632254| 0.38946992|  0.04609274| -0.9806099| -0.07099328|0.61245227|-0.020113053| 1.4441836|\n",
      "|-0.098627955|  0.34647804| 0.27216315|  -0.0129226| -0.6953838| -0.05380849| 1.0665938|  -1.2479742| 1.7245854|\n",
      "| -0.66006273|   1.0616008|-0.55292207| -0.48945764|-0.13641118| 0.028952759| 1.1040496|  -1.3827378| 1.3524104|\n",
      "|  0.08218294|   0.5848523|-0.13912922| -0.14707813|-0.19116047| -0.07136432|0.96827507|  -1.3028787|  2.300915|\n",
      "|   0.0784456|  -1.4810578| 0.57265776|  0.32067496|  1.0345173|-0.024157424| 1.4411427| -0.52423614| 1.2720771|\n",
      "| -0.82318723| -0.36864465| 0.07829511|  -0.1808107|-0.67242444|-0.061470542| 1.9374212|  -1.0083897| 0.6968852|\n",
      "|  0.59671736|   0.5848523| 0.19346413|  -0.1371872|-0.19645879| 0.009964322|0.96827507|  -1.2928978| 2.6057932|\n",
      "|  -0.9612035|  -1.5605159|-0.56329846| 0.027148023|-0.71127874| -0.08471591| 0.5328614| -0.13990337| 1.0139045|\n",
      "| -0.74344087|  -1.2426835| 0.27282518|   0.4037246| -0.9841421| -0.05610115| 1.2257773| -0.42940006|0.99310875|\n",
      "|   0.9784464|  -0.2891866| 0.24374022| -0.24670053| 0.28922042| -0.01102468| 1.1087307|  -1.2280084| 2.6450102|\n",
      "|  -0.5070446|  -1.0043093|-0.78254056|0.0122275995|  2.8465424|-0.060435444| 0.8980464|  -1.2080427| 2.1324868|\n",
      "| -0.18690155|   1.2205169|0.015323491|  0.12183313|-0.41015765|  0.04452552|  1.010412|  -1.3228445|  1.945999|\n",
      "|  -1.2551856|   1.6178073| -0.3341509|-0.060125165| -0.7554314| -0.08777025| 1.0291398|  -1.3477987|  1.255805|\n",
      "|   4.9607058|  -1.9578062|  1.4854684| -0.03948475|  2.1833694|0.0029250523|  1.024457|  -1.1581304| 5.8959594|\n",
      "|  0.73652315|  -1.6399739|  0.7913185| -0.05238397|    1.67738|  0.01944797| 1.0993668|  -1.1331724| 2.0677836|\n",
      "|   -0.505834|  0.18756187|-0.47093546| -0.24297306|-0.60619545| -0.10791535|  0.977639|  -1.2879055| 1.7652202|\n",
      "| -0.88477343|-0.050812364| -0.6318951| -0.15244243| -0.5258376| -0.15618815| 0.9823201|  -1.2879055| 1.5843443|\n",
      "| -0.42840376|   0.9821427| -0.2266495| -0.36083496| -0.6883194| -0.08552282| 0.5328614| -0.12493005|0.97526246|\n",
      "|   0.9369153|  -1.4810578|  0.6722208|-0.121177554|  0.3996021|  0.01291408| 1.1040496|  -1.1082181| 2.2553492|\n",
      "| -0.80702734| -0.92485124|-0.26602685|  -0.1560743|  1.4398388| -0.09314839|0.55627036| -0.09498342| 1.0328484|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec23b0-eaf2-4b6a-aa38-7a09873ed6eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stop Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8084bdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdownRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "shutdownRDD = use_stage_level_scheduling(spark, shutdownRDD)\n",
    "shutdownRDD.barrier().mapPartitions(lambda _: stop_triton(pids)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0138a029-87c5-497f-ac5c-3eed0e11b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not on_databricks: # on databricks, spark.stop() puts the cluster in a bad state\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24147e7-5695-44a0-9961-b94bfba1cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-dl-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
