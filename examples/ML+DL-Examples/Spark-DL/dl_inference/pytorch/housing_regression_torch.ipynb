{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792d95f9",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_efficientnet/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "# PySpark PyTorch Inference\n",
    "\n",
    "### Regression\n",
    "\n",
    "This notebook demonstrates distributed inference to perform regression on the California housing dataset.  \n",
    "\n",
    "Based on: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-neural-network-for-regression-with-pytorch.md  \n",
    "\n",
    "For the first MLP (array inputs) we'll also demonstrate accelerated inference on GPU with Torch-TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75930360-c5ce-49ef-a69a-da88fa69a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de685f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('models') if not os.path.exists('models') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5bc0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754b174",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Each label corresponds to the average house value in units of 100,000. We'll try to predict this value from the features:  \n",
    "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bee64cf-a44a-4aff-82db-c64ee3a8b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8644e508-5e4c-4cdd-9ed1-9235887d9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X.astype(np.float32))\n",
    "            self.y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6b55c3-dc7b-4831-9943-83efd48091bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HousingDataset(X, y)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d868f39d-4695-4110-91d2-6f7a09d73b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.6953,  1.0616,  0.1915, -0.4225, -0.6230, -0.0713,  1.0947, -0.8686],\n",
       "         [-0.1951,  0.1081,  0.1212, -0.0948,  0.5179, -0.0317, -1.3539,  1.2726],\n",
       "         [ 0.4848,  0.4259,  0.2762, -0.2329, -0.3448, -0.0226,  0.7576, -1.1731],\n",
       "         [-0.8216,  0.8232, -0.6655, -0.2007,  0.9807,  0.0812, -0.7312,  0.7635],\n",
       "         [ 1.7859, -1.3221,  0.6887, -0.1640,  3.8462, -0.0183,  1.0010, -1.2430],\n",
       "         [ 0.8630, -1.7194,  0.7781,  0.1056,  3.2466, -0.0062, -1.2322,  1.3924],\n",
       "         [-0.8578,  1.2205,  0.2019,  0.1206, -0.7281,  0.0223, -0.8015,  0.6637],\n",
       "         [-0.7268, -0.2097, -0.3582, -0.0703, -0.5753, -0.1680, -0.9466,  0.9232],\n",
       "         [ 5.8583,  1.8562,  1.5262, -0.0729, -0.7899,  0.0219, -0.7312,  0.6188],\n",
       "         [-0.1220,  1.0616, -0.5799, -0.2120, -0.1885, -0.0738,  0.7997, -1.1731]]),\n",
       " tensor([1.8370, 1.4470, 2.4320, 1.7310, 3.4160, 2.2100, 1.0640, 0.8110, 5.0000,\n",
       "         2.3920])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e817b9a",
   "metadata": {},
   "source": [
    "### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a441b60-dca4-44d2-bc1c-aa7336d704bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15cff2b4-9d23-4d2b-808a-a5edb8eda135",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "mlp = MLP().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2db3f9-5db8-4b42-89ad-e77f23c4c1fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   201: 0.709\n",
      "Loss after mini-batch   401: 0.454\n",
      "Loss after mini-batch   601: 0.328\n",
      "Loss after mini-batch   801: 0.281\n",
      "Loss after mini-batch  1001: 0.257\n",
      "Loss after mini-batch  1201: 0.241\n",
      "Loss after mini-batch  1401: 0.239\n",
      "Loss after mini-batch  1601: 0.219\n",
      "Loss after mini-batch  1801: 0.225\n",
      "Loss after mini-batch  2001: 0.212\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch   201: 0.212\n",
      "Loss after mini-batch   401: 0.200\n",
      "Loss after mini-batch   601: 0.202\n",
      "Loss after mini-batch   801: 0.214\n",
      "Loss after mini-batch  1001: 0.205\n",
      "Loss after mini-batch  1201: 0.208\n",
      "Loss after mini-batch  1401: 0.201\n",
      "Loss after mini-batch  1601: 0.197\n",
      "Loss after mini-batch  1801: 0.190\n",
      "Loss after mini-batch  2001: 0.193\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   201: 0.193\n",
      "Loss after mini-batch   401: 0.187\n",
      "Loss after mini-batch   601: 0.187\n",
      "Loss after mini-batch   801: 0.184\n",
      "Loss after mini-batch  1001: 0.188\n",
      "Loss after mini-batch  1201: 0.182\n",
      "Loss after mini-batch  1401: 0.191\n",
      "Loss after mini-batch  1601: 0.190\n",
      "Loss after mini-batch  1801: 0.186\n",
      "Loss after mini-batch  2001: 0.185\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   201: 0.189\n",
      "Loss after mini-batch   401: 0.178\n",
      "Loss after mini-batch   601: 0.180\n",
      "Loss after mini-batch   801: 0.184\n",
      "Loss after mini-batch  1001: 0.180\n",
      "Loss after mini-batch  1201: 0.180\n",
      "Loss after mini-batch  1401: 0.181\n",
      "Loss after mini-batch  1601: 0.178\n",
      "Loss after mini-batch  1801: 0.175\n",
      "Loss after mini-batch  2001: 0.182\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   201: 0.176\n",
      "Loss after mini-batch   401: 0.178\n",
      "Loss after mini-batch   601: 0.174\n",
      "Loss after mini-batch   801: 0.179\n",
      "Loss after mini-batch  1001: 0.177\n",
      "Loss after mini-batch  1201: 0.178\n",
      "Loss after mini-batch  1401: 0.184\n",
      "Loss after mini-batch  1601: 0.171\n",
      "Loss after mini-batch  1801: 0.175\n",
      "Loss after mini-batch  2001: 0.173\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 5):  # 5 epochs at maximum\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 200 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352539f5",
   "metadata": {},
   "source": [
    "### Save Model State Dict\n",
    "This saves the serialized object to disk using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b950a3ed-ffe1-477f-a84f-f71c85dbf9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to models/housing_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(mlp.state_dict(), \"models/housing_model.pt\")\n",
    "print(\"Saved PyTorch Model State to models/housing_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060fcca",
   "metadata": {},
   "source": [
    "### Save Model as TorchScript\n",
    "This saves an [intermediate representation of the compute graph](https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format), which does not require pickle (or even python). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fedb5d-c59e-4b0b-ba91-3dd15df1f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript Model to models/ts_housing_model.pt\n"
     ]
    }
   ],
   "source": [
    "scripted = torch.jit.script(mlp)\n",
    "scripted.save(\"models/ts_housing_model.pt\")\n",
    "print(\"Saved TorchScript Model to models/ts_housing_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101c0fe-65f1-411e-9192-e8a6b585ba0d",
   "metadata": {},
   "source": [
    "### Load and Test from Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7411b00f-88d2-40f5-b716-a26733c968ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_mlp = MLP().to(device)\n",
    "loaded_mlp.load_state_dict(torch.load(\"models/housing_model.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e226f449-2931-4492-9003-503cdc61f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d46af47e-db7e-42ee-9bd3-6e7d93850be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0813],\n",
       "        [2.8146],\n",
       "        [4.7754],\n",
       "        [1.2530],\n",
       "        [0.9696],\n",
       "        [1.3772],\n",
       "        [2.1451],\n",
       "        [2.0904],\n",
       "        [1.4945],\n",
       "        [1.4258]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_mlp(testX.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ae2c0f-1da5-45a4-bf32-ed8b562d7907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8530, 3.7340, 5.0000, 1.7920, 0.7080, 2.0430, 5.0000, 1.6540, 1.3750,\n",
       "        1.2830])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd329d",
   "metadata": {},
   "source": [
    "### Load and Test from TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "422e317f-c9bd-4f76-9463-7af2935d401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp = torch.jit.load(\"models/ts_housing_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cda8ec8-644e-4888-bfa0-b79425ece7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0813, 2.8146, 4.7754, 1.2530, 0.9696, 1.3772, 2.1451, 2.0904, 1.4945,\n",
       "        1.4258], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_mlp(testX.to(device)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b64e4",
   "metadata": {},
   "source": [
    "### Compile using the Torch JIT Compiler\n",
    "This leverages the [Torch-TensorRT inference compiler](https://pytorch.org/TensorRT/) for accelerated inference on GPUs using the `torch.compile` JIT interface under the hood. The compiler stack returns a [boxed-function](http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/) that triggers compilation on the first call.  \n",
    "\n",
    "Modules compiled in this fashion are [not serializable with pickle](https://github.com/pytorch/pytorch/issues/101107#issuecomment-1542688089), so we cannot send the compiled model directly to Spark.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613f24e",
   "metadata": {},
   "source": [
    "(You may see a warning about modelopt quantization. This is safe to ignore, as [implicit quantization](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#intro-quantization) is deprecated in the latest TensorRT. See [this link](https://pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/vgg16_fp8_ptq.html) for a guide to explicit quantization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ffb27fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n"
     ]
    }
   ],
   "source": [
    "import torch_tensorrt as trt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c10f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: set the filename for the TensorRT timing cache\n",
    "timestamp = time.time()\n",
    "timing_cache = f\"/tmp/timing_cache-{timestamp}.bin\"\n",
    "with open(timing_cache, \"wb\") as f:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4aa2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_bs1 = torch.randn((10, 8), dtype=torch.float).to(\"cuda\")\n",
    "# This indicates dimension 0 of inputs_bs1 is dynamic with a range of values [1, 50]. No recompilation will happen when the batch size changes.\n",
    "torch._dynamo.mark_dynamic(inputs_bs1, 0, min=1, max=50)\n",
    "trt_model = trt.compile(\n",
    "    loaded_mlp,\n",
    "    ir=\"torch_compile\",\n",
    "    inputs=inputs_bs1,\n",
    "    enabled_precisions={torch.float},\n",
    "    timing_cache_path=timing_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5da8cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)\n",
      "INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.\n",
      "INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.f32: 7>}, debug=False, workspace_size=0, min_block_size=5, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False, timing_cache_path='/tmp/timing_cache-1734546575.7906053.bin')\n",
      "\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Node _param_constant1 of op type get_attr does not have metadata. This could sometimes lead to undefined behavior.\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Some nodes do not have metadata (shape and dtype information). This could lead to problems sometimes if the graph has PyTorch and TensorRT segments.\n",
      "INFO:torch_tensorrt.dynamo._compiler:Partitioning the graph via the fast partitioner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 586, GPU 1533 (MiB)\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init builder kernel library: CPU +1634, GPU +288, now: CPU 2367, GPU 1821 (MiB)\n",
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/torch_tensorrt/dynamo/conversion/impl/activation/base.py:40: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  if input_val.dynamic_range is not None and dyn_range_fn is not None:\n",
      "\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.003660\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Detected 1 inputs and 1 output network tensors.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Host Persistent Memory: 22240\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Device Persistent Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Scratch Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Started assigning block shifts. This will take 10 steps to complete.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Algorithm ShiftNTopDown took 0.286488ms to assign 4 blocks to 10 nodes requiring 7168 bytes.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Activation Memory: 6656\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Weights Memory: 11648\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Engine generation completed in 1.41833 seconds.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 1 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 4115 MiB\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:01.421476\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 390420 bytes of Memory\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 26 bytes of code generator cache.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 52 timing cache entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0813],\n",
      "        [2.8146],\n",
      "        [4.7754],\n",
      "        [1.2530],\n",
      "        [0.9696],\n",
      "        [1.3772],\n",
      "        [2.1451],\n",
      "        [2.0904],\n",
      "        [1.4945],\n",
      "        [1.4258]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "stream = torch.cuda.Stream()\n",
    "with torch.no_grad(), torch.cuda.stream(stream):\n",
    "    testX = testX.to(device)\n",
    "    print(trt_model(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c55e07",
   "metadata": {},
   "source": [
    "### Compile using the Torch-TensorRT AOT Compiler\n",
    "Alternatively, use the Torch-TensorRT Dynamo backend for Ahead-of-Time (AOT) compilation to eagerly optimize the model in an explicit compilation phase. We first export the model to produce a traced graph representing the Tensor computation in an AOT fashion, which produces a `ExportedProgram` object which can be [serialized and reloaded](https://pytorch.org/TensorRT/user_guide/saving_models.html). We can then compile this IR using the Torch-TensorRT AOT compiler for inference.   \n",
    "\n",
    "[Read the docs](https://pytorch.org/TensorRT/user_guide/torch_tensorrt_explained.html) for more information on JIT vs AOT compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf36a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt.dynamo._compiler:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.f32: 7>}, debug=False, workspace_size=1073741824, min_block_size=5, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False, timing_cache_path='/tmp/timing_cache-1734546575.7906053.bin')\n",
      "\n",
      "INFO:torch_tensorrt.dynamo._compiler:Partitioning the graph via the fast partitioner\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 898, GPU 1535 (MiB)\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init builder kernel library: CPU +1632, GPU +286, now: CPU 2530, GPU 1821 (MiB)\n",
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/torch_tensorrt/dynamo/conversion/impl/activation/base.py:40: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  if input_val.dynamic_range is not None and dyn_range_fn is not None:\n",
      "\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.002958\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Detected 1 inputs and 1 output network tensors.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Host Persistent Memory: 18368\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Device Persistent Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Scratch Memory: 0\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Started assigning block shifts. This will take 6 steps to complete.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Algorithm ShiftNTopDown took 0.190575ms to assign 3 blocks to 6 nodes requiring 25088 bytes.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Activation Memory: 24576\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Weights Memory: 12292\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Engine generation completed in 1.28116 seconds.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 4140 MiB\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:01.283059\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 207892 bytes of Memory\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 26 bytes of code generator cache.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 101 timing cache entries\n"
     ]
    }
   ],
   "source": [
    "example_inputs = (torch.randn((10, 8), dtype=torch.float).to(\"cuda\"),)\n",
    "\n",
    "# Mark dim 1 (batch size) as dynamic\n",
    "batch = torch.export.Dim(\"batch\", min=1, max=64)\n",
    "# Produce traced graph in ExportedProgram format\n",
    "exp_program = torch.export.export(loaded_mlp, args=example_inputs, dynamic_shapes={\"x\": {0: batch}})\n",
    "# Compile the traced graph to produce an optimized module\n",
    "trt_gm = trt.dynamo.compile(exp_program,\n",
    "                            tuple(example_inputs),\n",
    "                            enabled_precisions={torch.float},\n",
    "                            timing_cache_path=timing_cache,\n",
    "                            workspace_size=1<<30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc4efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.export.exported_program.ExportedProgram'>\n",
      "<class 'torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl'>\n"
     ]
    }
   ],
   "source": [
    "print(type(exp_program))\n",
    "print(type(trt_gm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bcf9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0811],\n",
      "        [2.8141],\n",
      "        [4.7768],\n",
      "        [1.2529],\n",
      "        [0.9689],\n",
      "        [1.3768],\n",
      "        [2.1449],\n",
      "        [2.0906],\n",
      "        [1.4949],\n",
      "        [1.4257]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "stream = torch.cuda.Stream()\n",
    "with torch.no_grad(), torch.cuda.stream(stream):\n",
    "    testX = testX.to(device)\n",
    "    print(trt_gm(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb957a",
   "metadata": {},
   "source": [
    "We can run the optimized module with a few different batch sizes (without recompilation!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49f72c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "torch.Size([10, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = (torch.randn((10, 8), dtype=torch.float).cuda(),)\n",
    "inputs_bs1 = (torch.randn((1, 8), dtype=torch.float).cuda(),)\n",
    "inputs_bs50 = (torch.randn((50, 8), dtype=torch.float).cuda(),)\n",
    "\n",
    "stream = torch.cuda.Stream()\n",
    "with torch.no_grad(), torch.cuda.stream(stream):\n",
    "    print(\"Output shapes:\")\n",
    "    print(trt_gm(*inputs).shape)\n",
    "    print(trt_gm(*inputs_bs1).shape)\n",
    "    print(trt_gm(*inputs_bs50).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fef57d",
   "metadata": {},
   "source": [
    "We can serialize the ExportedProgram (a traced graph representing the model's forward function) using `torch.export.save` to be recompiled at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "876fea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.export.save(exp_program, \"models/trt_housing_model.ep\")\n",
    "print(\"Saved ExportedProgram to models/trt_model.ep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13631d1f-2c71-4bee-afcb-bd3b55ec87c5",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb71dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.\n",
      "  from typing.io import BinaryIO  # type: ignore[import]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, struct, pandas_udf, array\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769c060",
   "metadata": {},
   "source": [
    "Check the cluster environment to handle any platform-specific Spark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7727b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_databricks = os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", False)\n",
    "on_dataproc = os.environ.get(\"DATAPROC_VERSION\", False)\n",
    "on_standalone = not (on_databricks or on_dataproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e9dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 18:29:41 WARN Utils: Your hostname, cb4ae00-lcedt resolves to a loopback address: 127.0.1.1; using 10.110.47.100 instead (on interface eno1)\n",
      "24/12/18 18:29:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/18 18:29:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    if on_standalone:\n",
    "        import socket\n",
    "        conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "        hostname = socket.gethostname()\n",
    "        conf.setMaster(f\"spark://{hostname}:7077\")\n",
    "        conf.set(\"spark.pyspark.python\", f\"{conda_env}/bin/python\")\n",
    "        conf.set(\"spark.pyspark.driver.python\", f\"{conda_env}/bin/python\")\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_env}/lib:{conda_env}/lib/python3.11/site-packages/nvidia_pytriton.libs:$LD_LIBRARY_PATH\")\n",
    "    elif on_dataproc:\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conda_lib_path=\"/opt/conda/miniconda3/lib\"\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_lib_path}:$LD_LIBRARY_PATH\") \n",
    "\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.125\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "spark = SparkSession.builder.appName(\"spark-dl-examples\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9937e-2c70-4d67-b95f-4d9d5ab17c12",
   "metadata": {},
   "source": [
    "### Create Spark DataFrame from Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf35da14-61a3-4e7b-9d4f-086bf5e931b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95148019-ea95-40e5-a529-fcdb9a06f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(housing.data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f82d957c-6747-4408-aac8-45305afbfe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(X, columns=housing.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "881afee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/rishic/anaconda3/envs/spark-dl-torch/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:229: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(s.dtype):\n",
      "\n",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+\n",
      "|      MedInc|    HouseAge|   AveRooms|   AveBedrms| Population|    AveOccup|  Latitude|   Longitude|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+\n",
      "|  0.20909257|  -1.1632254| 0.38946992|  0.04609274| -0.9806099| -0.07099328|0.61245227|-0.020113053|\n",
      "|-0.098627955|  0.34647804| 0.27216315|  -0.0129226| -0.6953838| -0.05380849| 1.0665938|  -1.2479742|\n",
      "| -0.66006273|   1.0616008|-0.55292207| -0.48945764|-0.13641118| 0.028952759| 1.1040496|  -1.3827378|\n",
      "|  0.08218294|   0.5848523|-0.13912922| -0.14707813|-0.19116047| -0.07136432|0.96827507|  -1.3028787|\n",
      "|   0.0784456|  -1.4810578| 0.57265776|  0.32067496|  1.0345173|-0.024157424| 1.4411427| -0.52423614|\n",
      "| -0.82318723| -0.36864465| 0.07829511|  -0.1808107|-0.67242444|-0.061470542| 1.9374212|  -1.0083897|\n",
      "|  0.59671736|   0.5848523| 0.19346413|  -0.1371872|-0.19645879| 0.009964322|0.96827507|  -1.2928978|\n",
      "|  -0.9612035|  -1.5605159|-0.56329846| 0.027148023|-0.71127874| -0.08471591| 0.5328614| -0.13990337|\n",
      "| -0.74344087|  -1.2426835| 0.27282518|   0.4037246| -0.9841421| -0.05610115| 1.2257773| -0.42940006|\n",
      "|   0.9784464|  -0.2891866| 0.24374022| -0.24670053| 0.28922042| -0.01102468| 1.1087307|  -1.2280084|\n",
      "|  -0.5070446|  -1.0043093|-0.78254056|0.0122275995|  2.8465424|-0.060435444| 0.8980464|  -1.2080427|\n",
      "| -0.18690155|   1.2205169|0.015323491|  0.12183313|-0.41015765|  0.04452552|  1.010412|  -1.3228445|\n",
      "|  -1.2551856|   1.6178073| -0.3341509|-0.060125165| -0.7554314| -0.08777025| 1.0291398|  -1.3477987|\n",
      "|   4.9607058|  -1.9578062|  1.4854684| -0.03948475|  2.1833694|0.0029250523|  1.024457|  -1.1581304|\n",
      "|  0.73652315|  -1.6399739|  0.7913185| -0.05238397|    1.67738|  0.01944797| 1.0993668|  -1.1331724|\n",
      "|   -0.505834|  0.18756187|-0.47093546| -0.24297306|-0.60619545| -0.10791535|  0.977639|  -1.2879055|\n",
      "| -0.88477343|-0.050812364| -0.6318951| -0.15244243| -0.5258376| -0.15618815| 0.9823201|  -1.2879055|\n",
      "| -0.42840376|   0.9821427| -0.2266495| -0.36083496| -0.6883194| -0.08552282| 0.5328614| -0.12493005|\n",
      "|   0.9369153|  -1.4810578|  0.6722208|-0.121177554|  0.3996021|  0.01291408| 1.1040496|  -1.1082181|\n",
      "| -0.80702734| -0.92485124|-0.26602685|  -0.1560743|  1.4398388| -0.09314839|0.55627036| -0.09498342|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"MedInc\",FloatType(),True),\n",
    "    StructField(\"HouseAge\",FloatType(),True),\n",
    "    StructField(\"AveRooms\",FloatType(),True),\n",
    "    StructField(\"AveBedrms\",FloatType(),True),\n",
    "    StructField(\"Population\",FloatType(),True),\n",
    "    StructField(\"AveOccup\",FloatType(),True),\n",
    "    StructField(\"Latitude\",FloatType(),True),\n",
    "    StructField(\"Longitude\",FloatType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(pdf, schema=schema).repartition(8)\n",
    "df.show(truncate=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b33d367-fbf9-4918-b755-5447125547c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('MedInc', FloatType(), True), StructField('HouseAge', FloatType(), True), StructField('AveRooms', FloatType(), True), StructField('AveBedrms', FloatType(), True), StructField('Population', FloatType(), True), StructField('AveOccup', FloatType(), True), StructField('Latitude', FloatType(), True), StructField('Longitude', FloatType(), True)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "751bff7a-b687-4184-b3fa-b5f5b46ef5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"spark-dl-datasets/california_housing\"\n",
    "df.write.mode(\"overwrite\").parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3cd75",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API\n",
    "\n",
    "Distributed inference using the PySpark [predict_batch_udf](https://spark.apache.org/docs/3.4.0/api/python/reference/api/pyspark.ml.functions.predict_batch_udf.html#pyspark.ml.functions.predict_batch_udf):\n",
    "\n",
    "- predict_batch_fn uses PyTorch APIs to load the model and return a predict function which operates on numpy arrays \n",
    "- predict_batch_udf will convert the Spark DataFrame columns into numpy input batches for the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e40c266-24de-454d-a776-f3716ba50e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b144c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d608e2f-66a8-44b5-9cde-5f7837bf4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get absolute path to model\n",
    "model_path = \"{}/models/trt_housing_model.ep\".format(os.getcwd())\n",
    "\n",
    "if on_databricks:\n",
    "    import shutil\n",
    "    dbfs_model_path = \"/dbfs/FileStore/rishic/spark-dl-models/model.pt\"\n",
    "    shutil.copy(model_path, dbfs_model_path)\n",
    "    model_path = dbfs_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd143e7",
   "metadata": {},
   "source": [
    "For inference on Spark, we'll compile the model with the Torch-TensorRT AOT compiler and cache on the executor. We can specify dynamic batch sizes before compilation to [optimize across multiple input shapes](https://pytorch.org/TensorRT/user_guide/dynamic_shapes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc400771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2f45f5d-c941-4197-a274-1eec2af3fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import torch\n",
    "    import torch_tensorrt as trt\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        raise ValueError(\"This function uses the TensorRT model which requires a GPU device\")\n",
    "\n",
    "    example_inputs = (torch.randn((50, 8), dtype=torch.float).to(\"cuda\"),)\n",
    "    exp_program = torch.export.load(model_path)\n",
    "    trt_gm = trt.dynamo.compile(exp_program,\n",
    "                            tuple(example_inputs),\n",
    "                            enabled_precisions={torch.float},\n",
    "                            timing_cache_path=timing_cache,\n",
    "                            workspace_size=1<<30)\n",
    "\n",
    "    print(\"Model compiled.\")\n",
    "    \n",
    "    def predict(inputs):\n",
    "        stream = torch.cuda.Stream()\n",
    "        with torch.no_grad(), torch.cuda.stream(stream), trt.logging.errors():\n",
    "            print(f\"Predict {inputs.shape}\")\n",
    "            torch_inputs = torch.from_numpy(inputs).to(device)\n",
    "            outputs = trt_gm(torch_inputs) # .flatten()\n",
    "            return outputs.detach().cpu().numpy()\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "220a00a4-e842-4f5d-a4b3-7693d09e2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=FloatType(),\n",
    "                             input_tensor_shapes=[[8]],\n",
    "                             batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f3bf287-8ffc-4456-8772-e97c418d6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 ms, sys: 25.3 ms, total: 171 ms\n",
      "Wall time: 8.34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", regress(struct(*columns)))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cd23b71-296d-4ce7-b56c-567cc2eec79c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 ms, sys: 10.7 ms, total: 36.3 ms\n",
      "Wall time: 253 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75d16bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 ms, sys: 4.55 ms, total: 32 ms\n",
      "Wall time: 270 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "764a40d8-25f7-425c-ba03-fe8c45f4b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "|      MedInc|    HouseAge|   AveRooms|   AveBedrms| Population|    AveOccup|  Latitude|   Longitude|     preds|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "|  0.20909257|  -1.1632254| 0.38946992|  0.04609274| -0.9806099| -0.07099328|0.61245227|-0.020113053|  1.411905|\n",
      "|-0.098627955|  0.34647804| 0.27216315|  -0.0129226| -0.6953838| -0.05380849| 1.0665938|  -1.2479742| 1.7515419|\n",
      "| -0.66006273|   1.0616008|-0.55292207| -0.48945764|-0.13641118| 0.028952759| 1.1040496|  -1.3827378| 1.3486583|\n",
      "|  0.08218294|   0.5848523|-0.13912922| -0.14707813|-0.19116047| -0.07136432|0.96827507|  -1.3028787|  2.441918|\n",
      "|   0.0784456|  -1.4810578| 0.57265776|  0.32067496|  1.0345173|-0.024157424| 1.4411427| -0.52423614| 1.2384591|\n",
      "| -0.82318723| -0.36864465| 0.07829511|  -0.1808107|-0.67242444|-0.061470542| 1.9374212|  -1.0083897| 0.6311492|\n",
      "|  0.59671736|   0.5848523| 0.19346413|  -0.1371872|-0.19645879| 0.009964322|0.96827507|  -1.2928978| 2.6748414|\n",
      "|  -0.9612035|  -1.5605159|-0.56329846| 0.027148023|-0.71127874| -0.08471591| 0.5328614| -0.13990337| 1.0372864|\n",
      "| -0.74344087|  -1.2426835| 0.27282518|   0.4037246| -0.9841421| -0.05610115| 1.2257773| -0.42940006|0.92786956|\n",
      "|   0.9784464|  -0.2891866| 0.24374022| -0.24670053| 0.28922042| -0.01102468| 1.1087307|  -1.2280084| 2.6567924|\n",
      "|  -0.5070446|  -1.0043093|-0.78254056|0.0122275995|  2.8465424|-0.060435444| 0.8980464|  -1.2080427| 1.9954385|\n",
      "| -0.18690155|   1.2205169|0.015323491|  0.12183313|-0.41015765|  0.04452552|  1.010412|  -1.3228445| 1.9118243|\n",
      "|  -1.2551856|   1.6178073| -0.3341509|-0.060125165| -0.7554314| -0.08777025| 1.0291398|  -1.3477987| 1.1730174|\n",
      "|   4.9607058|  -1.9578062|  1.4854684| -0.03948475|  2.1833694|0.0029250523|  1.024457|  -1.1581304| 5.9049444|\n",
      "|  0.73652315|  -1.6399739|  0.7913185| -0.05238397|    1.67738|  0.01944797| 1.0993668|  -1.1331724| 2.0103073|\n",
      "|   -0.505834|  0.18756187|-0.47093546| -0.24297306|-0.60619545| -0.10791535|  0.977639|  -1.2879055| 1.7815019|\n",
      "| -0.88477343|-0.050812364| -0.6318951| -0.15244243| -0.5258376| -0.15618815| 0.9823201|  -1.2879055| 1.6335179|\n",
      "| -0.42840376|   0.9821427| -0.2266495| -0.36083496| -0.6883194| -0.08552282| 0.5328614| -0.12493005| 1.0147359|\n",
      "|   0.9369153|  -1.4810578|  0.6722208|-0.121177554|  0.3996021|  0.01291408| 1.1040496|  -1.1082181| 2.1365688|\n",
      "| -0.80702734| -0.92485124|-0.26602685|  -0.1560743|  1.4398388| -0.09314839|0.55627036| -0.09498342|0.92216444|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0aa85f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will clear the engine cache (containing previously compiled TensorRT engines) and resets the CUDA Context.\n",
    "torch._dynamo.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53536808",
   "metadata": {},
   "source": [
    "## Using Triton Inference Server\n",
    "In this section, we demonstrate integration with the [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server), an open-source, GPU-accelerated serving solution for DL.  \n",
    "We use [PyTriton](https://github.com/triton-inference-server/pytriton), a Flask-like framework that handles client/server communication with the Triton server.  \n",
    "\n",
    "The process looks like this:\n",
    "- Distribute a PyTriton task across the Spark cluster, instructing each node to launch a Triton server process.\n",
    "- Define a Triton inference function, which contains a client that binds to the local server on a given node and sends inference requests.\n",
    "- Wrap the Triton inference function in a predict_batch_udf to launch parallel inference requests using Spark.\n",
    "- Finally, distribute a shutdown signal to terminate the Triton server processes on each node.\n",
    "\n",
    "<img src=\"../images/spark-pytriton.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9ab4cdf-8103-447e-9ac8-944e2e527239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6632636e-67a3-406c-832c-758aac4245fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_server(model_path):\n",
    "    import signal\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    import torch_tensorrt as trt\n",
    "    from pytriton.decorators import batch\n",
    "    from pytriton.model_config import DynamicBatcher, ModelConfig, Tensor\n",
    "    from pytriton.triton import Triton\n",
    "    from pyspark import TaskContext\n",
    "\n",
    "    print(f\"SERVER: Initializing model on worker {TaskContext.get().partitionId()}.\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    exp_program = torch.export.load(model_path)\n",
    "    example_inputs = (torch.randn((50, 8), dtype=torch.float).to(\"cuda\"),)\n",
    "    trt_gm = trt.dynamo.compile(exp_program,\n",
    "                            tuple(example_inputs),\n",
    "                            enabled_precisions={torch.float},\n",
    "                            workspace_size=1<<30)\n",
    "\n",
    "    print(\"SERVER: Compiled model.\")\n",
    "\n",
    "    @batch\n",
    "    def _infer_fn(**inputs):\n",
    "        features = inputs[\"features\"]\n",
    "        if len(inputs[\"features\"]) != 1:\n",
    "            features = np.squeeze(features)\n",
    "        stream = torch.cuda.Stream()\n",
    "        with torch.no_grad(), torch.cuda.stream(stream):\n",
    "            torch_inputs = torch.from_numpy(features).to(device)\n",
    "            outputs = trt_gm(torch_inputs)\n",
    "            return {\n",
    "                \"preds\": outputs.cpu().numpy(),\n",
    "            }\n",
    "\n",
    "    with Triton() as triton:\n",
    "        triton.bind(\n",
    "            model_name=\"HousingModel\",\n",
    "            infer_func=_infer_fn,\n",
    "            inputs=[\n",
    "                Tensor(name=\"features\", dtype=np.float32, shape=(-1,)),\n",
    "            ],\n",
    "            outputs=[\n",
    "                Tensor(name=\"preds\", dtype=np.float32, shape=(-1,)),\n",
    "            ],\n",
    "            config=ModelConfig(\n",
    "                max_batch_size=50,\n",
    "                batcher=DynamicBatcher(max_queue_delay_microseconds=5000),  # 5ms\n",
    "            ),\n",
    "            strict=True,\n",
    "        )\n",
    "\n",
    "        def stop_triton(signum, frame):\n",
    "            print(\"SERVER: Received SIGTERM. Stopping Triton server.\")\n",
    "            triton.stop()\n",
    "\n",
    "        signal.signal(signal.SIGTERM, stop_triton)\n",
    "\n",
    "        print(\"SERVER: Serving inference\")\n",
    "        triton.serve()\n",
    "\n",
    "def start_triton(url, model_name, model_path):\n",
    "    import socket\n",
    "    import psutil\n",
    "    import multiprocessing as mp\n",
    "    from multiprocessing import Process\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    for conn in psutil.net_connections(kind=\"inet\"):\n",
    "        if conn.laddr.port == 8001:\n",
    "            print(f\"Process {conn.pid} is already running on port 8001. Please stop it before starting a new one.\")\n",
    "            return []\n",
    "\n",
    "    hostname = socket.gethostname()\n",
    "    process = Process(target=triton_server, args=(model_path,))\n",
    "    process.start()\n",
    "\n",
    "    client = ModelClient(url, model_name)\n",
    "    ready = False\n",
    "    while not ready:\n",
    "        try:\n",
    "            client.wait_for_server(5)\n",
    "            ready = True\n",
    "        except Exception as e:\n",
    "            print(f\"Waiting for server to be ready: {e}\")\n",
    "    \n",
    "    return [(hostname, process.pid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74121cd7",
   "metadata": {},
   "source": [
    "#### Start Triton servers\n",
    "\n",
    "To ensure that only one Triton inference server is started per node, we use stage-level scheduling to delegate each task to a separate GPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38eff4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _use_stage_level_scheduling(spark, rdd):\n",
    "\n",
    "    if spark.version < \"3.4.0\":\n",
    "        raise Exception(\"Stage-level scheduling is not supported in Spark < 3.4.0\")\n",
    "\n",
    "    executor_cores = spark.conf.get(\"spark.executor.cores\")\n",
    "    assert executor_cores is not None, \"spark.executor.cores is not set\"\n",
    "    executor_gpus = spark.conf.get(\"spark.executor.resource.gpu.amount\")\n",
    "    assert executor_gpus is not None and int(executor_gpus) <= 1, \"spark.executor.resource.gpu.amount must be set and <= 1\"\n",
    "\n",
    "    from pyspark.resource.profile import ResourceProfileBuilder\n",
    "    from pyspark.resource.requests import TaskResourceRequests\n",
    "\n",
    "    spark_plugins = spark.conf.get(\"spark.plugins\", \" \")\n",
    "    assert spark_plugins is not None\n",
    "    spark_rapids_sql_enabled = spark.conf.get(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    assert spark_rapids_sql_enabled is not None\n",
    "\n",
    "    task_cores = (\n",
    "        int(executor_cores)\n",
    "        if \"com.nvidia.spark.SQLPlugin\" in spark_plugins\n",
    "        and \"true\" == spark_rapids_sql_enabled.lower()\n",
    "        else (int(executor_cores) // 2) + 1\n",
    "    )\n",
    "\n",
    "    task_gpus = 1.0\n",
    "    treqs = TaskResourceRequests().cpus(task_cores).resource(\"gpu\", task_gpus)\n",
    "    rp = ResourceProfileBuilder().require(treqs).build\n",
    "    print(f\"Reqesting stage-level resources: (cores={task_cores}, gpu={task_gpus})\")\n",
    "\n",
    "    return rdd.withResources(rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcca988",
   "metadata": {},
   "source": [
    "**Specify the number of nodes in the cluster.**  \n",
    "Following the README, the example standalone cluster uses 1 node. The example Databricks/Dataproc cluster scripts use 2 nodes by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "160a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 1  # Change based on cluster setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bca2f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    }
   ],
   "source": [
    "url = \"localhost\"\n",
    "model_name = \"HousingModel\"\n",
    "\n",
    "sc = spark.sparkContext\n",
    "nodeRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "nodeRDD = _use_stage_level_scheduling(spark, nodeRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5358d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton Server PIDs:\n",
      " {\n",
      "    \"cb4ae00-lcedt\": 161803\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pids = nodeRDD.barrier().mapPartitions(lambda _: start_triton(url, model_name, model_path)).collectAsMap()\n",
    "print(\"Triton Server PIDs:\\n\", json.dumps(pids, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ac038",
   "metadata": {},
   "source": [
    "#### Define client function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ae91c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fn(url, model_name, init_timeout_s):\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    print(f\"Connecting to Triton model {model_name} at {url}.\")\n",
    "\n",
    "    def infer_batch(inputs):\n",
    "        with ModelClient(url, model_name, init_timeout_s=init_timeout_s) as client:\n",
    "            result_data = client.infer_batch(inputs)\n",
    "            return result_data[\"preds\"]\n",
    "        \n",
    "    return infer_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8514e-01de-481f-86aa-75afd99bcc7c",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5eae04bc-75ca-421a-87c8-ac507ce1f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b350bd8e-9b8f-4511-9ddf-76d917b21b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3e64fda-117b-4810-a9a2-dd498239496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress = predict_batch_udf(partial(triton_fn, url=\"localhost\", model_name=\"HousingModel\", init_timeout_s=500),\n",
    "                               input_tensor_shapes=[[8]],\n",
    "                               return_type=FloatType(),\n",
    "                               batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a24149a5-3adc-4089-8769-13cf1e44547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 2.78 ms, total: 146 ms\n",
      "Wall time: 3.16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions = df.withColumn(\"preds\", regress(struct(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df2ce39f-30af-491a-8472-800fb1ce8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 ms, sys: 3.5 ms, total: 30 ms\n",
      "Wall time: 893 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca6f3eaa-9569-45d0-88bf-9aa0757e1ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 ms, sys: 3.34 ms, total: 29 ms\n",
      "Wall time: 852 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", regress(array(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b79c62c8-e1e8-4467-8aef-8939c31833b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "|      MedInc|    HouseAge|   AveRooms|   AveBedrms| Population|    AveOccup|  Latitude|   Longitude|     preds|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "|  0.20909257|  -1.1632254| 0.38946992|  0.04609274| -0.9806099| -0.07099328|0.61245227|-0.020113053|  1.411905|\n",
      "|-0.098627955|  0.34647804| 0.27216315|  -0.0129226| -0.6953838| -0.05380849| 1.0665938|  -1.2479742| 1.7515419|\n",
      "| -0.66006273|   1.0616008|-0.55292207| -0.48945764|-0.13641118| 0.028952759| 1.1040496|  -1.3827378| 1.3486583|\n",
      "|  0.08218294|   0.5848523|-0.13912922| -0.14707813|-0.19116047| -0.07136432|0.96827507|  -1.3028787|  2.441918|\n",
      "|   0.0784456|  -1.4810578| 0.57265776|  0.32067496|  1.0345173|-0.024157424| 1.4411427| -0.52423614| 1.2384591|\n",
      "| -0.82318723| -0.36864465| 0.07829511|  -0.1808107|-0.67242444|-0.061470542| 1.9374212|  -1.0083897| 0.6311492|\n",
      "|  0.59671736|   0.5848523| 0.19346413|  -0.1371872|-0.19645879| 0.009964322|0.96827507|  -1.2928978| 2.6748414|\n",
      "|  -0.9612035|  -1.5605159|-0.56329846| 0.027148023|-0.71127874| -0.08471591| 0.5328614| -0.13990337| 1.0372864|\n",
      "| -0.74344087|  -1.2426835| 0.27282518|   0.4037246| -0.9841421| -0.05610115| 1.2257773| -0.42940006|0.92786956|\n",
      "|   0.9784464|  -0.2891866| 0.24374022| -0.24670053| 0.28922042| -0.01102468| 1.1087307|  -1.2280084| 2.6567924|\n",
      "|  -0.5070446|  -1.0043093|-0.78254056|0.0122275995|  2.8465424|-0.060435444| 0.8980464|  -1.2080427| 1.9954385|\n",
      "| -0.18690155|   1.2205169|0.015323491|  0.12183313|-0.41015765|  0.04452552|  1.010412|  -1.3228445| 1.9118243|\n",
      "|  -1.2551856|   1.6178073| -0.3341509|-0.060125165| -0.7554314| -0.08777025| 1.0291398|  -1.3477987| 1.1730174|\n",
      "|   4.9607058|  -1.9578062|  1.4854684| -0.03948475|  2.1833694|0.0029250523|  1.024457|  -1.1581304| 5.9049444|\n",
      "|  0.73652315|  -1.6399739|  0.7913185| -0.05238397|    1.67738|  0.01944797| 1.0993668|  -1.1331724| 2.0103073|\n",
      "|   -0.505834|  0.18756187|-0.47093546| -0.24297306|-0.60619545| -0.10791535|  0.977639|  -1.2879055| 1.7815019|\n",
      "| -0.88477343|-0.050812364| -0.6318951| -0.15244243| -0.5258376| -0.15618815| 0.9823201|  -1.2879055| 1.6335179|\n",
      "| -0.42840376|   0.9821427| -0.2266495| -0.36083496| -0.6883194| -0.08552282| 0.5328614| -0.12493005| 1.0147359|\n",
      "|   0.9369153|  -1.4810578|  0.6722208|-0.121177554|  0.3996021|  0.01291408| 1.1040496|  -1.1082181| 2.1365688|\n",
      "| -0.80702734| -0.92485124|-0.26602685|  -0.1560743|  1.4398388| -0.09314839|0.55627036| -0.09498342|0.92216444|\n",
      "+------------+------------+-----------+------------+-----------+------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec23b0-eaf2-4b6a-aa38-7a09873ed6eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stop Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8084bdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(pids):\n",
    "    import os\n",
    "    import socket\n",
    "    import signal\n",
    "    import time \n",
    "    \n",
    "    hostname = socket.gethostname()\n",
    "    pid = pids.get(hostname, None)\n",
    "    assert pid is not None, f\"Could not find pid for {hostname}\"\n",
    "    os.kill(pid, signal.SIGTERM)\n",
    "    time.sleep(7)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            os.kill(pid, 0)\n",
    "        except OSError:\n",
    "            return [True]\n",
    "        time.sleep(5)\n",
    "\n",
    "    return [False]\n",
    "\n",
    "shutdownRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "shutdownRDD = _use_stage_level_scheduling(spark, shutdownRDD)\n",
    "shutdownRDD.barrier().mapPartitions(lambda _: stop_triton(pids)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0138a029-87c5-497f-ac5c-3eed0e11b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24147e7-5695-44a0-9961-b94bfba1cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-dl-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
