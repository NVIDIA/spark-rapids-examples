{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6810cc-5982-4293-bfbd-c91ef0aca204",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_efficientnet/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "# PySpark Tensorflow Inference\n",
    "\n",
    "### Flower Recognition with Keras Resnet50\n",
    "\n",
    "In this notebook, we demonstrate distribute inference with Resnet50 on the Databricks flower photos dataset.  \n",
    "From: https://docs.databricks.com/_static/notebooks/deep-learning/keras-metadata.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e3a8d",
   "metadata": {},
   "source": [
    "Note that cuFFT/cuDNN/cuBLAS registration errors are expected (as of `tf=2.17.0`) and will not affect behavior, as noted in [this issue.](https://github.com/tensorflow/tensorflow/issues/62075)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf329ac8-0763-44bc-b0f6-b634b7dc480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 21:47:32.701834: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-06 21:47:32.709303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-06 21:47:32.717762: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-06 21:47:32.720294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-06 21:47:32.726934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-06 21:47:33.116070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import uuid\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532d562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('models') if not os.path.exists('models') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75175140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe61b8",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b474339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct, pandas_udf, PandasUDFType\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from typing import Iterator, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182cacb",
   "metadata": {},
   "source": [
    "Check the cluster environment to handle any platform-specific Spark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564b1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_databricks = os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", False)\n",
    "on_dataproc = os.environ.get(\"DATAPROC_VERSION\", False)\n",
    "on_standalone = not (on_databricks or on_dataproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cdd0b",
   "metadata": {},
   "source": [
    "#### Create Spark Session\n",
    "\n",
    "For local standalone clusters, we'll connect to the cluster and create the Spark Session.  \n",
    "For CSP environments, Spark will either be preconfigured (Databricks) or we'll need to create the Spark Session (Dataproc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d72768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/06 21:47:34 WARN Utils: Your hostname, cb4ae00-lcedt resolves to a loopback address: 127.0.1.1; using 10.110.47.100 instead (on interface eno1)\n",
      "25/01/06 21:47:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/06 21:47:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    if on_standalone:\n",
    "        import socket\n",
    "        \n",
    "        conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "        hostname = socket.gethostname()\n",
    "        conf.setMaster(f\"spark://{hostname}:7077\")\n",
    "        conf.set(\"spark.pyspark.python\", f\"{conda_env}/bin/python\")\n",
    "        conf.set(\"spark.pyspark.driver.python\", f\"{conda_env}/bin/python\")\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_env}/lib:{conda_env}/lib/python3.11/site-packages/nvidia_pytriton.libs:$LD_LIBRARY_PATH\")\n",
    "        source = \"/usr/lib/x86_64-linux-gnu/libstdc++.so.6\"\n",
    "        target = f\"{conda_env}/lib/libstdc++.so.6\"\n",
    "        try:\n",
    "            if os.path.islink(target) or os.path.exists(target):\n",
    "                os.remove(target)\n",
    "            os.symlink(source, target)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating symlink: {e}\")\n",
    "    elif on_dataproc:\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conda_lib_path=\"/opt/conda/miniconda3/lib\"\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_lib_path}:$LD_LIBRARY_PATH\") \n",
    "\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.125\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "    conf.set(\"spark.driver.memory\", \"8g\")\n",
    "    conf.set(\"spark.executor.memory\", \"8g\")\n",
    "\n",
    "conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"512\")\n",
    "conf.set(\"spark.sql.parquet.columnarReaderBatchSize\", \"1024\")\n",
    "spark = SparkSession.builder.appName(\"spark-dl-examples\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c406fa",
   "metadata": {},
   "source": [
    "Define the input and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c566dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdirs(\"spark-dl-datasets\") if not os.path.exists(\"spark-dl-datasets\") else None\n",
    "data_path = \"spark-dl-datasets/flowers_{uuid}.parquet\".format(uuid=str(uuid.uuid1()))\n",
    "local_file_path = f\"{os.getcwd()}/{data_path}\"\n",
    "output_file_path = \"predictions/predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d08a7-66b9-444f-b362-d8df692aef1c",
   "metadata": {},
   "source": [
    "### Prepare trained model and data for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da083168-137f-492c-8769-d8f1e2111756",
   "metadata": {},
   "source": [
    "Load the ResNet-50 Model and broadcast the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddc715a-cdbc-4c49-93e9-58c9d88511da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 21:47:35.333033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 45735 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50()\n",
    "bc_model_weights = sc.broadcast(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dddfa3-e8df-4e8e-8251-64457f1ebf80",
   "metadata": {},
   "source": [
    "Load the data and save the datasets to one Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0738bec-97d4-4946-8c49-5e6d07ff1afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 3670\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(f\"Image count: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54f470a-d308-4426-8ed0-33f95155bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(data_dir) for f in filenames if os.path.splitext(f)[1] == '.jpg']\n",
    "files = files[:2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f94ee0-f1ea-47f6-a77e-be8da5d1b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = []\n",
    "for file in files:\n",
    "    img = Image.open(file)\n",
    "    img = img.resize([224, 224])\n",
    "    data = np.asarray(img, dtype=\"float32\").reshape([224*224*3])\n",
    "\n",
    "    image_data.append({\"data\": data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(image_data, columns=['data']).to_parquet(data_path)\n",
    "\n",
    "if on_databricks:\n",
    "    dbutils.fs.mkdirs(\"/FileStore/spark-dl-datasets\")\n",
    "    shutil.copy(local_file_path, \"/dbfs/FileStore/{}\".format(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2414b0f-58f2-4e4a-9d09-8ea95b38d413",
   "metadata": {},
   "source": [
    "### Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670328e3-7274-4d78-b315-487750166a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/resnet50_model.keras'\n",
    "model.save(model_path)\n",
    "\n",
    "# For cloud environments, copy the model to the distributed file system.\n",
    "if on_databricks:\n",
    "    import shutil\n",
    "    dbutils.fs.mkdirs(\"/FileStore/spark-dl-models\")\n",
    "    dbfs_model_path = \"/dbfs/FileStore/spark-dl-models/resnet50_model.keras\"\n",
    "    shutil.copy(model_path, dbfs_model_path)\n",
    "    model_path = dbfs_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827ad56-1af0-41b7-be68-94bd203a2a70",
   "metadata": {},
   "source": [
    "### Load the data into Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ddc22d0-b88a-4906-bd47-bf247e34feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"dbfs:/FileStore/{}\".format(data_path))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865929b0-b016-4de4-996d-7f16176cf49c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model inference via Pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5a747",
   "metadata": {},
   "source": [
    "Define the function to parse the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67b3128-13c1-44f1-a0c0-7cf7a836fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(image_data):\n",
    "    image = tf.image.convert_image_dtype(\n",
    "        image_data, dtype=tf.float32) * (2. / 255) - 1\n",
    "    image = tf.reshape(image, [224, 224, 3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e4ba2",
   "metadata": {},
   "source": [
    "Define the function for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b33185f-6d1e-4ca9-9757-fdc3d736496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def pandas_predict_udf(iter: Iterator[Tuple[pd.Series]]) -> Iterator[pd.Series]:\n",
    "\n",
    "    # Enable GPU memory growth to avoid CUDA OOM\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    batch_size = 64\n",
    "    model = ResNet50(weights=None)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    for image_batch in iter:\n",
    "        images = np.vstack(image_batch)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "        dataset = dataset.map(parse_image, num_parallel_calls=8).prefetch(\n",
    "            5000).batch(batch_size)\n",
    "        preds = model.predict(dataset)\n",
    "        yield pd.Series(list(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08190547",
   "metadata": {},
   "source": [
    "Run model inference and save the results to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8c05da-db38-45ef-81d0-1f862f575ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 ms, sys: 21 ms, total: 63.9 ms\n",
      "Wall time: 14.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions_1 = df.select(pandas_predict_udf(col(\"data\")).alias(\"prediction\"))\n",
    "results = predictions_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08cb2a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                          prediction|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[1.2938889E-4, 2.4666305E-4, 6.765791E-5, 1.2263245E-4, 5.7486624E-5, 3.9616702E-4, 7.0566134E-6,...|\n",
      "|[4.4501914E-5, 3.5403698E-4, 4.6702033E-5, 8.102543E-5, 3.1704556E-5, 1.9194305E-4, 7.905952E-6, ...|\n",
      "|[1.05672516E-4, 2.2686279E-4, 3.0055395E-5, 6.523785E-5, 2.352077E-5, 3.7122983E-4, 3.3315896E-6,...|\n",
      "|[2.0331638E-5, 2.2746396E-4, 7.828012E-5, 6.986782E-5, 4.705316E-5, 9.80732E-5, 5.561918E-6, 2.35...|\n",
      "|[1.130241E-4, 2.3187004E-4, 5.296914E-5, 1.0871329E-4, 4.027478E-5, 3.7183522E-4, 5.5931855E-6, 3...|\n",
      "|[9.094467E-5, 2.06384E-4, 4.514821E-5, 7.665891E-5, 3.2262324E-5, 3.3875552E-4, 3.831814E-6, 4.18...|\n",
      "|[1.07847634E-4, 3.7848807E-4, 7.660533E-5, 1.2446754E-4, 4.7595917E-5, 3.333814E-4, 1.0669675E-5,...|\n",
      "|[2.2261223E-5, 2.734666E-4, 3.8122747E-5, 6.2266954E-5, 1.7935155E-5, 1.7268128E-4, 6.034271E-6, ...|\n",
      "|[1.1065645E-4, 2.900581E-4, 4.2585547E-5, 1.074203E-4, 3.052314E-5, 4.794604E-4, 6.4872897E-6, 3....|\n",
      "|[9.673917E-5, 2.058331E-4, 7.4652424E-5, 1.1323769E-4, 4.6106186E-5, 2.8604185E-4, 5.62365E-6, 5....|\n",
      "|[7.411196E-5, 3.291524E-4, 1.3454164E-4, 1.7738447E-4, 8.467504E-5, 2.2466244E-4, 1.3621126E-5, 1...|\n",
      "|[8.721524E-5, 2.7338538E-4, 3.5964815E-5, 7.792533E-5, 2.3559302E-5, 3.6789547E-4, 3.5665628E-6, ...|\n",
      "|[9.723709E-5, 2.7619812E-4, 5.7464153E-5, 1.10104906E-4, 3.8317143E-5, 3.490506E-4, 6.1553183E-6,...|\n",
      "|[6.940235E-5, 2.5377885E-4, 5.057188E-5, 1.1485363E-4, 3.0059196E-5, 2.7862669E-4, 5.024019E-6, 5...|\n",
      "|[4.2095784E-5, 2.4891715E-4, 1.236292E-4, 1.4306813E-4, 7.3354306E-5, 1.6047148E-4, 7.958807E-6, ...|\n",
      "|[2.7327887E-5, 3.8553146E-4, 1.2939748E-4, 1.5762268E-4, 7.307493E-5, 8.5530424E-5, 1.2648808E-5,...|\n",
      "|[3.036101E-5, 3.5572305E-4, 1.600718E-4, 2.1437313E-4, 8.063033E-5, 1.02061334E-4, 1.3876456E-5, ...|\n",
      "|[3.3109587E-5, 2.8182982E-4, 1.7998899E-4, 2.0246049E-4, 1.3720036E-4, 1.01000114E-4, 3.427488E-5...|\n",
      "|[4.549448E-5, 2.8782588E-4, 2.3703449E-4, 2.448979E-4, 1.20997625E-4, 1.3744453E-4, 1.62803E-5, 2...|\n",
      "|[1.2242574E-4, 2.8095162E-4, 6.332559E-5, 1.0209269E-4, 4.335324E-5, 3.906304E-4, 8.205706E-6, 6....|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_1.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40799f8e-443e-40ca-919b-391f901cb3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_1.write.mode(\"overwrite\").parquet(output_file_path + \"_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a69aa9",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API\n",
    "\n",
    "Distributed inference using the PySpark [predict_batch_udf](https://spark.apache.org/docs/3.4.0/api/python/reference/api/pyspark.ml.functions.predict_batch_udf.html#pyspark.ml.functions.predict_batch_udf):\n",
    "\n",
    "- predict_batch_fn uses Tensorflow APIs to load the model and return a predict function which operates on numpy arrays \n",
    "- predict_batch_udf will convert the Spark DataFrame columns into numpy input batches for the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dda88b46-6300-4bf7-bc10-7403f4fbbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "    # Enable GPU memory growth\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    model = ResNet50()\n",
    "    def predict(inputs):\n",
    "        inputs = inputs * (2. / 255) - 1\n",
    "        return model.predict(inputs)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cff0e851-563d-40b6-9d05-509c22b3b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(predict_batch_fn,\n",
    "                             input_tensor_shapes=[[224, 224, 3]],\n",
    "                             return_type=ArrayType(FloatType()),\n",
    "                             batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa7c156f-e2b3-4837-9427-ccf3a5720412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/FileStore/{}\".format(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80bc50ad-eaf5-4fce-a354-5e17d65e2da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.4 ms, sys: 36.5 ms, total: 94.8 ms\n",
      "Wall time: 18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions_2 = df.select(classify(struct(\"data\")).alias(\"prediction\"))\n",
    "results = predictions_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41cace80-7a4b-4929-8e63-9c83f9745e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.9 ms, sys: 18.2 ms, total: 86 ms\n",
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions_2 = df.select(classify(\"data\").alias(\"prediction\"))\n",
    "results = predictions_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56a2ec8a-de09-4d7c-9666-1b3c76f10657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.2 ms, sys: 20.1 ms, total: 66.2 ms\n",
      "Wall time: 15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions_2 = df.select(classify(col(\"data\")).alias(\"prediction\"))\n",
    "results = predictions_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dcf3791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                          prediction|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[1.2938889E-4, 2.4666305E-4, 6.765791E-5, 1.2263245E-4, 5.7486624E-5, 3.9616702E-4, 7.0566134E-6,...|\n",
      "|[4.4501914E-5, 3.5403698E-4, 4.6702033E-5, 8.102543E-5, 3.1704556E-5, 1.9194305E-4, 7.905952E-6, ...|\n",
      "|[1.05672516E-4, 2.2686279E-4, 3.0055395E-5, 6.523785E-5, 2.352077E-5, 3.7122983E-4, 3.3315896E-6,...|\n",
      "|[2.0331638E-5, 2.2746396E-4, 7.828012E-5, 6.986782E-5, 4.705316E-5, 9.80732E-5, 5.561918E-6, 2.35...|\n",
      "|[1.130241E-4, 2.3187004E-4, 5.296914E-5, 1.0871329E-4, 4.027478E-5, 3.7183522E-4, 5.5931855E-6, 3...|\n",
      "|[9.094467E-5, 2.06384E-4, 4.514821E-5, 7.665891E-5, 3.2262324E-5, 3.3875552E-4, 3.831814E-6, 4.18...|\n",
      "|[1.07847634E-4, 3.7848807E-4, 7.660533E-5, 1.2446754E-4, 4.7595917E-5, 3.333814E-4, 1.0669675E-5,...|\n",
      "|[2.2261223E-5, 2.734666E-4, 3.8122747E-5, 6.2266954E-5, 1.7935155E-5, 1.7268128E-4, 6.034271E-6, ...|\n",
      "|[1.1065645E-4, 2.900581E-4, 4.2585547E-5, 1.074203E-4, 3.052314E-5, 4.794604E-4, 6.4872897E-6, 3....|\n",
      "|[9.673917E-5, 2.058331E-4, 7.4652424E-5, 1.1323769E-4, 4.6106186E-5, 2.8604185E-4, 5.62365E-6, 5....|\n",
      "|[7.411196E-5, 3.291524E-4, 1.3454164E-4, 1.7738447E-4, 8.467504E-5, 2.2466244E-4, 1.3621126E-5, 1...|\n",
      "|[8.721524E-5, 2.7338538E-4, 3.5964815E-5, 7.792533E-5, 2.3559302E-5, 3.6789547E-4, 3.5665628E-6, ...|\n",
      "|[9.723709E-5, 2.7619812E-4, 5.7464153E-5, 1.10104906E-4, 3.8317143E-5, 3.490506E-4, 6.1553183E-6,...|\n",
      "|[6.940235E-5, 2.5377885E-4, 5.057188E-5, 1.1485363E-4, 3.0059196E-5, 2.7862669E-4, 5.024019E-6, 5...|\n",
      "|[4.2095784E-5, 2.4891715E-4, 1.236292E-4, 1.4306813E-4, 7.3354306E-5, 1.6047148E-4, 7.958807E-6, ...|\n",
      "|[2.7327887E-5, 3.8553146E-4, 1.2939748E-4, 1.5762268E-4, 7.307493E-5, 8.5530424E-5, 1.2648808E-5,...|\n",
      "|[3.036101E-5, 3.5572305E-4, 1.600718E-4, 2.1437313E-4, 8.063033E-5, 1.02061334E-4, 1.3876456E-5, ...|\n",
      "|[3.3109587E-5, 2.8182982E-4, 1.7998899E-4, 2.0246049E-4, 1.3720036E-4, 1.01000114E-4, 3.427488E-5...|\n",
      "|[4.549448E-5, 2.8782588E-4, 2.3703449E-4, 2.448979E-4, 1.20997625E-4, 1.3744453E-4, 1.62803E-5, 2...|\n",
      "|[1.2242574E-4, 2.8095162E-4, 6.332559E-5, 1.0209269E-4, 4.335324E-5, 3.906304E-4, 8.205706E-6, 6....|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_2.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc511eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_2.write.mode(\"overwrite\").parquet(output_file_path + \"_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ca7fb",
   "metadata": {},
   "source": [
    "## Using Triton Inference Server\n",
    "In this section, we demonstrate integration with the [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server), an open-source, GPU-accelerated serving solution for DL.  \n",
    "We use [PyTriton](https://github.com/triton-inference-server/pytriton), a Flask-like framework that handles client/server communication with the Triton server.  \n",
    "\n",
    "The process looks like this:\n",
    "- Distribute a PyTriton task across the Spark cluster, instructing each node to launch a Triton server process.\n",
    "- Define a Triton inference function, which contains a client that binds to the local server on a given node and sends inference requests.\n",
    "- Wrap the Triton inference function in a predict_batch_udf to launch parallel inference requests using Spark.\n",
    "- Finally, distribute a shutdown signal to terminate the Triton server processes on each node.\n",
    "\n",
    "<img src=\"../images/spark-pytriton.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2605d134-ef75-4d94-9b16-2c6d85f29bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c8c0744-0558-4dac-bbfe-8bdde4b2af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_server(ports):\n",
    "    import time\n",
    "    import signal\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.applications import ResNet50\n",
    "    from pytriton.decorators import batch\n",
    "    from pytriton.model_config import DynamicBatcher, ModelConfig, Tensor\n",
    "    from pytriton.triton import Triton, TritonConfig\n",
    "    from pyspark import TaskContext\n",
    "\n",
    "    print(f\"SERVER: Initializing ResNet on worker {TaskContext.get().partitionId()}.\")\n",
    "\n",
    "    # Enable GPU memory growth\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    model = ResNet50()\n",
    "    normalization_layer = tf.keras.layers.Rescaling(scale=2./255, offset=-1)\n",
    "\n",
    "    @batch\n",
    "    def _infer_fn(**inputs):\n",
    "        images = inputs[\"images\"]\n",
    "        normalized_images = normalization_layer(images)\n",
    "        return {\n",
    "            \"preds\": model.predict(normalized_images),\n",
    "        }\n",
    "\n",
    "    workspace_path = f\"triton_workspace_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "    triton_conf = TritonConfig(http_port=ports[0], grpc_port=ports[1], metrics_port=ports[2])\n",
    "    with Triton(config=triton_conf, workspace=workspace_path) as triton:\n",
    "        triton.bind(\n",
    "            model_name=\"ResNet50\",\n",
    "            infer_func=_infer_fn,\n",
    "            inputs=[\n",
    "                Tensor(name=\"images\", dtype=np.float32, shape=(224, 224, 3)),\n",
    "            ],\n",
    "            outputs=[\n",
    "                Tensor(name=\"preds\", dtype=np.float32, shape=(-1,)),\n",
    "            ],\n",
    "            config=ModelConfig(\n",
    "                max_batch_size=100,\n",
    "                batcher=DynamicBatcher(max_queue_delay_microseconds=5000),  # 5ms\n",
    "            ),\n",
    "            strict=True,\n",
    "        )\n",
    "\n",
    "        def stop_triton(signum, frame):\n",
    "            print(\"SERVER: Received SIGTERM. Stopping Triton server.\")\n",
    "            triton.stop()\n",
    "\n",
    "        signal.signal(signal.SIGTERM, stop_triton)\n",
    "\n",
    "        print(\"SERVER: Serving inference\")\n",
    "        triton.serve()\n",
    "\n",
    "def start_triton(ports, model_name):\n",
    "    import socket\n",
    "    from pathlib import Path\n",
    "    from multiprocessing import Process\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    os.chdir(Path.home())\n",
    "\n",
    "    hostname = socket.gethostname()\n",
    "    process = Process(target=triton_server, args=(ports,))\n",
    "    process.start()\n",
    "\n",
    "    client = ModelClient(f\"http://localhost:{ports[0]}\", model_name)\n",
    "    patience = 8\n",
    "    while patience > 0:\n",
    "        try:\n",
    "            client.wait_for_server(5)\n",
    "            return [(hostname, process.pid)]\n",
    "        except Exception:\n",
    "            print(\"Waiting for server to be ready...\")\n",
    "            patience -= 1\n",
    "\n",
    "    emsg = \"Failure: client waited too long for server startup.\"\n",
    "    print(emsg)\n",
    "    return [(hostname, emsg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f7037",
   "metadata": {},
   "source": [
    "#### Start Triton servers\n",
    "\n",
    "To ensure that only one Triton inference server is started per node, we use stage-level scheduling to delegate each task to a separate GPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13196ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _use_stage_level_scheduling(spark, rdd):\n",
    "\n",
    "    if spark.version < \"3.4.0\":\n",
    "        raise Exception(\"Stage-level scheduling is not supported in Spark < 3.4.0\")\n",
    "\n",
    "    executor_cores = spark.conf.get(\"spark.executor.cores\")\n",
    "    assert executor_cores is not None, \"spark.executor.cores is not set\"\n",
    "    executor_gpus = spark.conf.get(\"spark.executor.resource.gpu.amount\")\n",
    "    assert executor_gpus is not None and int(executor_gpus) <= 1, \"spark.executor.resource.gpu.amount must be set and <= 1\"\n",
    "\n",
    "    from pyspark.resource.profile import ResourceProfileBuilder\n",
    "    from pyspark.resource.requests import TaskResourceRequests\n",
    "\n",
    "    spark_plugins = spark.conf.get(\"spark.plugins\", \" \")\n",
    "    assert spark_plugins is not None\n",
    "    spark_rapids_sql_enabled = spark.conf.get(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    assert spark_rapids_sql_enabled is not None\n",
    "\n",
    "    task_cores = (\n",
    "        int(executor_cores)\n",
    "        if \"com.nvidia.spark.SQLPlugin\" in spark_plugins\n",
    "        and \"true\" == spark_rapids_sql_enabled.lower()\n",
    "        else (int(executor_cores) // 2) + 1\n",
    "    )\n",
    "\n",
    "    task_gpus = 1.0\n",
    "    treqs = TaskResourceRequests().cpus(task_cores).resource(\"gpu\", task_gpus)\n",
    "    rp = ResourceProfileBuilder().require(treqs).build\n",
    "    print(f\"Reqesting stage-level resources: (cores={task_cores}, gpu={task_gpus})\")\n",
    "\n",
    "    return rdd.withResources(rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a387dc",
   "metadata": {},
   "source": [
    "**Specify the number of nodes in the cluster.**  \n",
    "Following the README, the example standalone cluster uses 1 node. The example Databricks/Dataproc cluster scripts use 4 nodes by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "132fbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change based on cluster setup\n",
    "num_nodes = 1 if on_standalone else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2309a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "nodeRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "nodeRDD = _use_stage_level_scheduling(spark, nodeRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e2a89",
   "metadata": {},
   "source": [
    "Triton occupies ports for HTTP requests, GRPC requests, and the metrics service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "825370dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ports():\n",
    "    import psutil\n",
    "    \n",
    "    ports = []\n",
    "    conns = [conn.laddr.port for conn in psutil.net_connections(kind=\"inet\")]\n",
    "    i = 7000\n",
    "    while len(ports) < 3:\n",
    "        if i not in conns:\n",
    "            ports.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    return ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfc8834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ports [7000, 7001, 7002]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ResNet50\"\n",
    "ports = find_ports()\n",
    "assert len(ports) == 3\n",
    "print(f\"Using ports {ports}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad24bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton Server PIDs:\n",
      " {\n",
      "    \"cb4ae00-lcedt\": 2869535\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pids = nodeRDD.barrier().mapPartitions(lambda _: start_triton(ports, model_name)).collectAsMap()\n",
    "print(\"Triton Server PIDs:\\n\", json.dumps(pids, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ebdbe",
   "metadata": {},
   "source": [
    "#### Define client function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa34bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"http://localhost:{ports[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5ab49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fn(url, model_name, init_timeout_s):\n",
    "    import numpy as np\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    print(f\"CLIENT: Connecting to {model_name} at {url}\")\n",
    "\n",
    "    def infer_batch(inputs):\n",
    "        with ModelClient(url, model_name, init_timeout_s=init_timeout_s) as client:\n",
    "            result_data = client.infer_batch(inputs)\n",
    "            return result_data[\"preds\"]\n",
    "            \n",
    "    return infer_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2328e",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbfc9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/FileStore/{}\".format(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07365c-0a14-49b3-9bd8-cfb35f48b089",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fabcaeb-5a44-42bb-8097-5dbc2d0cee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "classify = predict_batch_udf(partial(triton_fn, url=url, model_name=model_name, init_timeout_s=500),\n",
    "                             input_tensor_shapes=[[224, 224, 3]],\n",
    "                             return_type=ArrayType(FloatType()),\n",
    "                             batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e595473d-1a5d-46a6-a6ba-89d2ea903de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.6 ms, sys: 22.3 ms, total: 70.9 ms\n",
      "Wall time: 17.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions_3 = df.select(classify(struct(\"data\")).alias(\"prediction\"))\n",
    "results = predictions_3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f66d468-e0b1-4589-8606-b3848063a823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.4 ms, sys: 23.5 ms, total: 76.9 ms\n",
      "Wall time: 12.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions_3 = df.select(classify(\"data\").alias(\"prediction\"))\n",
    "results = predictions_3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "632c4c3a-fa52-4c3d-b71e-7526286e353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.9 ms, sys: 20 ms, total: 73.9 ms\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions_3 = df.select(classify(col(\"data\")).alias(\"prediction\"))\n",
    "results = predictions_3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49870e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                          prediction|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[1.2964063E-4, 2.4653607E-4, 6.7508765E-5, 1.2236452E-4, 5.7346635E-5, 3.9642912E-4, 7.033199E-6,...|\n",
      "|[4.4486973E-5, 3.5260408E-4, 4.684452E-5, 8.12069E-5, 3.179397E-5, 1.9187202E-4, 7.887208E-6, 1.3...|\n",
      "|[1.059436E-4, 2.2737762E-4, 3.0225037E-5, 6.550149E-5, 2.3658315E-5, 3.7172026E-4, 3.353684E-6, 2...|\n",
      "|[2.0393689E-5, 2.2818097E-4, 7.841931E-5, 6.991323E-5, 4.704759E-5, 9.822018E-5, 5.5858673E-6, 2....|\n",
      "|[1.13108545E-4, 2.3128217E-4, 5.283139E-5, 1.0866656E-4, 4.0229144E-5, 3.7223354E-4, 5.5677583E-6...|\n",
      "|[9.1271184E-5, 2.0681013E-4, 4.5193243E-5, 7.6812066E-5, 3.2361808E-5, 3.399333E-4, 3.8415465E-6,...|\n",
      "|[1.0792112E-4, 3.7743401E-4, 7.618583E-5, 1.24259E-4, 4.7426664E-5, 3.3307416E-4, 1.0592865E-5, 9...|\n",
      "|[2.2220212E-5, 2.7357432E-4, 3.8200575E-5, 6.235621E-5, 1.7954999E-5, 1.7249273E-4, 6.021971E-6, ...|\n",
      "|[1.1044029E-4, 2.8961376E-4, 4.2384647E-5, 1.0728626E-4, 3.0468744E-5, 4.796082E-4, 6.4537376E-6,...|\n",
      "|[9.68494E-5, 2.0567125E-4, 7.450887E-5, 1.13256065E-4, 4.609738E-5, 2.8675792E-4, 5.603957E-6, 5....|\n",
      "|[7.420906E-5, 3.2883475E-4, 1.3444667E-4, 1.7758778E-4, 8.4717096E-5, 2.2534849E-4, 1.3623082E-5,...|\n",
      "|[8.755989E-5, 2.7312606E-4, 3.59614E-5, 7.7967066E-5, 2.3571063E-5, 3.6875304E-4, 3.5629025E-6, 3...|\n",
      "|[9.7425895E-5, 2.7611412E-4, 5.74094E-5, 1.1035101E-4, 3.8303257E-5, 3.4981826E-4, 6.167147E-6, 4...|\n",
      "|[6.92996E-5, 2.5326438E-4, 5.063317E-5, 1.1494952E-4, 3.0212495E-5, 2.7857954E-4, 5.0324948E-6, 5...|\n",
      "|[4.2184765E-5, 2.4904116E-4, 1.237565E-4, 1.4271903E-4, 7.3208634E-5, 1.6054673E-4, 7.938735E-6, ...|\n",
      "|[2.719573E-5, 3.8372327E-4, 1.291892E-4, 1.5711001E-4, 7.3108524E-5, 8.553368E-5, 1.2617156E-5, 1...|\n",
      "|[3.0565643E-5, 3.55542E-4, 1.5949155E-4, 2.1368133E-4, 8.043127E-5, 1.02662845E-4, 1.3859853E-5, ...|\n",
      "|[3.311506E-5, 2.8069926E-4, 1.7956384E-4, 2.0205336E-4, 1.3665091E-4, 1.0115404E-4, 3.409792E-5, ...|\n",
      "|[4.573667E-5, 2.888326E-4, 2.3792271E-4, 2.460216E-4, 1.2164583E-4, 1.3814335E-4, 1.6352218E-5, 2...|\n",
      "|[1.2279079E-4, 2.8073761E-4, 6.365874E-5, 1.0251792E-4, 4.3527238E-5, 3.914249E-4, 8.236801E-6, 6...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_3.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86cd59f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_3.write.mode(\"overwrite\").parquet(output_file_path + \"_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc06b7e-f750-40b5-9208-a035db11d937",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stop Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbfcaa51-3b9f-43ff-a4a8-4b46766115b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(pids):\n",
    "    import os\n",
    "    import socket\n",
    "    import signal\n",
    "    import time \n",
    "    \n",
    "    hostname = socket.gethostname()\n",
    "    pid = pids.get(hostname, None)\n",
    "    assert pid is not None, f\"Could not find pid for {hostname}\"\n",
    "    \n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            os.kill(pid, signal.SIGTERM)\n",
    "        except OSError:\n",
    "            return [True]\n",
    "        time.sleep(5)\n",
    "\n",
    "    return [False]\n",
    "\n",
    "shutdownRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "shutdownRDD = _use_stage_level_scheduling(spark, shutdownRDD)\n",
    "shutdownRDD.barrier().mapPartitions(lambda _: stop_triton(pids)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d88639b-d934-4eb4-ae2f-cc13b9b10456",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cc28a-34d7-479c-be7e-9a380d39e25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-dl-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
