{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f7ac5d-4a95-4170-a0ac-a7faac9d9ef4",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_efficientnet/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "# PySpark Huggingface Inferencing\n",
    "### Sentiment Analysis using Pipelines with PyTorch\n",
    "\n",
    "In this notebook, we demonstrate distributed inference with Huggingface Pipelines to perform sentiment analysis.  \n",
    "From: https://huggingface.co/docs/transformers/quicktour#pipeline-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd0f77b-ee1b-4477-a038-d25a4f1da0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f756c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "553b28d2-a5d1-4d07-8a49-8f82b808e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b91fe91-b725-4564-ae93-56e3fb51e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier((\"We are very happy to show you the ðŸ¤— Transformers library.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be39eb3-462c-42ff-b8f4-09f4e4fe3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752f929",
   "metadata": {},
   "source": [
    "Let's try a different model and tokenizer in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9861865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506e7834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65e3279959b4f87b6a4f710d7227dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312017fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272652983665466}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92b15e-0da0-46c3-81a3-fabaedbfc42c",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69dd6a1a-f450-47f0-9dbf-ad250585a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct, pandas_udf\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c19ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "datasets.disable_progress_bars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a0210",
   "metadata": {},
   "source": [
    "Check the cluster environment to handle any platform-specific Spark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79aaf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_databricks = os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", False)\n",
    "on_dataproc = os.environ.get(\"DATAPROC_VERSION\", False)\n",
    "on_standalone = not (on_databricks or on_dataproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f9c38",
   "metadata": {},
   "source": [
    "#### Create Spark Session\n",
    "\n",
    "For local standalone clusters, we'll connect to the cluster and create the Spark Session.  \n",
    "For CSP environments, Spark will either be preconfigured (Databricks) or we'll need to create the Spark Session (Dataproc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e0e0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/06 18:29:40 WARN Utils: Your hostname, cb4ae00-lcedt resolves to a loopback address: 127.0.1.1; using 10.110.47.100 instead (on interface eno1)\n",
      "25/01/06 18:29:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/06 18:29:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    if on_standalone:\n",
    "        import socket\n",
    "        conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "        hostname = socket.gethostname()\n",
    "        conf.setMaster(f\"spark://{hostname}:7077\")\n",
    "        conf.set(\"spark.pyspark.python\", f\"{conda_env}/bin/python\")\n",
    "        conf.set(\"spark.pyspark.driver.python\", f\"{conda_env}/bin/python\")\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_env}/lib:{conda_env}/lib/python3.11/site-packages/nvidia_pytriton.libs:$LD_LIBRARY_PATH\")\n",
    "    elif on_dataproc:\n",
    "        # Point PyTriton to correct libpython3.11.so:\n",
    "        conda_lib_path=\"/opt/conda/miniconda3/lib\"\n",
    "        conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{conda_lib_path}:$LD_LIBRARY_PATH\") \n",
    "\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"0.125\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "\n",
    "conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\")\n",
    "spark = SparkSession.builder.appName(\"spark-dl-examples\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d70208",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "dataset = dataset.to_pandas().drop(columns=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f421d",
   "metadata": {},
   "source": [
    "#### Create PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac24f3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('text', StringType(), True)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(dataset).repartition(8)\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d1876b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06ec6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/06 18:29:47 WARN TaskSetManager: Stage 6 contains a task of very large size (4021 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(text=\"Anyone remember the first CKY, CKY2K etc..? Back when it was about making crazy cool stuff, rather than watching Bam Margera act like a douchebag, spoiled 5 year old, super/rock-star wannabe.<br /><br />The show used to be awesome, however, Bam's fame and wealth has led him to believe, that we now enjoy him acting childish and idiotic, more than actual cool stuff, that used to be in ex. CKY2K.<br /><br />The acts are so repetitive, there's like nothing new, except annoying stupidity and rehearsed comments... The only things we see is Bam Margera, so busy showing us how much he doesn't care, how much money he got or whatsoever.<br /><br />I really got nothing much left to say except, give us back CKY2K, cause Bam suck..<br /><br />I enjoy watching Steve-o, Knoxville etc. a thousand times more.\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeadf4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/06 18:29:47 WARN TaskSetManager: Stage 9 contains a task of very large size (4021 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"spark-dl-datasets/imdb_test\"\n",
    "if on_databricks:\n",
    "    dbutils.fs.mkdirs(\"/FileStore/spark-dl-datasets\")\n",
    "    data_path = \"dbfs:/FileStore/\" + data_path\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cddc95",
   "metadata": {},
   "source": [
    "#### Load and preprocess DataFrame\n",
    "\n",
    "Define our preprocess function. We'll take the first sentence from each sample as our input for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9665b7b6-d7e9-4bd4-b29d-7a449ac5b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def preprocess(text: pd.Series) -> pd.Series:\n",
    "    return pd.Series([s.split(\".\")[0] for s in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74cfa3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                               input|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|Doesn't anyone bother to check where this kind of sludge comes from before blathering on about it...|\n",
      "|                          There were two things I hated about WASTED : The directing and the script |\n",
      "|                                I'm rather surprised that anybody found this film touching or moving|\n",
      "|Cultural Vandalism Is the new Hallmark production of Gulliver's Travels an act of cultural vandal...|\n",
      "|I was at Wrestlemania VI in Toronto as a 10 year old, and the event I saw then was pretty differe...|\n",
      "|                                                                     This movie has been done before|\n",
      "|[ as a new resolution for this year 2005, i decide to write a comment for each movie I saw in the...|\n",
      "|This movie is over hyped!! I am sad to say that I manage to watch the first 15 minutes of this mo...|\n",
      "|This show had a promising start as sort of the opposite of 'Oceans 11' but has developed into a s...|\n",
      "|MINOR PLOT SPOILERS AHEAD!!!<br /><br />How did such talented actors get involved in such mindles...|\n",
      "|                              There is not one character on this sitcom with any redeeming qualities|\n",
      "|                                  Tommy Lee Jones was the best Woodroe and no one can play Woodroe F|\n",
      "|                                 My wife rented this movie and then conveniently never got to see it|\n",
      "|This is one of those star-filled over-the-top comedies that could a) be hysterical, or b) wish th...|\n",
      "|This excruciatingly boring and unfunny movie made me think that Chaplin was the real Hitler, as o...|\n",
      "|                           you will likely be sorely disappointed by this sequel that's not a sequel|\n",
      "|                          If I was British, I would be embarrassed by this portrayal of incompetence|\n",
      "|One of those movies in which there are no big twists whatsoever and you can predict pretty much w...|\n",
      "|                        This show is like watching someone who is in training to someday host a show|\n",
      "|                                                                                                Sigh|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(data_path).limit(256).repartition(8)\n",
    "df = df.select(preprocess(col(\"text\")).alias(\"input\")).cache()\n",
    "df.show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad92750",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API\n",
    "\n",
    "Distributed inference using the PySpark [predict_batch_udf](https://spark.apache.org/docs/3.4.0/api/python/reference/api/pyspark.ml.functions.predict_batch_udf.html#pyspark.ml.functions.predict_batch_udf):\n",
    "\n",
    "- predict_batch_fn uses PyTorch APIs to load the model and return a predict function which operates on numpy arrays \n",
    "- predict_batch_udf will convert the Spark DataFrame columns into numpy input batches for the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0da9d25c-5ebe-4503-bb19-154fcc047cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import torch\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pipe = pipeline(\"sentiment-analysis\", device=device)\n",
    "    def predict(inputs):\n",
    "        return pipe(inputs.tolist())\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78afef29-ee30-4267-9fb6-be2dcb86cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=StructType([\n",
    "                                 StructField(\"label\", StringType(), True),\n",
    "                                 StructField(\"score\", FloatType(), True)\n",
    "                             ]),\n",
    "                             batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5bc327e-89cf-4731-82e6-e66cb93deef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 4.23 ms, total: 19 ms\n",
      "Wall time: 3.15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(struct(\"input\"))).select(\"input\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac642895-cfd6-47ee-9b21-02e7835424e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 ms, sys: 2.33 ms, total: 4.91 ms\n",
      "Wall time: 393 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", classify(\"input\")).select(\"input\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76a44d80-d5db-405f-989c-7246379cfb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 ms, sys: 2.41 ms, total: 5.06 ms\n",
      "Wall time: 398 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", classify(col(\"input\"))).select(\"input\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c01761b3-c766-46b0-ae0b-fcf968ffb3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "|                                                                           input|   label|     score|\n",
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "|Doesn't anyone bother to check where this kind of sludge comes from before bl...|NEGATIVE| 0.9984042|\n",
      "|      There were two things I hated about WASTED : The directing and the script |NEGATIVE| 0.9979019|\n",
      "|            I'm rather surprised that anybody found this film touching or moving|POSITIVE| 0.8392794|\n",
      "|Cultural Vandalism Is the new Hallmark production of Gulliver's Travels an ac...|NEGATIVE|0.99726933|\n",
      "|I was at Wrestlemania VI in Toronto as a 10 year old, and the event I saw the...|POSITIVE|0.98212516|\n",
      "|                                                 This movie has been done before|NEGATIVE|0.94194806|\n",
      "|[ as a new resolution for this year 2005, i decide to write a comment for eac...|NEGATIVE|0.99678314|\n",
      "|This movie is over hyped!! I am sad to say that I manage to watch the first 1...|NEGATIVE| 0.9985846|\n",
      "|This show had a promising start as sort of the opposite of 'Oceans 11' but ha...|NEGATIVE|0.99926823|\n",
      "|MINOR PLOT SPOILERS AHEAD!!!<br /><br />How did such talented actors get invo...|NEGATIVE| 0.9995671|\n",
      "|          There is not one character on this sitcom with any redeeming qualities|NEGATIVE|0.99856514|\n",
      "|              Tommy Lee Jones was the best Woodroe and no one can play Woodroe F|POSITIVE| 0.9945687|\n",
      "|             My wife rented this movie and then conveniently never got to see it|NEGATIVE| 0.9984137|\n",
      "|This is one of those star-filled over-the-top comedies that could a) be hyste...|NEGATIVE| 0.9953224|\n",
      "|This excruciatingly boring and unfunny movie made me think that Chaplin was t...|NEGATIVE| 0.9997607|\n",
      "|       you will likely be sorely disappointed by this sequel that's not a sequel|NEGATIVE|0.99971956|\n",
      "|      If I was British, I would be embarrassed by this portrayal of incompetence|NEGATIVE|0.99651587|\n",
      "|One of those movies in which there are no big twists whatsoever and you can p...|NEGATIVE|0.99860746|\n",
      "|    This show is like watching someone who is in training to someday host a show|NEGATIVE|  0.970153|\n",
      "|                                                                            Sigh|NEGATIVE|0.99231356|\n",
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1a6ce",
   "metadata": {},
   "source": [
    "## Using Triton Inference Server\n",
    "In this section, we demonstrate integration with the [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server), an open-source, GPU-accelerated serving solution for DL.  \n",
    "We use [PyTriton](https://github.com/triton-inference-server/pytriton), a Flask-like framework that handles client/server communication with the Triton server.  \n",
    "\n",
    "The process looks like this:\n",
    "- Distribute a PyTriton task across the Spark cluster, instructing each node to launch a Triton server process.\n",
    "- Define a Triton inference function, which contains a client that binds to the local server on a given node and sends inference requests.\n",
    "- Wrap the Triton inference function in a predict_batch_udf to launch parallel inference requests using Spark.\n",
    "- Finally, distribute a shutdown signal to terminate the Triton server processes on each node.\n",
    "\n",
    "<img src=\"../images/spark-pytriton.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d4be844-4b8c-47df-bd09-0c280c7ff16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e53df9f-43cb-4c38-b8ac-dc2cbad99815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_server(ports):\n",
    "    import time\n",
    "    import signal\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from transformers import pipeline\n",
    "    from pytriton.decorators import batch\n",
    "    from pytriton.model_config import DynamicBatcher, ModelConfig, Tensor\n",
    "    from pytriton.triton import Triton, TritonConfig\n",
    "    from pyspark import TaskContext\n",
    "\n",
    "    print(f\"SERVER: Initializing pipeline on worker {TaskContext.get().partitionId()}.\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pipe = pipeline(\"sentiment-analysis\", device=device)\n",
    "    print(f\"SERVER: Using {device} device.\")\n",
    "\n",
    "    @batch\n",
    "    def _infer_fn(**inputs):\n",
    "        sentences = np.squeeze(inputs[\"text\"]).tolist()\n",
    "        print(f\"SERVER: Received batch of size {len(sentences)}\")\n",
    "        decoded_sentences = [s.decode(\"utf-8\") for s in sentences]\n",
    "        return {\n",
    "            \"outputs\": np.array([[json.dumps(o)] for o in pipe(decoded_sentences)])\n",
    "        }\n",
    "\n",
    "    workspace_path = f\"triton_workspace_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "    triton_conf = TritonConfig(http_port=ports[0], grpc_port=ports[1], metrics_port=ports[2])\n",
    "    with Triton(config=triton_conf, workspace=workspace_path) as triton:\n",
    "        triton.bind(\n",
    "            model_name=\"SentimentAnalysis\",\n",
    "            infer_func=_infer_fn,\n",
    "            inputs=[\n",
    "                Tensor(name=\"text\", dtype=object, shape=(-1,)),\n",
    "            ],\n",
    "            outputs=[\n",
    "                Tensor(name=\"outputs\", dtype=object, shape=(-1,)),\n",
    "            ],\n",
    "            config=ModelConfig(\n",
    "                max_batch_size=64,\n",
    "                batcher=DynamicBatcher(max_queue_delay_microseconds=5000),  # 5ms\n",
    "            ),\n",
    "            strict=True,\n",
    "        )\n",
    "\n",
    "        def stop_triton(signum, frame):\n",
    "            print(\"SERVER: Received SIGTERM. Stopping Triton server.\")\n",
    "            triton.stop()\n",
    "\n",
    "        signal.signal(signal.SIGTERM, stop_triton)\n",
    "\n",
    "        print(\"SERVER: Serving inference\")\n",
    "        triton.serve()\n",
    "\n",
    "def start_triton(ports, model_name):\n",
    "    import socket\n",
    "    from pathlib import Path\n",
    "    from multiprocessing import Process\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    os.chdir(Path.home())\n",
    "\n",
    "    hostname = socket.gethostname()\n",
    "    process = Process(target=triton_server, args=(ports,))\n",
    "    process.start()\n",
    "\n",
    "    client = ModelClient(f\"http://localhost:{ports[0]}\", model_name)\n",
    "    patience = 8\n",
    "    while patience > 0:\n",
    "        try:\n",
    "            client.wait_for_server(5)\n",
    "            return [(hostname, process.pid)]\n",
    "        except Exception:\n",
    "            print(\"Waiting for server to be ready...\")\n",
    "            patience -= 1\n",
    "\n",
    "    emsg = \"Failure: client waited too long for server startup.\"\n",
    "    print(emsg)\n",
    "    return [(hostname, emsg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f4f2d",
   "metadata": {},
   "source": [
    "#### Start Triton servers\n",
    "\n",
    "To ensure that only one Triton inference server is started per node, we use stage-level scheduling to delegate each task to a separate GPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c4f2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _use_stage_level_scheduling(spark, rdd):\n",
    "\n",
    "    if spark.version < \"3.4.0\":\n",
    "        raise Exception(\"Stage-level scheduling is not supported in Spark < 3.4.0\")\n",
    "\n",
    "    executor_cores = spark.conf.get(\"spark.executor.cores\")\n",
    "    assert executor_cores is not None, \"spark.executor.cores is not set\"\n",
    "    executor_gpus = spark.conf.get(\"spark.executor.resource.gpu.amount\")\n",
    "    assert executor_gpus is not None and int(executor_gpus) <= 1, \"spark.executor.resource.gpu.amount must be set and <= 1\"\n",
    "\n",
    "    from pyspark.resource.profile import ResourceProfileBuilder\n",
    "    from pyspark.resource.requests import TaskResourceRequests\n",
    "\n",
    "    spark_plugins = spark.conf.get(\"spark.plugins\", \" \")\n",
    "    assert spark_plugins is not None\n",
    "    spark_rapids_sql_enabled = spark.conf.get(\"spark.rapids.sql.enabled\", \"true\")\n",
    "    assert spark_rapids_sql_enabled is not None\n",
    "\n",
    "    task_cores = (\n",
    "        int(executor_cores)\n",
    "        if \"com.nvidia.spark.SQLPlugin\" in spark_plugins\n",
    "        and \"true\" == spark_rapids_sql_enabled.lower()\n",
    "        else (int(executor_cores) // 2) + 1\n",
    "    )\n",
    "\n",
    "    task_gpus = 1.0\n",
    "    treqs = TaskResourceRequests().cpus(task_cores).resource(\"gpu\", task_gpus)\n",
    "    rp = ResourceProfileBuilder().require(treqs).build\n",
    "    print(f\"Reqesting stage-level resources: (cores={task_cores}, gpu={task_gpus})\")\n",
    "\n",
    "    return rdd.withResources(rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463c517",
   "metadata": {},
   "source": [
    "**Specify the number of nodes in the cluster.**  \n",
    "Following the README, the example standalone cluster uses 1 node. The example Databricks/Dataproc cluster scripts use 4 nodes by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4757163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change based on cluster setup\n",
    "num_nodes = 1 if on_standalone else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "nodeRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "nodeRDD = _use_stage_level_scheduling(spark, nodeRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febf6e8",
   "metadata": {},
   "source": [
    "Triton occupies ports for HTTP requests, GRPC requests, and the metrics service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e786e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ports():\n",
    "    import psutil\n",
    "    \n",
    "    ports = []\n",
    "    conns = [conn.laddr.port for conn in psutil.net_connections(kind=\"inet\")]\n",
    "    i = 7000\n",
    "    while len(ports) < 3:\n",
    "        if i not in conns:\n",
    "            ports.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    return ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79a4e9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ports [7000, 7001, 7002]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"SentimentAnalysis\"\n",
    "\n",
    "ports = find_ports()\n",
    "assert len(ports) == 3\n",
    "print(f\"Using ports {ports}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a1a4c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton Server PIDs:\n",
      " {\n",
      "    \"cb4ae00-lcedt\": 2571652\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pids = nodeRDD.barrier().mapPartitions(lambda _: start_triton(ports, model_name)).collectAsMap()\n",
    "print(\"Triton Server PIDs:\\n\", json.dumps(pids, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae0b8e",
   "metadata": {},
   "source": [
    "#### Define client function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6899d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"http://localhost:{ports[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14760940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fn(url, model_name, init_timeout_s):\n",
    "    import numpy as np\n",
    "    from pytriton.client import ModelClient\n",
    "\n",
    "    print(f\"Connecting to Triton model {model_name} at {url}.\")\n",
    "\n",
    "    def infer_batch(inputs):\n",
    "        with ModelClient(url, model_name, init_timeout_s=init_timeout_s) as client:\n",
    "            flattened = np.squeeze(inputs).tolist()\n",
    "            # Encode batch\n",
    "            encoded_batch = [[text.encode(\"utf-8\")] for text in flattened]\n",
    "            encoded_batch_np = np.array(encoded_batch, dtype=np.bytes_)\n",
    "            # Run inference\n",
    "            result_data = client.infer_batch(encoded_batch_np)\n",
    "            result_data = np.squeeze(result_data[\"outputs\"], -1)\n",
    "            return [json.loads(o) for o in result_data]\n",
    "        \n",
    "    return infer_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741e23a",
   "metadata": {},
   "source": [
    "#### Load and preprocess DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccc884a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def preprocess(text: pd.Series) -> pd.Series:\n",
    "    return pd.Series([s.split(\".\")[0] for s in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c426fdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/06 18:29:56 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(data_path).limit(256).repartition(8)\n",
    "df = df.select(preprocess(col(\"text\")).alias(\"input\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da06df4",
   "metadata": {},
   "source": [
    "#### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3930cfcd-3284-4c6a-a9b5-36b8053fe899",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(partial(triton_fn, url=url, model_name=model_name, init_timeout_s=600),\n",
    "                             return_type=StructType([\n",
    "                                 StructField(\"label\", StringType(), True),\n",
    "                                 StructField(\"score\", FloatType(), True)\n",
    "                             ]),\n",
    "                             input_tensor_shapes=[[1]],\n",
    "                             batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8eecbf23-4e9e-4d4c-8645-98209b25db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 ms, sys: 3.77 ms, total: 20.4 ms\n",
      "Wall time: 2.58 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(struct(\"input\"))).select(\"input\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "566ba28c-0ca4-4479-a24a-c8a362228b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.77 ms, sys: 0 ns, total: 2.77 ms\n",
      "Wall time: 462 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", classify(\"input\")).select(\"input\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44c7e776-08da-484a-ba07-9d6add1a0f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 ms, sys: 2.71 ms, total: 5.22 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", classify(col(\"input\"))).select(\"input\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f61d79f8-661e-4d9e-a3aa-c0754b854603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+--------+----------+\n",
      "|                                                                 input|   label|     score|\n",
      "+----------------------------------------------------------------------+--------+----------+\n",
      "|Doesn't anyone bother to check where this kind of sludge comes from...|NEGATIVE| 0.9984042|\n",
      "|There were two things I hated about WASTED : The directing and the ...|NEGATIVE| 0.9979019|\n",
      "|  I'm rather surprised that anybody found this film touching or moving|POSITIVE| 0.8392794|\n",
      "|Cultural Vandalism Is the new Hallmark production of Gulliver's Tra...|NEGATIVE|0.99726933|\n",
      "|I was at Wrestlemania VI in Toronto as a 10 year old, and the event...|POSITIVE|0.98212516|\n",
      "|                                       This movie has been done before|NEGATIVE|0.94194806|\n",
      "|[ as a new resolution for this year 2005, i decide to write a comme...|NEGATIVE|0.99678314|\n",
      "|This movie is over hyped!! I am sad to say that I manage to watch t...|NEGATIVE| 0.9985846|\n",
      "|This show had a promising start as sort of the opposite of 'Oceans ...|NEGATIVE|0.99926823|\n",
      "|MINOR PLOT SPOILERS AHEAD!!!<br /><br />How did such talented actor...|NEGATIVE| 0.9995671|\n",
      "|There is not one character on this sitcom with any redeeming qualities|NEGATIVE|0.99856514|\n",
      "|    Tommy Lee Jones was the best Woodroe and no one can play Woodroe F|POSITIVE| 0.9945687|\n",
      "|   My wife rented this movie and then conveniently never got to see it|NEGATIVE| 0.9984137|\n",
      "|This is one of those star-filled over-the-top comedies that could a...|NEGATIVE| 0.9953224|\n",
      "|This excruciatingly boring and unfunny movie made me think that Cha...|NEGATIVE| 0.9997607|\n",
      "|you will likely be sorely disappointed by this sequel that's not a ...|NEGATIVE|0.99971956|\n",
      "|If I was British, I would be embarrassed by this portrayal of incom...|NEGATIVE|0.99651587|\n",
      "|One of those movies in which there are no big twists whatsoever and...|NEGATIVE|0.99860746|\n",
      "|This show is like watching someone who is in training to someday ho...|NEGATIVE|  0.970153|\n",
      "|                                                                  Sigh|NEGATIVE|0.99231356|\n",
      "+----------------------------------------------------------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248858c",
   "metadata": {},
   "source": [
    "#### Shut down server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3a4e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reqesting stage-level resources: (cores=5, gpu=1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(pids):\n",
    "    import os\n",
    "    import socket\n",
    "    import signal\n",
    "    import time \n",
    "    \n",
    "    hostname = socket.gethostname()\n",
    "    pid = pids.get(hostname, None)\n",
    "    assert pid is not None, f\"Could not find pid for {hostname}\"\n",
    "    \n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            os.kill(pid, signal.SIGTERM)\n",
    "        except OSError:\n",
    "            return [True]\n",
    "        time.sleep(5)\n",
    "\n",
    "    return [False]\n",
    "\n",
    "shutdownRDD = sc.parallelize(list(range(num_nodes)), num_nodes)\n",
    "shutdownRDD = _use_stage_level_scheduling(spark, shutdownRDD)\n",
    "shutdownRDD.barrier().mapPartitions(lambda _: stop_triton(pids)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f19643c-4ee4-44f2-b762-2078c0c8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a538c47-317d-4cac-b9b9-559e88677518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-dl-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
