{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f7ac5d-4a95-4170-a0ac-a7faac9d9ef4",
   "metadata": {},
   "source": [
    "# PySpark Huggingface Inferencing\n",
    "### Text Classification using Pipelines with Tensorflow\n",
    "\n",
    "Based on: https://huggingface.co/docs/transformers/quicktour#pipeline-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd0f77b-ee1b-4477-a038-d25a4f1da0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:33:36.783509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 17:33:36.800587: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 17:33:36.805800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 17:33:36.819189: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 17:33:37.652146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/rishic/anaconda3/envs/spark-dl-tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80fc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device if tensorflow gpu is available\n",
    "device = 0 if tf.config.list_physical_devices('GPU') else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e60a2877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553b28d2-a5d1-4d07-8a49-8f82b808e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "2024-09-25 17:33:42.546470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30099 MB memory:  -> device: 0, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:34:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.547912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31135 MB memory:  -> device: 1, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:36:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.549203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31135 MB memory:  -> device: 2, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:39:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.550429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 31135 MB memory:  -> device: 3, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.551725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 31135 MB memory:  -> device: 4, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:57:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.552935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 31135 MB memory:  -> device: 5, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:59:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.554159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 31135 MB memory:  -> device: 6, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5c:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.555381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 31135 MB memory:  -> device: 7, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5e:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.556662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:8 with 31135 MB memory:  -> device: 8, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b7:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.557902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:9 with 31135 MB memory:  -> device: 9, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b9:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.559120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:10 with 31135 MB memory:  -> device: 10, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:bc:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.560361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:11 with 31135 MB memory:  -> device: 11, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:be:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.561590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:12 with 31135 MB memory:  -> device: 12, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e0:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.562780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:13 with 31135 MB memory:  -> device: 13, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e2:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.564011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:14 with 31135 MB memory:  -> device: 14, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e5:00.0, compute capability: 7.0\n",
      "2024-09-25 17:33:42.565150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:15 with 31135 MB memory:  -> device: 15, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e7:00.0, compute capability: 7.0\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "/rishic/anaconda3/envs/spark-dl-tf/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b91fe91-b725-4564-ae93-56e3fb51e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier((\"We are very happy to show you the ðŸ¤— Transformers library.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be39eb3-462c-42ff-b8f4-09f4e4fe3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c90100",
   "metadata": {},
   "source": [
    "#### Use another model and tokenizer in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9d3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e21b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31079133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272652983665466}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92b15e-0da0-46c3-81a3-fabaedbfc42c",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dd6a1a-f450-47f0-9dbf-ad250585a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, struct, pandas_udf\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0e0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "24/09/25 17:33:48 WARN Utils: Your hostname, dgx2h0194.spark.sjc4.nvmetal.net resolves to a loopback address: 127.0.1.1; using 10.150.30.2 instead (on interface enp134s0f0np0)\n",
      "24/09/25 17:33:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/25 17:33:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "num_threads = 6\n",
    "\n",
    "# Creating a local Spark session for demonstration, in case it hasn't already been created.\n",
    "\n",
    "_config = {\n",
    "    \"spark.master\": f\"local[{num_threads}]\",\n",
    "    \"spark.driver.host\": \"127.0.0.1\",\n",
    "    \"spark.task.maxFailures\": \"1\",\n",
    "    \"spark.driver.memory\": \"8g\",\n",
    "    \"spark.executor.memory\": \"8g\",\n",
    "    \"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\": \"false\",\n",
    "    \"spark.sql.pyspark.jvmStacktrace.enabled\": \"true\",\n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "    \"spark.python.worker.reuse\": \"true\",\n",
    "}\n",
    "spark = SparkSession.builder.appName(\"spark-dl-example\")\n",
    "for key, value in _config.items():\n",
    "    spark = spark.config(key, value)\n",
    "spark = spark.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d70208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDB dataset\n",
    "data = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "lines = []\n",
    "for example in data:\n",
    "    # first sentence only\n",
    "    lines.append([example[\"text\"]])\n",
    "\n",
    "len(lines)\n",
    "\n",
    "df = spark.createDataFrame(lines, ['lines']).repartition(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac24f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 17:33:56 WARN TaskSetManager: Stage 0 contains a task of very large size (5123 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"imdb_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9665b7b6-d7e9-4bd4-b29d-7a449ac5b574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|                                                                        sentence|\n",
      "+--------------------------------------------------------------------------------+\n",
      "|In following Dylan Moran's star from the charming misanthrope bookstore owner...|\n",
      "|Here in Australia Nights in Rodanthe is being promoted in the same class as t...|\n",
      "|           The Tender Hook, or, Who Killed The Australian Film Industry? Case No|\n",
      "|The only reason I'm even giving this movie a 4 is because it was made in to a...|\n",
      "|Hooray for Title Misspellings! After reading reviews and contemplating, my gi...|\n",
      "|                        This movie makes you wish imdb would let you vote a zero|\n",
      "|To me this film is just a very very lame teen party movie with all the normal...|\n",
      "|                                                                         I tried|\n",
      "|Awkward disaster mishmash has a team of scavengers coming across the overturn...|\n",
      "|According to the blurb on the back of the DVD case; Jonothan Ross 'laughed un...|\n",
      "|           It'll be a blue Christmas indeed if you subject you're family to this|\n",
      "|This is the story of a maniac cop who, for some reason, has it in for a young...|\n",
      "|I bought a tape of this film based on the recommendation of other IMDb users ...|\n",
      "|             I usually don't consider turning a movie off unless it's REALLY bad|\n",
      "|I watched this years ago on television when I was sick (I don't know, I tend ...|\n",
      "|Am I the only one to notice that the \"realism\" of the 19th century ship is er...|\n",
      "|This movie, despite its list of B, C, and D list celebs, is a complete waste ...|\n",
      "|To put in simple words or rather a word, would be best suited by PATHETIC !!!...|\n",
      "|You may like Tim Burton's fantasies, but not in a commercial-like show off la...|\n",
      "|                      I've expected a comedy about the NVA, but this is a parody|\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only use first sentence of IMDB reviews\n",
    "@pandas_udf(\"string\")\n",
    "def first_sentence(text: pd.Series) -> pd.Series:\n",
    "    return pd.Series([s.split(\".\")[0] for s in text])\n",
    "\n",
    "df = spark.read.parquet(\"imdb_test\").withColumn(\"sentence\", first_sentence(col(\"lines\"))).select(\"sentence\").limit(100).cache()\n",
    "df.show(truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da9d25c-5ebe-4503-bb19-154fcc047cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import tensorflow as tf\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Enable GPU memory growth\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    device = 0 if tf.config.list_physical_devices('GPU') else -1\n",
    "    pipe = pipeline(\"sentiment-analysis\", device=device)\n",
    "    def predict(inputs):\n",
    "        return pipe(inputs.tolist())\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78afef29-ee30-4267-9fb6-be2dcb86cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=StructType([\n",
    "                                 StructField(\"label\", StringType(), True),\n",
    "                                 StructField(\"score\", FloatType(), True)\n",
    "                             ]),\n",
    "                             batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5bc327e-89cf-4731-82e6-e66cb93deef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:34:01.869946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 17:34:01.886921: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 17:34:01.891972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 17:34:01.904245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 17:34:02.586399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "2024-09-25 17:34:07.401728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 27727 MB memory:  -> device: 0, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:34:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.403125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30827 MB memory:  -> device: 1, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:36:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.404374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30827 MB memory:  -> device: 2, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:39:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.405625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30827 MB memory:  -> device: 3, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.406860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 30827 MB memory:  -> device: 4, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:57:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.408086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 30827 MB memory:  -> device: 5, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:59:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.409349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 30827 MB memory:  -> device: 6, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5c:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.410879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 30827 MB memory:  -> device: 7, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5e:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.412141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:8 with 30827 MB memory:  -> device: 8, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b7:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.413328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:9 with 30827 MB memory:  -> device: 9, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b9:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.414533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:10 with 30827 MB memory:  -> device: 10, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:bc:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.415768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:11 with 30827 MB memory:  -> device: 11, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:be:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.416947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:12 with 30827 MB memory:  -> device: 12, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e0:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.418134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:13 with 30827 MB memory:  -> device: 13, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e2:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.419377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:14 with 30827 MB memory:  -> device: 14, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e5:00.0, compute capability: 7.0\n",
      "2024-09-25 17:34:07.420560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:15 with 30827 MB memory:  -> device: 15, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e7:00.0, compute capability: 7.0\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "/rishic/anaconda3/envs/spark-dl-tf/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.7 ms, sys: 60.1 ms, total: 109 ms\n",
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(struct(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac642895-cfd6-47ee-9b21-02e7835424e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 ms, sys: 7.42 ms, total: 41.2 ms\n",
      "Wall time: 6.99 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(\"sentence\")).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76a44d80-d5db-405f-989c-7246379cfb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 ms, sys: 18.5 ms, total: 42.8 ms\n",
      "Wall time: 7.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(col(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c01761b3-c766-46b0-ae0b-fcf968ffb3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "|                                                                        sentence|   label|     score|\n",
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "|In following Dylan Moran's star from the charming misanthrope bookstore owner...|NEGATIVE| 0.9464624|\n",
      "|Here in Australia Nights in Rodanthe is being promoted in the same class as t...|NEGATIVE| 0.9811111|\n",
      "|           The Tender Hook, or, Who Killed The Australian Film Industry? Case No|NEGATIVE|0.98526716|\n",
      "|The only reason I'm even giving this movie a 4 is because it was made in to a...|NEGATIVE| 0.9995627|\n",
      "|Hooray for Title Misspellings! After reading reviews and contemplating, my gi...|NEGATIVE|0.99946946|\n",
      "|                        This movie makes you wish imdb would let you vote a zero|NEGATIVE| 0.9981305|\n",
      "|To me this film is just a very very lame teen party movie with all the normal...|NEGATIVE| 0.9989936|\n",
      "|                                                                         I tried|POSITIVE|0.90713096|\n",
      "|Awkward disaster mishmash has a team of scavengers coming across the overturn...|NEGATIVE|0.99977237|\n",
      "|According to the blurb on the back of the DVD case; Jonothan Ross 'laughed un...|NEGATIVE|0.95564634|\n",
      "|           It'll be a blue Christmas indeed if you subject you're family to this|POSITIVE| 0.9996747|\n",
      "|This is the story of a maniac cop who, for some reason, has it in for a young...|NEGATIVE|0.95095587|\n",
      "|I bought a tape of this film based on the recommendation of other IMDb users ...|NEGATIVE| 0.9994485|\n",
      "|             I usually don't consider turning a movie off unless it's REALLY bad|NEGATIVE| 0.9983766|\n",
      "|I watched this years ago on television when I was sick (I don't know, I tend ...|NEGATIVE| 0.9987041|\n",
      "|Am I the only one to notice that the \"realism\" of the 19th century ship is er...|NEGATIVE|   0.99914|\n",
      "|This movie, despite its list of B, C, and D list celebs, is a complete waste ...|NEGATIVE|0.99979585|\n",
      "|To put in simple words or rather a word, would be best suited by PATHETIC !!!...|NEGATIVE|0.99920565|\n",
      "|You may like Tim Burton's fantasies, but not in a commercial-like show off la...|NEGATIVE| 0.9734402|\n",
      "|                      I've expected a comedy about the NVA, but this is a parody|NEGATIVE| 0.9439921|\n",
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb826fde-99d9-43fe-8ddc-f5acbe76b4e9",
   "metadata": {},
   "source": [
    "### Using Triton Inference Server\n",
    "\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment.  \n",
    "While the examples above use Tensorflow, note that inference on the Triton server is run using PyTorch. PyTorch will be included in the tarball below, and no further environment changes are required by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10368010-f94d-4167-91a1-2cf9ed91a2c9",
   "metadata": {},
   "source": [
    "This notebook uses the [Python backend with a custom execution environment](https://github.com/triton-inference-server/python_backend#creating-custom-execution-environments) with the compatible versions of Python/Numpy for Triton 24.08, using a conda-pack environment created as follows:\n",
    "```\n",
    "conda create -n huggingface -c conda-forge python=3.10.0\n",
    "conda activate huggingface\n",
    "\n",
    "export PYTHONNOUSERSITE=True\n",
    "pip install numpy==1.26.4 conda-pack sentencepiece sentence_transformers transformers\n",
    "\n",
    "conda-pack  # huggingface.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d4be844-4b8c-47df-bd09-0c280c7ff16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import col, struct, pandas_udf\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e53df9f-43cb-4c38-b8ac-dc2cbad99815",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# copy custom model to expected layout for Triton\n",
    "rm -rf models\n",
    "mkdir -p models\n",
    "cp -r models_config/hf_pipeline models\n",
    "\n",
    "# add custom execution environment\n",
    "cp huggingface.tar.gz models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a5b06-126a-4bc4-baae-a45ea30832a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "144acb8e-4c08-40fc-a9ed-f721c409ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>> containers: ['743900a7a117']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_executors = 1\n",
    "triton_models_dir = \"{}/models\".format(os.getcwd())\n",
    "huggingface_cache_dir = \"{}/.cache/huggingface\".format(os.path.expanduser('~'))\n",
    "nodeRDD = sc.parallelize(list(range(num_executors)), num_executors)\n",
    "\n",
    "def start_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    if containers:\n",
    "        print(\">>>> containers: {}\".format([c.short_id for c in containers]))\n",
    "    else:\n",
    "        container=client.containers.run(\n",
    "            \"nvcr.io/nvidia/tritonserver:24.08-py3\", \"tritonserver --model-repository=/models\",\n",
    "            detach=True,\n",
    "            device_requests=[docker.types.DeviceRequest(device_ids=[\"0\"], capabilities=[['gpu']])],\n",
    "            environment=[\n",
    "                \"TRANSFORMERS_CACHE=/cache\"\n",
    "            ],\n",
    "            name=\"spark-triton\",\n",
    "            network_mode=\"host\",\n",
    "            remove=True,\n",
    "            shm_size=\"256M\",\n",
    "            volumes={\n",
    "                triton_models_dir: {\"bind\": \"/models\", \"mode\": \"ro\"},\n",
    "                huggingface_cache_dir: {\"bind\": \"/cache\", \"mode\": \"rw\"}\n",
    "            }\n",
    "        )\n",
    "        print(\">>>> starting triton: {}\".format(container.short_id))\n",
    "        # wait for triton to be running\n",
    "        time.sleep(15)\n",
    "        \n",
    "        client = grpcclient.InferenceServerClient(\"localhost:8001\")\n",
    "        \n",
    "        elapsed = 0\n",
    "        timeout = 120\n",
    "        ready = False\n",
    "        while not ready and elapsed < timeout:\n",
    "            try:\n",
    "                time.sleep(5)\n",
    "                elapsed += 5\n",
    "                ready = client.is_server_ready()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return [True]\n",
    "\n",
    "nodeRDD.barrier().mapPartitions(start_triton).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d77ab-60d3-45eb-a9c2-dc811eca0af4",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d53fb283-bf9e-4571-8c68-b75a41f1f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use first sentence of IMDB reviews\n",
    "@pandas_udf(\"string\")\n",
    "def first_sentence(text: pd.Series) -> pd.Series:\n",
    "    return pd.Series([s.split(\".\")[0] for s in text])\n",
    "\n",
    "df = spark.read.parquet(\"imdb_test\").withColumn(\"sentence\", first_sentence(col(\"lines\"))).select(\"sentence\").limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29b0cc0d-c480-4e4a-bd41-207dc314cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fn(triton_uri, model_name):\n",
    "    import numpy as np\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    np_types = {\n",
    "      \"BOOL\": np.dtype(np.bool_),\n",
    "      \"INT8\": np.dtype(np.int8),\n",
    "      \"INT16\": np.dtype(np.int16),\n",
    "      \"INT32\": np.dtype(np.int32),\n",
    "      \"INT64\": np.dtype(np.int64),\n",
    "      \"FP16\": np.dtype(np.float16),\n",
    "      \"FP32\": np.dtype(np.float32),\n",
    "      \"FP64\": np.dtype(np.float64),\n",
    "      \"FP64\": np.dtype(np.double),\n",
    "      \"BYTES\": np.dtype(object)\n",
    "    }\n",
    "\n",
    "    client = grpcclient.InferenceServerClient(triton_uri)\n",
    "    model_meta = client.get_model_metadata(model_name)\n",
    "    \n",
    "    def predict(inputs):\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            # single ndarray input\n",
    "            request = [grpcclient.InferInput(model_meta.inputs[0].name, inputs.shape, model_meta.inputs[0].datatype)]\n",
    "            request[0].set_data_from_numpy(inputs.astype(np_types[model_meta.inputs[0].datatype]))\n",
    "        else:\n",
    "            # dict of multiple ndarray inputs\n",
    "            request = [grpcclient.InferInput(i.name, inputs[i.name].shape, i.datatype) for i in model_meta.inputs]\n",
    "            for i in request:\n",
    "                i.set_data_from_numpy(inputs[i.name()].astype(np_types[i.datatype()]))\n",
    "        \n",
    "        response = client.infer(model_name, inputs=request)\n",
    "        \n",
    "        if len(model_meta.outputs) > 1:\n",
    "            # return dictionary of numpy arrays\n",
    "            return {o.name: response.as_numpy(o.name) for o in model_meta.outputs}\n",
    "        else:\n",
    "            # return single numpy array\n",
    "            return response.as_numpy(model_meta.outputs[0].name)\n",
    "        \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3930cfcd-3284-4c6a-a9b5-36b8053fe899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "classify = predict_batch_udf(partial(triton_fn, triton_uri=\"localhost:8001\", model_name=\"hf_pipeline\"),\n",
    "                             return_type=StructType([\n",
    "                                 StructField(\"label\", StringType(), True),\n",
    "                                 StructField(\"score\", FloatType(), True)\n",
    "                             ]),\n",
    "                             input_tensor_shapes=[[1]],\n",
    "                             batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eecbf23-4e9e-4d4c-8645-98209b25db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 ms, sys: 32 ms, total: 47.9 ms\n",
      "Wall time: 5.74 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(struct(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "566ba28c-0ca4-4479-a24a-c8a362228b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 ms, sys: 22.5 ms, total: 48 ms\n",
      "Wall time: 5.85 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(\"sentence\")).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44c7e776-08da-484a-ba07-9d6add1a0f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 ms, sys: 21.8 ms, total: 45.2 ms\n",
      "Wall time: 5.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(col(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f61d79f8-661e-4d9e-a3aa-c0754b854603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|sentence                                                                                                                                                                                                                                                                                                |label   |score     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|In following Dylan Moran's star from the charming misanthrope bookstore owner in the surrealist sitcom Black Books, I could see his comic potential begging to be utilised in theater or larger cinematic avenues                                                                                       |NEGATIVE|0.94646263|\n",
      "|Here in Australia Nights in Rodanthe is being promoted in the same class as the Notebook                                                                                                                                                                                                                |NEGATIVE|0.9811111 |\n",
      "|The Tender Hook, or, Who Killed The Australian Film Industry? Case No                                                                                                                                                                                                                                   |NEGATIVE|0.98526716|\n",
      "|The only reason I'm even giving this movie a 4 is because it was made in to an episode of Mystery Science Theater 3000                                                                                                                                                                                  |NEGATIVE|0.9995627 |\n",
      "|Hooray for Title Misspellings! After reading reviews and contemplating, my girlfriend and I confirmed that this movie is an utter piece of trash                                                                                                                                                        |NEGATIVE|0.99946946|\n",
      "|This movie makes you wish imdb would let you vote a zero                                                                                                                                                                                                                                                |NEGATIVE|0.9981305 |\n",
      "|To me this film is just a very very lame teen party movie with all the normal clichÃ©s and boring stereotyped characters (Nerds, Jocks, Popular girls, Sleezy guys, etc) but with an underlying anti drug/drinking theme                                                                                 |NEGATIVE|0.9989936 |\n",
      "|I tried                                                                                                                                                                                                                                                                                                 |POSITIVE|0.907131  |\n",
      "|Awkward disaster mishmash has a team of scavengers coming across the overturned S                                                                                                                                                                                                                       |NEGATIVE|0.99977237|\n",
      "|According to the blurb on the back of the DVD case; Jonothan Ross 'laughed until a little bit of wee came out'                                                                                                                                                                                          |NEGATIVE|0.95564646|\n",
      "|It'll be a blue Christmas indeed if you subject you're family to this                                                                                                                                                                                                                                   |POSITIVE|0.9996747 |\n",
      "|This is the story of a maniac cop who, for some reason, has it in for a young college stud and his mates                                                                                                                                                                                                |NEGATIVE|0.95095587|\n",
      "|I bought a tape of this film based on the recommendation of other IMDb users and have to say that I was very disappointed                                                                                                                                                                               |NEGATIVE|0.9994485 |\n",
      "|I usually don't consider turning a movie off unless it's REALLY bad                                                                                                                                                                                                                                     |NEGATIVE|0.9983766 |\n",
      "|I watched this years ago on television when I was sick (I don't know, I tend to be more complacent with my TV viewing when I'm sick; too much effort to use the remote control, I guess)                                                                                                                |NEGATIVE|0.9987041 |\n",
      "|Am I the only one to notice that the \"realism\" of the 19th century ship is erroneous                                                                                                                                                                                                                    |NEGATIVE|0.99914   |\n",
      "|This movie, despite its list of B, C, and D list celebs, is a complete waste of 90 minutes                                                                                                                                                                                                              |NEGATIVE|0.99979585|\n",
      "|To put in simple words or rather a word, would be best suited by PATHETIC !!!!!! The movie starts with attracting a little interest by the plot, but, BUT as few minutes by audience is getting restless for restrooms and getting snacks, or to get a breathe of fresh air outside the closed dark hall|NEGATIVE|0.99920565|\n",
      "|You may like Tim Burton's fantasies, but not in a commercial-like show off lasting 8 minutes                                                                                                                                                                                                            |NEGATIVE|0.9734402 |\n",
      "|I've expected a comedy about the NVA, but this is a parody                                                                                                                                                                                                                                              |NEGATIVE|0.9439922 |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197c146-1794-47f0-bcd9-7e8d8ab8625f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stop Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "425d3b28-7705-45ba-8a18-ad34fc895219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>> stopping containers: ['743900a7a117']\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    print(\">>>> stopping containers: {}\".format([c.short_id for c in containers]))\n",
    "    if containers:\n",
    "        container=containers[0]\n",
    "        container.stop(timeout=120)\n",
    "\n",
    "    return [True]\n",
    "\n",
    "nodeRDD.barrier().mapPartitions(stop_triton).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f19643c-4ee4-44f2-b762-2078c0c8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a538c47-317d-4cac-b9b9-559e88677518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
