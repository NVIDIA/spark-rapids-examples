{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f7ac5d-4a95-4170-a0ac-a7faac9d9ef4",
   "metadata": {},
   "source": [
    "# PySpark Huggingface Inferencing\n",
    "### Text Classification using Pipelines with Tensorflow\n",
    "\n",
    "Based on: https://huggingface.co/docs/transformers/quicktour#pipeline-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799fd4f",
   "metadata": {},
   "source": [
    "### Using TensorFlow\n",
    "Note that cuFFT/cuDNN/cuBLAS registration errors are expected with `tf=2.17.0` and will not affect behavior, as noted in [this issue.](https://github.com/tensorflow/tensorflow/issues/62075)  \n",
    "This notebook does not demonstrate inference with TensorRT, as [TF-TRT](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#tensorrt-10) does not yet support `tf=2.17.0`. See the `pytorch` notebooks for TensorRT demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd0f77b-ee1b-4477-a038-d25a4f1da0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 16:47:48.209366: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-03 16:47:48.215921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-03 16:47:48.223519: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-03 16:47:48.225906: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-03 16:47:48.231640: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-03 16:47:48.625790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80fc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device if tensorflow gpu is available\n",
    "device = 0 if tf.config.list_physical_devices('GPU') else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60a2877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553b28d2-a5d1-4d07-8a49-8f82b808e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "2024-10-03 16:47:49.863791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46447 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b91fe91-b725-4564-ae93-56e3fb51e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997794032096863}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier((\"We are very happy to show you the ðŸ¤— Transformers library.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be39eb3-462c-42ff-b8f4-09f4e4fe3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5282\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c90100",
   "metadata": {},
   "source": [
    "#### Use another model and tokenizer in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9d3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e21b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31079133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272655963897705}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92b15e-0da0-46c3-81a3-fabaedbfc42c",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69dd6a1a-f450-47f0-9dbf-ad250585a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, struct, pandas_udf\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "\n",
    "conf = SparkConf()\n",
    "if 'spark' not in globals():\n",
    "    # If Spark is not already started with Jupyter, attach to Spark Standalone\n",
    "    import socket\n",
    "    hostname = socket.gethostname()\n",
    "    conf.setMaster(f\"spark://{hostname}:7077\") # assuming Master is on default port 7077\n",
    "conf.set(\"spark.task.maxFailures\", \"1\")\n",
    "conf.set(\"spark.driver.memory\", \"8g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.pyspark.python\", f\"{conda_env}/bin/python\")\n",
    "conf.set(\"spark.pyspark.driver.python\", f\"{conda_env}/bin/python\")\n",
    "conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"spark-dl-examples\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d70208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDB dataset\n",
    "data = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "lines = []\n",
    "for example in data:\n",
    "    # first sentence only\n",
    "    lines.append([example[\"text\"]])\n",
    "\n",
    "len(lines)\n",
    "\n",
    "df = spark.createDataFrame(lines, ['lines']).repartition(8).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac24f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 16:47:58 WARN TaskSetManager: Stage 0 contains a task of very large size (3860 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"imdb_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9665b7b6-d7e9-4bd4-b29d-7a449ac5b574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|                                                                        sentence|\n",
      "+--------------------------------------------------------------------------------+\n",
      "|                                                                                |\n",
      "|Hard up, No proper jobs going down at the pit, why not rent your kids! DIY pi...|\n",
      "|I watched this movie to see the direction one of the most promising young tal...|\n",
      "|                        This movie makes you wish imdb would let you vote a zero|\n",
      "|I never want to see this movie again!<br /><br />Not only is it dreadfully ba...|\n",
      "|(As a note, I'd like to say that I saw this movie at my annual church camp, w...|\n",
      "|                 Don't get me wrong, I love the TV series of League Of Gentlemen|\n",
      "|Did you ever think, like after watching a horror movie with a group of friend...|\n",
      "|                                                             Awful, awful, awful|\n",
      "|This movie seems a little clunky around the edges, like not quite enough zani...|\n",
      "|I rented this movie hoping that it would provide some good entertainment and ...|\n",
      "|Well, where to start describing this celluloid debacle? You already know the ...|\n",
      "|                                  I hoped for this show to be somewhat realistic|\n",
      "|                                                   All I have to say is one word|\n",
      "|Honestly awful film, bad editing, awful lighting, dire dialog and scrappy scr...|\n",
      "|This critique tells the story of 4 little friends who went to watch Angels an...|\n",
      "|                                          This review contains a partial spoiler|\n",
      "|            I'm rather surprised that anybody found this film touching or moving|\n",
      "|       If you like bad movies (and you must to watch this one) here's a good one|\n",
      "|This is really bad, the characters were bland, the story was boring, and ther...|\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only use first sentence of IMDB reviews\n",
    "@pandas_udf(\"string\")\n",
    "def first_sentence(text: pd.Series) -> pd.Series:\n",
    "    return pd.Series([s.split(\".\")[0] for s in text])\n",
    "\n",
    "df = spark.read.parquet(\"imdb_test\").withColumn(\"sentence\", first_sentence(col(\"lines\"))).select(\"sentence\").limit(100).cache()\n",
    "df.show(truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0da9d25c-5ebe-4503-bb19-154fcc047cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import tensorflow as tf\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Enable GPU memory growth\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    device = 0 if tf.config.list_physical_devices('GPU') else -1\n",
    "    pipe = pipeline(\"sentiment-analysis\", device=device)\n",
    "    def predict(inputs):\n",
    "        return pipe(inputs.tolist())\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78afef29-ee30-4267-9fb6-be2dcb86cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=StructType([\n",
    "                                 StructField(\"label\", StringType(), True),\n",
    "                                 StructField(\"score\", FloatType(), True)\n",
    "                             ]),\n",
    "                             batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5bc327e-89cf-4731-82e6-e66cb93deef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.15 ms, sys: 6.76 ms, total: 15.9 ms\n",
      "Wall time: 5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(struct(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac642895-cfd6-47ee-9b21-02e7835424e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.86 ms, sys: 2.19 ms, total: 7.05 ms\n",
      "Wall time: 2.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(\"sentence\")).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76a44d80-d5db-405f-989c-7246379cfb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 ms, sys: 1.96 ms, total: 5.87 ms\n",
      "Wall time: 2.76 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(col(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c01761b3-c766-46b0-ae0b-fcf968ffb3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "|                                                                        sentence|   label|     score|\n",
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "|                                                                                |POSITIVE|0.74807304|\n",
      "|Hard up, No proper jobs going down at the pit, why not rent your kids! DIY pi...|NEGATIVE| 0.9996724|\n",
      "|I watched this movie to see the direction one of the most promising young tal...|POSITIVE| 0.9994948|\n",
      "|                        This movie makes you wish imdb would let you vote a zero|NEGATIVE| 0.9981299|\n",
      "|I never want to see this movie again!<br /><br />Not only is it dreadfully ba...|NEGATIVE|0.99883264|\n",
      "|(As a note, I'd like to say that I saw this movie at my annual church camp, w...|POSITIVE| 0.9901753|\n",
      "|                 Don't get me wrong, I love the TV series of League Of Gentlemen|POSITIVE|0.99983096|\n",
      "|Did you ever think, like after watching a horror movie with a group of friend...|POSITIVE| 0.9992768|\n",
      "|                                                             Awful, awful, awful|NEGATIVE| 0.9997433|\n",
      "|This movie seems a little clunky around the edges, like not quite enough zani...|NEGATIVE| 0.9996525|\n",
      "|I rented this movie hoping that it would provide some good entertainment and ...|NEGATIVE|0.99643254|\n",
      "|Well, where to start describing this celluloid debacle? You already know the ...|NEGATIVE|0.99973005|\n",
      "|                                  I hoped for this show to be somewhat realistic|POSITIVE| 0.8417903|\n",
      "|                                                   All I have to say is one word|NEGATIVE|0.97844803|\n",
      "|Honestly awful film, bad editing, awful lighting, dire dialog and scrappy scr...|NEGATIVE| 0.9997701|\n",
      "|This critique tells the story of 4 little friends who went to watch Angels an...|POSITIVE| 0.9942386|\n",
      "|                                          This review contains a partial spoiler|NEGATIVE|0.99620205|\n",
      "|            I'm rather surprised that anybody found this film touching or moving|POSITIVE|0.83874947|\n",
      "|       If you like bad movies (and you must to watch this one) here's a good one|POSITIVE| 0.9936475|\n",
      "|This is really bad, the characters were bland, the story was boring, and ther...|NEGATIVE|0.99953806|\n",
      "+--------------------------------------------------------------------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb826fde-99d9-43fe-8ddc-f5acbe76b4e9",
   "metadata": {},
   "source": [
    "### Using Triton Inference Server\n",
    "\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10368010-f94d-4167-91a1-2cf9ed91a2c9",
   "metadata": {},
   "source": [
    "This notebook uses the [Python backend with a custom execution environment](https://github.com/triton-inference-server/python_backend#creating-custom-execution-environments) with the compatible versions of Python/Numpy for Triton 24.08, using a conda-pack environment created as follows:\n",
    "```\n",
    "conda create -n huggingface-tf -c conda-forge python=3.10.0\n",
    "conda activate huggingface-tf\n",
    "\n",
    "export PYTHONNOUSERSITE=True\n",
    "pip install numpy==1.26.4 tensorflow[and-cuda] tf-keras transformers conda-pack\n",
    "\n",
    "conda-pack  # huggingface-tf.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d4be844-4b8c-47df-bd09-0c280c7ff16b",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import col, struct, pandas_udf\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e53df9f-43cb-4c38-b8ac-dc2cbad99815",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# copy custom model to expected layout for Triton\n",
    "rm -rf models\n",
    "mkdir -p models\n",
    "cp -r models_config/hf_pipeline_tf models\n",
    "\n",
    "# add custom execution environment\n",
    "cp huggingface-tf.tar.gz models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a5b06-126a-4bc4-baae-a45ea30832a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "144acb8e-4c08-40fc-a9ed-f721c409ee68",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_executors = 1\n",
    "triton_models_dir = \"{}/models\".format(os.getcwd())\n",
    "huggingface_cache_dir = \"{}/.cache/huggingface\".format(os.path.expanduser('~'))\n",
    "nodeRDD = sc.parallelize(list(range(num_executors)), num_executors)\n",
    "\n",
    "def start_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    if containers:\n",
    "        print(\">>>> containers: {}\".format([c.short_id for c in containers]))\n",
    "    else:\n",
    "        container=client.containers.run(\n",
    "            \"nvcr.io/nvidia/tritonserver:24.08-py3\", \"tritonserver --model-repository=/models\",\n",
    "            detach=True,\n",
    "            device_requests=[docker.types.DeviceRequest(device_ids=[\"0\"], capabilities=[['gpu']])],\n",
    "            environment=[\n",
    "                \"TRANSFORMERS_CACHE=/cache\"\n",
    "            ],\n",
    "            name=\"spark-triton\",\n",
    "            network_mode=\"host\",\n",
    "            remove=True,\n",
    "            shm_size=\"256M\",\n",
    "            volumes={\n",
    "                triton_models_dir: {\"bind\": \"/models\", \"mode\": \"ro\"},\n",
    "                huggingface_cache_dir: {\"bind\": \"/cache\", \"mode\": \"rw\"}\n",
    "            }\n",
    "        )\n",
    "        print(\">>>> starting triton: {}\".format(container.short_id))\n",
    "        # wait for triton to be running\n",
    "        time.sleep(15)\n",
    "        \n",
    "        client = grpcclient.InferenceServerClient(\"localhost:8001\")\n",
    "        \n",
    "        elapsed = 0\n",
    "        timeout = 120\n",
    "        ready = False\n",
    "        while not ready and elapsed < timeout:\n",
    "            try:\n",
    "                time.sleep(5)\n",
    "                elapsed += 5\n",
    "                ready = client.is_server_ready()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return [True]\n",
    "\n",
    "nodeRDD.barrier().mapPartitions(start_triton).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d77ab-60d3-45eb-a9c2-dc811eca0af4",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d53fb283-bf9e-4571-8c68-b75a41f1f067",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [],
   "source": [
    "# only use first sentence of IMDB reviews\n",
    "@pandas_udf(\"string\")\n",
    "def first_sentence(text: pd.Series) -> pd.Series:\n",
    "    return pd.Series([s.split(\".\")[0] for s in text])\n",
    "\n",
    "df = spark.read.parquet(\"imdb_test\").withColumn(\"sentence\", first_sentence(col(\"lines\"))).select(\"sentence\").limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29b0cc0d-c480-4e4a-bd41-207dc314cba5",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [],
   "source": [
    "def triton_fn(triton_uri, model_name):\n",
    "    import numpy as np\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    np_types = {\n",
    "      \"BOOL\": np.dtype(np.bool_),\n",
    "      \"INT8\": np.dtype(np.int8),\n",
    "      \"INT16\": np.dtype(np.int16),\n",
    "      \"INT32\": np.dtype(np.int32),\n",
    "      \"INT64\": np.dtype(np.int64),\n",
    "      \"FP16\": np.dtype(np.float16),\n",
    "      \"FP32\": np.dtype(np.float32),\n",
    "      \"FP64\": np.dtype(np.float64),\n",
    "      \"FP64\": np.dtype(np.double),\n",
    "      \"BYTES\": np.dtype(object)\n",
    "    }\n",
    "\n",
    "    client = grpcclient.InferenceServerClient(triton_uri)\n",
    "    model_meta = client.get_model_metadata(model_name)\n",
    "    \n",
    "    def predict(inputs):\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            # single ndarray input\n",
    "            request = [grpcclient.InferInput(model_meta.inputs[0].name, inputs.shape, model_meta.inputs[0].datatype)]\n",
    "            request[0].set_data_from_numpy(inputs.astype(np_types[model_meta.inputs[0].datatype]))\n",
    "        else:\n",
    "            # dict of multiple ndarray inputs\n",
    "            request = [grpcclient.InferInput(i.name, inputs[i.name].shape, i.datatype) for i in model_meta.inputs]\n",
    "            for i in request:\n",
    "                i.set_data_from_numpy(inputs[i.name()].astype(np_types[i.datatype()]))\n",
    "        \n",
    "        response = client.infer(model_name, inputs=request)\n",
    "        \n",
    "        if len(model_meta.outputs) > 1:\n",
    "            # return dictionary of numpy arrays\n",
    "            return {o.name: response.as_numpy(o.name) for o in model_meta.outputs}\n",
    "        else:\n",
    "            # return single numpy array\n",
    "            return response.as_numpy(model_meta.outputs[0].name)\n",
    "        \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3930cfcd-3284-4c6a-a9b5-36b8053fe899",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "classify = predict_batch_udf(partial(triton_fn, triton_uri=\"localhost:8001\", model_name=\"hf_pipeline_tf\"),\n",
    "                             return_type=StructType([\n",
    "                                 StructField(\"label\", StringType(), True),\n",
    "                                 StructField(\"score\", FloatType(), True)\n",
    "                             ]),\n",
    "                             input_tensor_shapes=[[1]],\n",
    "                             batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eecbf23-4e9e-4d4c-8645-98209b25db2c",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 ms, sys: 5.9 ms, total: 28.4 ms\n",
      "Wall time: 24.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(struct(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "566ba28c-0ca4-4479-a24a-c8a362228b89",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 10.1 ms, total: 22.3 ms\n",
      "Wall time: 23.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(\"sentence\")).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44c7e776-08da-484a-ba07-9d6add1a0f15",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.74 ms, sys: 8.23 ms, total: 17 ms\n",
      "Wall time: 23.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# note: expanding the \"struct\" return_type to top-level columns\n",
    "preds = df.withColumn(\"preds\", classify(col(\"sentence\"))).select(\"sentence\", \"preds.*\")\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f61d79f8-661e-4d9e-a3aa-c0754b854603",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|sentence                                                                                                                                                                                                                                                                   |label   |score     |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|                                                                                                                                                                                                                                                                           |POSITIVE|0.74807304|\n",
      "|Hard up, No proper jobs going down at the pit, why not rent your kids! DIY pimp story without the gratuitous sex scenes, either hard core or soft core, therefore reads like a public information film from the fifties, give this a wide miss, use a barge pole if you can|NEGATIVE|0.9996724 |\n",
      "|I watched this movie to see the direction one of the most promising young talents in movies was going                                                                                                                                                                      |POSITIVE|0.9994948 |\n",
      "|This movie makes you wish imdb would let you vote a zero                                                                                                                                                                                                                   |NEGATIVE|0.9981299 |\n",
      "|I never want to see this movie again!<br /><br />Not only is it dreadfully bad, but I can't stand seeing my hero Stan Laurel looking so old and sick                                                                                                                       |NEGATIVE|0.99883264|\n",
      "|(As a note, I'd like to say that I saw this movie at my annual church camp, where the entire youth group laughed at it                                                                                                                                                     |POSITIVE|0.9901753 |\n",
      "|Don't get me wrong, I love the TV series of League Of Gentlemen                                                                                                                                                                                                            |POSITIVE|0.99983096|\n",
      "|Did you ever think, like after watching a horror movie with a group of friends: \"Wow, this is so cool! We have got to make a splatter horror movie ourselves some day soon                                                                                                 |POSITIVE|0.9992768 |\n",
      "|Awful, awful, awful                                                                                                                                                                                                                                                        |NEGATIVE|0.9997433 |\n",
      "|This movie seems a little clunky around the edges, like not quite enough zaniness was thrown it when it should have been                                                                                                                                                   |NEGATIVE|0.9996525 |\n",
      "|I rented this movie hoping that it would provide some good entertainment and some cool poker knowledge or stories                                                                                                                                                          |NEGATIVE|0.99643254|\n",
      "|Well, where to start describing this celluloid debacle? You already know the big fat NADA passing as a plot, so let's jut point out that this is so PC it's offensive                                                                                                      |NEGATIVE|0.99973005|\n",
      "|I hoped for this show to be somewhat realistic                                                                                                                                                                                                                             |POSITIVE|0.8417903 |\n",
      "|All I have to say is one word                                                                                                                                                                                                                                              |NEGATIVE|0.97844803|\n",
      "|Honestly awful film, bad editing, awful lighting, dire dialog and scrappy screenplay                                                                                                                                                                                       |NEGATIVE|0.9997701 |\n",
      "|This critique tells the story of 4 little friends who went to watch Angels and Demons the movie on the first night it came out, even though it was a school night, because \"Angels and Demons is worth it                                                                  |POSITIVE|0.9942386 |\n",
      "|This review contains a partial spoiler                                                                                                                                                                                                                                     |NEGATIVE|0.99620205|\n",
      "|I'm rather surprised that anybody found this film touching or moving                                                                                                                                                                                                       |POSITIVE|0.83874947|\n",
      "|If you like bad movies (and you must to watch this one) here's a good one                                                                                                                                                                                                  |POSITIVE|0.9936475 |\n",
      "|This is really bad, the characters were bland, the story was boring, and there is no sex scene                                                                                                                                                                             |NEGATIVE|0.99953806|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197c146-1794-47f0-bcd9-7e8d8ab8625f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stop Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "425d3b28-7705-45ba-8a18-ad34fc895219",
   "metadata": {
    "tags": [
     "TRITON"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    print(\">>>> stopping containers: {}\".format([c.short_id for c in containers]))\n",
    "    if containers:\n",
    "        container=containers[0]\n",
    "        container.stop(timeout=120)\n",
    "\n",
    "    return [True]\n",
    "\n",
    "nodeRDD.barrier().mapPartitions(stop_triton).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f19643c-4ee4-44f2-b762-2078c0c8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a538c47-317d-4cac-b9b9-559e88677518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-dl-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
