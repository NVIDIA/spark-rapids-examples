{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777fc40d",
   "metadata": {},
   "source": [
    "# PySpark Huggingface Inferencing\n",
    "## Conditional generation with Tensorflow\n",
    "\n",
    "From: https://huggingface.co/docs/transformers/model_doc/t5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c79ac4-bf25-421e-b55e-020d6d9e15d5",
   "metadata": {},
   "source": [
    "### Using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f0dbf3-712b-4c58-85eb-261ce15bb2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rishic/anaconda3/envs/spark-dl-tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-25 17:37:28.748970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 17:37:28.769960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 17:37:28.775143: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 17:37:28.788944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 17:37:29.494783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFT5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275890d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2684fb41-9467-40c0-9d7e-a1cc867c5a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:37:34.350552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31135 MB memory:  -> device: 0, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:34:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.351970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31135 MB memory:  -> device: 1, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:36:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.353233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31135 MB memory:  -> device: 2, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:39:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.354461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 31135 MB memory:  -> device: 3, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.355676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 31135 MB memory:  -> device: 4, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:57:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.356984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 31135 MB memory:  -> device: 5, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:59:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.358203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 31135 MB memory:  -> device: 6, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5c:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.359411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 31135 MB memory:  -> device: 7, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5e:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.360687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:8 with 31135 MB memory:  -> device: 8, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b7:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.361913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:9 with 31135 MB memory:  -> device: 9, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b9:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.363133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:10 with 31135 MB memory:  -> device: 10, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:bc:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.364395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:11 with 31135 MB memory:  -> device: 11, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:be:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.365601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:12 with 31135 MB memory:  -> device: 12, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e0:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.366774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:13 with 31135 MB memory:  -> device: 13, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e2:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.367988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:14 with 31135 MB memory:  -> device: 14, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e5:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:34.369181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:15 with 31135 MB memory:  -> device: 15, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e7:00.0, compute capability: 7.0\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "task_prefix = \"translate English to German: \"\n",
    "\n",
    "lines = [\n",
    "    \"The house is wonderful\",\n",
    "    \"Welcome to NYC\",\n",
    "    \"HuggingFace is a company\"\n",
    "]\n",
    "\n",
    "input_sequences = [task_prefix + l for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb2dfdb-0ad3-4d0f-81a4-268d92c53759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rishic/anaconda3/envs/spark-dl-tf/lib/python3.11/site-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727285857.016900 2000247 service.cc:146] XLA service 0x7f8e980456a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727285857.016924 2000247 service.cc:154]   StreamExecutor device (0): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016927 2000247 service.cc:154]   StreamExecutor device (1): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016930 2000247 service.cc:154]   StreamExecutor device (2): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016932 2000247 service.cc:154]   StreamExecutor device (3): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016934 2000247 service.cc:154]   StreamExecutor device (4): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016936 2000247 service.cc:154]   StreamExecutor device (5): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016939 2000247 service.cc:154]   StreamExecutor device (6): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016941 2000247 service.cc:154]   StreamExecutor device (7): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016944 2000247 service.cc:154]   StreamExecutor device (8): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016946 2000247 service.cc:154]   StreamExecutor device (9): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016947 2000247 service.cc:154]   StreamExecutor device (10): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016949 2000247 service.cc:154]   StreamExecutor device (11): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016951 2000247 service.cc:154]   StreamExecutor device (12): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016954 2000247 service.cc:154]   StreamExecutor device (13): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016957 2000247 service.cc:154]   StreamExecutor device (14): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285857.016959 2000247 service.cc:154]   StreamExecutor device (15): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "2024-09-25 17:37:37.020733: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-25 17:37:37.057775: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-09-25 17:37:37.077344: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1727285857.095681 2000247 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_sequences, \n",
    "                      padding=\"longest\", \n",
    "                      max_length=max_source_length,\n",
    "                      return_tensors=\"tf\").input_ids\n",
    "outputs = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720158d4-e0e0-4904-b096-e5aede756afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Das Haus ist wunderbar',\n",
       " 'Willkommen in NYC',\n",
       " 'HuggingFace ist ein Unternehmen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(o, skip_special_tokens=True) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4b364b-13cb-48ea-a97a-ccfc9e408075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546eabe0",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f6db1f0-7d68-4af7-8bd6-c9fa45906c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68121304-f1df-466e-9347-c9d2b36a9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6279a849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "24/09/25 17:37:39 WARN Utils: Your hostname, dgx2h0194.spark.sjc4.nvmetal.net resolves to a loopback address: 127.0.1.1; using 10.150.30.2 instead (on interface enp134s0f0np0)\n",
      "24/09/25 17:37:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/25 17:37:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "num_threads = 6\n",
    "\n",
    "# Creating a local Spark session for demonstration, in case it hasn't already been created.\n",
    "\n",
    "_config = {\n",
    "    \"spark.master\": f\"local[{num_threads}]\",\n",
    "    \"spark.driver.host\": \"127.0.0.1\",\n",
    "    \"spark.task.maxFailures\": \"1\",\n",
    "    \"spark.driver.memory\": \"8g\",\n",
    "    \"spark.executor.memory\": \"8g\",\n",
    "    \"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\": \"false\",\n",
    "    \"spark.sql.pyspark.jvmStacktrace.enabled\": \"true\",\n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "    \"spark.python.worker.reuse\": \"true\",\n",
    "}\n",
    "spark = SparkSession.builder.appName(\"spark-dl-example\")\n",
    "for key, value in _config.items():\n",
    "    spark = spark.config(key, value)\n",
    "spark = spark.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8453111-d068-49bb-ab91-8ae3d8bcdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load IMDB reviews (test) dataset\n",
    "data = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad01d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = []\n",
    "for example in data:\n",
    "    lines.append([example[\"text\"].split(\".\")[0]])\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5b472-47e8-4804-9907-772793fedb2b",
   "metadata": {},
   "source": [
    "### Create PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24d9404-0269-476e-a9dd-1842667c915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('lines', StringType(), True)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(lines, ['lines']).repartition(10)\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4384c762-1f79-4f60-876c-94b1f552e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lines=\"i do not understand at all why this movie received such good grades from critics - - i've seen tens of documentaries (on TV) about the wine world which were much much better when (if) you watch it, please think of two very annoying aspects of mondovino : first, the filming is just awful and terrible and upsetting : to me, it looked like the guy behind the camera just received the material and was playing with it : plenty of zooms (for no purpose other than pushing the button in/out) for instance - - i almost stopped to watch it because of that ! secondly, the interviewer (the director i think) is not really relevant : he looks like and ask questions like a boy scout, not like a journalist, even if the general idea and themes would have been interesting, too bad conclusion: overrated documentary, maybe only for guys who do not know nothing about wine => not recommended at all (2/10)\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba3513-82dd-47e7-8193-eb4389458757",
   "metadata": {},
   "source": [
    "### Save the test dataset as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7eec8ec-4126-4890-b957-025809fad67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"imdb_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e1fc8-42a3-47dd-b3c0-47efd5be1040",
   "metadata": {},
   "source": [
    "### Check arrow memory configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20554ea5-01be-4a30-8607-db5d87786fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"512\")\n",
    "# This line will fail if the vectorized reader runs out of memory\n",
    "assert len(df.head()) > 0, \"`df` should not be empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4ecab-c9d9-466f-ba49-902ad1fd5488",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API (PyTorch)\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a00479-1347-4de8-8431-faa77f8cdf4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import col, pandas_udf, struct\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9a0889a-35b4-493a-8197-1146fc7efd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use first sentence and add prefix for conditional generation\n",
    "def preprocess(text: pd.Series, prefix: str = \"\") -> pd.Series:\n",
    "    @pandas_udf(\"string\")\n",
    "    def _preprocess(text: pd.Series) -> pd.Series:\n",
    "        return pd.Series([prefix + s.split(\".\")[0] for s in text])\n",
    "    return _preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c483e4d4-9ab1-416f-a766-694e17490fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                   lines|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|A ridiculous movie, a terrible editing job, worst screenplay, ridiculous acting, a story that is completely ununderst...|\n",
      "|                                                        Most of this film was okay, for a sequel of a sequel of a sequel|\n",
      "|                                                                                                                 I tried|\n",
      "|                                             This movie attempted to make Stu Ungar's life interesting by being creative|\n",
      "|After I saw this I concluded that it was most likely a chick flick; afterward I found out that Keira's mother wrote t...|\n",
      "|Jeff Speakman never really made it beyond the lowest ranks of martial-artists-turned-actors (lower than Don \"The Drag...|\n",
      "|I haven't seen this movie in years, the last time i did i was really drunk after 5 pints of tenant's at my local With...|\n",
      "|                                                                                Now don't get me wrong I love bad movies|\n",
      "|There I am sitting at home in the morning, suddenly my brother flips on what appears to be the stupidest looking movi...|\n",
      "|Yes, it was an awful movie, but there was a song near the beginning of the movie, I think, called \"I got a Woody\" or ...|\n",
      "|                                                        This was the most uninteresting horror flick I have seen to date|\n",
      "|Another in the long line of Conan wannabes that tired to cash in on that movie's success, this Italian monstrosity is...|\n",
      "|Oh, why did it have to end like this? Laurel and Hardy's last film, from the crudely cranked-up Cuckoo theme (with er...|\n",
      "|Julie Andrews satirically prods her own goody-two-shoes image in this overproduced musical comedy-drama, but if she a...|\n",
      "|                                                                                            'Leatherheads' tries so hard|\n",
      "|                                   If I wouldn't have had any expectations of this film, it might have received a 5 or 6|\n",
      "|With several name actors (Lance Henrikson, David Warner, Joe Don Baker), why was Jeffery Combs given the lead? Henrik...|\n",
      "|                                                                                                   Woa, talk about awful|\n",
      "|                                                                                      The guy mentioned to sue for the 1|\n",
      "|                                                                                           (Warning: Some spoilers ahead|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only use first N examples, since this is slow\n",
    "df = spark.read.parquet(\"imdb_test\").limit(100)\n",
    "df.show(truncate=120)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831bc52c-a5c6-4c29-a6da-0566b5167773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use first 100 rows, since generation takes a while\n",
    "df1 = df.withColumn(\"input\", preprocess(col(\"lines\"), \"Translate English to German: \")).select(\"input\").limit(100).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46dac59c-5a54-4576-91e0-279c8b375b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fef1d846-5852-4762-8527-602f32c0d7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                   input|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|Translate English to German: A ridiculous movie, a terrible editing job, worst screenplay, ridiculous acting, a story...|\n",
      "|                           Translate English to German: Most of this film was okay, for a sequel of a sequel of a sequel|\n",
      "|                                                                                    Translate English to German: I tried|\n",
      "|                Translate English to German: This movie attempted to make Stu Ungar's life interesting by being creative|\n",
      "|Translate English to German: After I saw this I concluded that it was most likely a chick flick; afterward I found ou...|\n",
      "|Translate English to German: Jeff Speakman never really made it beyond the lowest ranks of martial-artists-turned-act...|\n",
      "|Translate English to German: I haven't seen this movie in years, the last time i did i was really drunk after 5 pints...|\n",
      "|                                                   Translate English to German: Now don't get me wrong I love bad movies|\n",
      "|Translate English to German: There I am sitting at home in the morning, suddenly my brother flips on what appears to ...|\n",
      "|Translate English to German: Yes, it was an awful movie, but there was a song near the beginning of the movie, I thin...|\n",
      "|                           Translate English to German: This was the most uninteresting horror flick I have seen to date|\n",
      "|Translate English to German: Another in the long line of Conan wannabes that tired to cash in on that movie's success...|\n",
      "|Translate English to German: Oh, why did it have to end like this? Laurel and Hardy's last film, from the crudely cra...|\n",
      "|Translate English to German: Julie Andrews satirically prods her own goody-two-shoes image in this overproduced music...|\n",
      "|                                                               Translate English to German: 'Leatherheads' tries so hard|\n",
      "|      Translate English to German: If I wouldn't have had any expectations of this film, it might have received a 5 or 6|\n",
      "|Translate English to German: With several name actors (Lance Henrikson, David Warner, Joe Don Baker), why was Jeffery...|\n",
      "|                                                                      Translate English to German: Woa, talk about awful|\n",
      "|                                                         Translate English to German: The guy mentioned to sue for the 1|\n",
      "|                                                              Translate English to German: (Warning: Some spoilers ahead|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7ae69d3-70c2-4765-928f-c96a7ba59829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from transformers import TFT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "    # Enable GPU memory growth\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "    def predict(inputs):\n",
    "        flattened = np.squeeze(inputs).tolist()   # convert 2d numpy array of string into flattened python list\n",
    "        input_ids = tokenizer(flattened, \n",
    "                              padding=\"longest\", \n",
    "                              max_length=128,\n",
    "                              return_tensors=\"tf\").input_ids\n",
    "        output_ids = model.generate(input_ids)\n",
    "        string_outputs = np.array([tokenizer.decode(o, skip_special_tokens=True) for o in output_ids])\n",
    "        print(\"predict: {}\".format(len(flattened)))\n",
    "        return string_outputs\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36684f59-d947-43f8-a2e8-c7a423764e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=StringType(),\n",
    "                             batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a01c855-8fa1-4765-a3a5-2c9dd872df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:37:52.935639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 17:37:52.953270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 17:37:52.958829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 17:37:52.972056: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 17:37:53.657994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-25 17:37:58.208058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30291 MB memory:  -> device: 0, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:34:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.209551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30827 MB memory:  -> device: 1, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:36:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.210848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30827 MB memory:  -> device: 2, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:39:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.212170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30827 MB memory:  -> device: 3, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.213414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 30827 MB memory:  -> device: 4, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:57:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.214664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 30827 MB memory:  -> device: 5, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:59:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.215914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 30827 MB memory:  -> device: 6, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5c:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.217179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 30827 MB memory:  -> device: 7, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5e:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.218452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:8 with 30827 MB memory:  -> device: 8, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b7:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.219679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:9 with 30827 MB memory:  -> device: 9, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b9:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.220877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:10 with 30827 MB memory:  -> device: 10, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:bc:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.222146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:11 with 30827 MB memory:  -> device: 11, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:be:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.223342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:12 with 30827 MB memory:  -> device: 12, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e0:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.224546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:13 with 30827 MB memory:  -> device: 13, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e2:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.225682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:14 with 30827 MB memory:  -> device: 14, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e5:00.0, compute capability: 7.0\n",
      "2024-09-25 17:37:58.226880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:15 with 30827 MB memory:  -> device: 15, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e7:00.0, compute capability: 7.0\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "/rishic/anaconda3/envs/spark-dl-tf/lib/python3.11/site-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727285881.307966 2002067 service.cc:146] XLA service 0x7fadd4004c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727285881.308000 2002067 service.cc:154]   StreamExecutor device (0): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308006 2002067 service.cc:154]   StreamExecutor device (1): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308011 2002067 service.cc:154]   StreamExecutor device (2): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308015 2002067 service.cc:154]   StreamExecutor device (3): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308019 2002067 service.cc:154]   StreamExecutor device (4): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308023 2002067 service.cc:154]   StreamExecutor device (5): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308026 2002067 service.cc:154]   StreamExecutor device (6): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308032 2002067 service.cc:154]   StreamExecutor device (7): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308037 2002067 service.cc:154]   StreamExecutor device (8): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308042 2002067 service.cc:154]   StreamExecutor device (9): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308047 2002067 service.cc:154]   StreamExecutor device (10): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308052 2002067 service.cc:154]   StreamExecutor device (11): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308057 2002067 service.cc:154]   StreamExecutor device (12): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308062 2002067 service.cc:154]   StreamExecutor device (13): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308067 2002067 service.cc:154]   StreamExecutor device (14): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285881.308073 2002067 service.cc:154]   StreamExecutor device (15): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "2024-09-25 17:38:01.312220: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-25 17:38:01.354201: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-09-25 17:38:01.374634: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1727285881.392987 2002067 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 102 ms, sys: 126 ms, total: 228 ms\n",
      "Wall time: 29.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "preds = df1.withColumn(\"preds\", generate(struct(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d912d4b0-cd0b-44ea-859a-b23455cc2700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10                                                         (0 + 1) / 1]\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 ms, sys: 102 ms, total: 151 ms\n",
      "Wall time: 19.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", generate(\"input\"))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe3d88b-30f7-468f-8db8-1f4118d0f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10                                                         (0 + 1) / 1]\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.2 ms, sys: 103 ms, total: 163 ms\n",
      "Wall time: 20.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", generate(col(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ad9b365-4b9a-438e-8fdf-47da55cb1cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10                                                         (0 + 1) / 1]\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                                                       input|                                                       preds|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|Translate English to German: A ridiculous movie, a terrib...|Ein lächerlicher Film, eine schreckliche Bearbeitung, sch...|\n",
      "|Translate English to German: Most of this film was okay, ...|Der größte Teil dieses Films war okay, für eine Fortsetzu...|\n",
      "|                        Translate English to German: I tried|                   Ich habe versucht, Englisch zu übersetzen|\n",
      "|Translate English to German: This movie attempted to make...|Dieser Film versuchte, das Leben von Stu Ungar interessan...|\n",
      "|Translate English to German: After I saw this I concluded...|Nach meiner Anzeige kam ich zu dem Schluss, dass es höchs...|\n",
      "|Translate English to German: Jeff Speakman never really m...|Jeff Speakman hat es nie wirklich über die niedrigsten Kl...|\n",
      "|Translate English to German: I haven't seen this movie in...|Ich habe diesen Film nicht in Jahren gesehen, das letzte ...|\n",
      "|Translate English to German: Now don't get me wrong I lov...|  Jetzt mache ich mir nicht recht, ich liebe schlechte Filme|\n",
      "|Translate English to German: There I am sitting at home i...|   Dort sitze ich morgens zu Hause, plötzlich dreht mein Bru|\n",
      "|Translate English to German: Yes, it was an awful movie, ...|              Ja, es war ein schrecklicher Film, aber es gab|\n",
      "|Translate English to German: This was the most uninterest...|Dies war der größte Horrorfilm, den ich bisher gesehen habe.|\n",
      "|Translate English to German: Another in the long line of ...|      Ein weiterer in der langen Linie von Conan-Wünstlingen|\n",
      "|Translate English to German: Oh, why did it have to end l...|        Laurel und Hardy's letzter Film, von dem grotesken C|\n",
      "|Translate English to German: Julie Andrews satirically pr...|  Julie Andrews sah in diesem überproduzierten musikalischen|\n",
      "|   Translate English to German: 'Leatherheads' tries so hard|                                'Leatherheads' tries so hard|\n",
      "|Translate English to German: If I wouldn't have had any e...|Wenn ich keine Erwartungen an diesen Film hätte, hätte er...|\n",
      "|Translate English to German: With several name actors (La...|        Warum hat Jeffery Combs mit mehreren Namenssachen (L|\n",
      "|          Translate English to German: Woa, talk about awful|                         Woa, sprechen Sie über schreckliche|\n",
      "|Translate English to German: The guy mentioned to sue for...|                    Der Name hat es gesagt, daß er für die 1|\n",
      "|  Translate English to German: (Warning: Some spoilers ahead|              (Warning: Einige Vorschläge für die Übersetzer|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 121\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eb0c83b-d91b-4f8c-a5e7-c35f55c88108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use first 100 rows, since generation takes a while\n",
    "df2 = df.withColumn(\"input\", preprocess(col(\"lines\"), \"Translate English to French: \")).select(\"input\").limit(100).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "054f94fd-fe79-41e7-b1c7-6124083acc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                   input|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|Translate English to French: A ridiculous movie, a terrible editing job, worst screenplay, ridiculous acting, a story...|\n",
      "|                           Translate English to French: Most of this film was okay, for a sequel of a sequel of a sequel|\n",
      "|                                                                                    Translate English to French: I tried|\n",
      "|                Translate English to French: This movie attempted to make Stu Ungar's life interesting by being creative|\n",
      "|Translate English to French: After I saw this I concluded that it was most likely a chick flick; afterward I found ou...|\n",
      "|Translate English to French: Jeff Speakman never really made it beyond the lowest ranks of martial-artists-turned-act...|\n",
      "|Translate English to French: I haven't seen this movie in years, the last time i did i was really drunk after 5 pints...|\n",
      "|                                                   Translate English to French: Now don't get me wrong I love bad movies|\n",
      "|Translate English to French: There I am sitting at home in the morning, suddenly my brother flips on what appears to ...|\n",
      "|Translate English to French: Yes, it was an awful movie, but there was a song near the beginning of the movie, I thin...|\n",
      "|                           Translate English to French: This was the most uninteresting horror flick I have seen to date|\n",
      "|Translate English to French: Another in the long line of Conan wannabes that tired to cash in on that movie's success...|\n",
      "|Translate English to French: Oh, why did it have to end like this? Laurel and Hardy's last film, from the crudely cra...|\n",
      "|Translate English to French: Julie Andrews satirically prods her own goody-two-shoes image in this overproduced music...|\n",
      "|                                                               Translate English to French: 'Leatherheads' tries so hard|\n",
      "|      Translate English to French: If I wouldn't have had any expectations of this film, it might have received a 5 or 6|\n",
      "|Translate English to French: With several name actors (Lance Henrikson, David Warner, Joe Don Baker), why was Jeffery...|\n",
      "|                                                                      Translate English to French: Woa, talk about awful|\n",
      "|                                                         Translate English to French: The guy mentioned to sue for the 1|\n",
      "|                                                              Translate English to French: (Warning: Some spoilers ahead|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f6b70f9-188a-402b-9143-78a5788140e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:39:09.073889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 17:39:09.092523: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 17:39:09.097933: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 17:39:09.111018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 17:39:09.877488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-25 17:39:14.649415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30291 MB memory:  -> device: 0, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:34:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.650861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30827 MB memory:  -> device: 1, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:36:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.652156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30827 MB memory:  -> device: 2, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:39:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.653407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30827 MB memory:  -> device: 3, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.654660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 30827 MB memory:  -> device: 4, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:57:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.655938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 30827 MB memory:  -> device: 5, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:59:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.657175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 30827 MB memory:  -> device: 6, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5c:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.658321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 30827 MB memory:  -> device: 7, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:5e:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.659461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:8 with 30827 MB memory:  -> device: 8, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b7:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.660671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:9 with 30827 MB memory:  -> device: 9, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:b9:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.661824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:10 with 30827 MB memory:  -> device: 10, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:bc:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.663071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:11 with 30827 MB memory:  -> device: 11, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:be:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.664310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:12 with 30827 MB memory:  -> device: 12, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e0:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.665515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:13 with 30827 MB memory:  -> device: 13, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e2:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.666748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:14 with 30827 MB memory:  -> device: 14, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e5:00.0, compute capability: 7.0\n",
      "2024-09-25 17:39:14.667984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:15 with 30827 MB memory:  -> device: 15, name: Tesla V100-SXM3-32GB-H, pci bus id: 0000:e7:00.0, compute capability: 7.0\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "/rishic/anaconda3/envs/spark-dl-tf/lib/python3.11/site-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727285957.911404 2005620 service.cc:146] XLA service 0x7fadc0004560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727285957.911434 2005620 service.cc:154]   StreamExecutor device (0): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911437 2005620 service.cc:154]   StreamExecutor device (1): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911439 2005620 service.cc:154]   StreamExecutor device (2): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911441 2005620 service.cc:154]   StreamExecutor device (3): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911444 2005620 service.cc:154]   StreamExecutor device (4): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911446 2005620 service.cc:154]   StreamExecutor device (5): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911448 2005620 service.cc:154]   StreamExecutor device (6): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911450 2005620 service.cc:154]   StreamExecutor device (7): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911452 2005620 service.cc:154]   StreamExecutor device (8): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911454 2005620 service.cc:154]   StreamExecutor device (9): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911457 2005620 service.cc:154]   StreamExecutor device (10): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911459 2005620 service.cc:154]   StreamExecutor device (11): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911461 2005620 service.cc:154]   StreamExecutor device (12): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911463 2005620 service.cc:154]   StreamExecutor device (13): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911465 2005620 service.cc:154]   StreamExecutor device (14): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "I0000 00:00:1727285957.911467 2005620 service.cc:154]   StreamExecutor device (15): Tesla V100-SXM3-32GB-H, Compute Capability 7.0\n",
      "2024-09-25 17:39:17.915143: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-25 17:39:17.952069: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-09-25 17:39:17.973232: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1727285957.992033 2005620 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 119 ms, total: 237 ms\n",
      "Wall time: 30.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "preds = df2.withColumn(\"preds\", generate(struct(\"input\")))\n",
    "result = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "031a6a5e-7999-4653-b394-19ed478d8c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10                                                         (0 + 1) / 1]\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.7 ms, sys: 70.2 ms, total: 151 ms\n",
      "Wall time: 20.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df2.withColumn(\"preds\", generate(\"input\"))\n",
    "result = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "229b6515-82f6-4e9c-90f0-a9c3cfb26301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10                                                         (0 + 1) / 1]\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65 ms, sys: 80.8 ms, total: 146 ms\n",
      "Wall time: 20.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df2.withColumn(\"preds\", generate(col(\"input\")))\n",
    "result = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8be750ac-fa39-452e-bb4c-c2270bc2f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 10                                                         (0 + 1) / 1]\n",
      "predict: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                                                       input|                                                       preds|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|Translate English to French: A ridiculous movie, a terrib...|Un film ridicule, un terrible travail de rédaction, le pi...|\n",
      "|Translate English to French: Most of this film was okay, ...|La plupart de ce film était en bonne et due forme, pour u...|\n",
      "|                        Translate English to French: I tried|                                                 J'ai essayé|\n",
      "|Translate English to French: This movie attempted to make...|Ce film tentait de rendre la vie de Stu Ungar intéressant...|\n",
      "|Translate English to French: After I saw this I concluded...|Après avoir vu ce film, j'ai conclu qu'il était très prob...|\n",
      "|Translate English to French: Jeff Speakman never really m...|Jeff Speakman n'a jamais vraiment franchi les rangs les p...|\n",
      "|Translate English to French: I haven't seen this movie in...|Je n'ai pas vu ce film depuis des années, la dernière foi...|\n",
      "|Translate English to French: Now don't get me wrong I lov...|     Maintenant, ne pas me grecher Je aime les mauvais films|\n",
      "|Translate English to French: There I am sitting at home i...|       l'intérieur, je me trouve à la maison le matin, sous-|\n",
      "|Translate English to French: Yes, it was an awful movie, ...|  Oui, c'était un film terrible, mais il y avait une chanson|\n",
      "|Translate English to French: This was the most uninterest...|         Ce fut le plus inquiétant et le plus inquiétant d'h|\n",
      "|Translate English to French: Another in the long line of ...|  Une autre en longue ligne de conan qui a eu la fatigue de |\n",
      "|Translate English to French: Oh, why did it have to end l...|            Laurel et Hardy s'en sont rendus dans le dernier|\n",
      "|Translate English to French: Julie Andrews satirically pr...|Julie Andrews s'enrôle satiriquement dans cette comédie m...|\n",
      "|   Translate English to French: 'Leatherheads' tries so hard|                                'Leatherheads' essaie si dur|\n",
      "|Translate English to French: If I wouldn't have had any e...|                       Si je n'aurais pas eu d'attentes à ce|\n",
      "|Translate English to French: With several name actors (La...|Avec plusieurs acteurs de nom (Lance Henrikson, David War...|\n",
      "|          Translate English to French: Woa, talk about awful|                                                            |\n",
      "|Translate English to French: The guy mentioned to sue for...|Le châssis mentionné pour intenter une action en justice ...|\n",
      "|  Translate English to French: (Warning: Some spoilers ahead|               (Avertissement : Quelques spoilers à l'avenir|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 121\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabb2a8-3880-46ec-8e01-5a10f71fe83d",
   "metadata": {},
   "source": [
    "### Using Triton Inference Server\n",
    "\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment.  \n",
    "While the examples above use Tensorflow, note that inference on the Triton server is run using PyTorch. PyTorch will be included in the tarball below, and no further environment changes are required by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98fa52-7665-49bf-865a-feec86effe23",
   "metadata": {},
   "source": [
    "This notebook uses the [Python backend with a custom execution environment](https://github.com/triton-inference-server/python_backend#creating-custom-execution-environments) with the compatible versions of Python/Numpy for Triton 24.08, using a conda-pack environment created as follows:\n",
    "```\n",
    "conda create -n huggingface -c conda-forge python=3.10.0\n",
    "conda activate huggingface\n",
    "\n",
    "export PYTHONNOUSERSITE=True\n",
    "pip install numpy<2 conda-pack sentencepiece sentence_transformers transformers\n",
    "\n",
    "conda-pack  # huggingface.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b858cf85-82e6-41ef-905b-d8c5d6fea492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05ce7c77-d562-45e8-89bb-cd656aba5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# copy custom model to expected layout for Triton\n",
    "rm -rf models\n",
    "mkdir -p models\n",
    "cp -r models_config/hf_generation models\n",
    "\n",
    "# add custom execution environment\n",
    "cp huggingface.tar.gz models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552865c-5dad-4f25-8834-f41e253ac2f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afd00b7e-8150-4c95-a2e4-037e9c90f92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>> starting triton: 5e892cc5bbfe                                  (0 + 1) / 1]\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_executors = 1\n",
    "triton_models_dir = \"{}/models\".format(os.getcwd())\n",
    "huggingface_cache_dir = \"{}/.cache/huggingface\".format(os.path.expanduser('~'))\n",
    "nodeRDD = sc.parallelize(list(range(num_executors)), num_executors)\n",
    "\n",
    "def start_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    if containers:\n",
    "        print(\">>>> containers: {}\".format([c.short_id for c in containers]))\n",
    "    else:\n",
    "        container=client.containers.run(\n",
    "            \"nvcr.io/nvidia/tritonserver:24.08-py3\", \"tritonserver --model-repository=/models\",\n",
    "            detach=True,\n",
    "            device_requests=[docker.types.DeviceRequest(device_ids=[\"0\"], capabilities=[['gpu']])],\n",
    "            environment=[\n",
    "                \"TRANSFORMERS_CACHE=/cache\"\n",
    "            ],\n",
    "            name=\"spark-triton\",\n",
    "            network_mode=\"host\",\n",
    "            remove=True,\n",
    "            shm_size=\"1G\",\n",
    "            volumes={\n",
    "                triton_models_dir: {\"bind\": \"/models\", \"mode\": \"ro\"},\n",
    "                huggingface_cache_dir: {\"bind\": \"/cache\", \"mode\": \"rw\"}\n",
    "            }\n",
    "        )\n",
    "        print(\">>>> starting triton: {}\".format(container.short_id))\n",
    "\n",
    "        # wait for triton to be running\n",
    "        time.sleep(15)\n",
    "        client = grpcclient.InferenceServerClient(\"localhost:8001\")\n",
    "        ready = False\n",
    "        while not ready:\n",
    "            try:\n",
    "                ready = client.is_server_ready()\n",
    "            except Exception as e:\n",
    "                time.sleep(5)\n",
    "\n",
    "    return [True]\n",
    "\n",
    "nodeRDD.barrier().mapPartitions(start_triton).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d2df6-49fc-4be7-a534-a087dfe31c84",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a997c33-5202-466d-8304-b8c30f32978f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import col, pandas_udf, struct\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dea1875-6b95-4fc0-926d-a625a441b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use first N examples, since this is slow\n",
    "df = spark.read.parquet(\"imdb_test\").limit(100).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d6c54e7-534d-406f-b8e6-fd592efd0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use first sentence and add prefix for conditional generation\n",
    "def preprocess(text: pd.Series, prefix: str = \"\") -> pd.Series:\n",
    "    @pandas_udf(\"string\")\n",
    "    def _preprocess(text: pd.Series) -> pd.Series:\n",
    "        return pd.Series([prefix + s.split(\".\")[0] for s in text])\n",
    "    return _preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc1bbbe3-4232-49e5-80f6-99976524b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use first 100 rows, since generation takes a while\n",
    "df1 = df.withColumn(\"input\", preprocess(col(\"lines\"), \"Translate English to German: \")).select(\"input\").limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d10c61c-6102-4d19-8dd6-0c7b5b65343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                   input|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|Translate English to German: A ridiculous movie, a terrible editing job, worst screenplay, ridiculous acting, a story...|\n",
      "|                           Translate English to German: Most of this film was okay, for a sequel of a sequel of a sequel|\n",
      "|                                                                                    Translate English to German: I tried|\n",
      "|                Translate English to German: This movie attempted to make Stu Ungar's life interesting by being creative|\n",
      "|Translate English to German: After I saw this I concluded that it was most likely a chick flick; afterward I found ou...|\n",
      "|Translate English to German: Jeff Speakman never really made it beyond the lowest ranks of martial-artists-turned-act...|\n",
      "|Translate English to German: I haven't seen this movie in years, the last time i did i was really drunk after 5 pints...|\n",
      "|                                                   Translate English to German: Now don't get me wrong I love bad movies|\n",
      "|Translate English to German: There I am sitting at home in the morning, suddenly my brother flips on what appears to ...|\n",
      "|Translate English to German: Yes, it was an awful movie, but there was a song near the beginning of the movie, I thin...|\n",
      "|                           Translate English to German: This was the most uninteresting horror flick I have seen to date|\n",
      "|Translate English to German: Another in the long line of Conan wannabes that tired to cash in on that movie's success...|\n",
      "|Translate English to German: Oh, why did it have to end like this? Laurel and Hardy's last film, from the crudely cra...|\n",
      "|Translate English to German: Julie Andrews satirically prods her own goody-two-shoes image in this overproduced music...|\n",
      "|                                                               Translate English to German: 'Leatherheads' tries so hard|\n",
      "|      Translate English to German: If I wouldn't have had any expectations of this film, it might have received a 5 or 6|\n",
      "|Translate English to German: With several name actors (Lance Henrikson, David Warner, Joe Don Baker), why was Jeffery...|\n",
      "|                                                                      Translate English to German: Woa, talk about awful|\n",
      "|                                                         Translate English to German: The guy mentioned to sue for the 1|\n",
      "|                                                              Translate English to German: (Warning: Some spoilers ahead|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e0907da-a5d9-4c3b-9db4-ce5e70ca9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fn(triton_uri, model_name):\n",
    "    import numpy as np\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    np_types = {\n",
    "      \"BOOL\": np.dtype(np.bool_),\n",
    "      \"INT8\": np.dtype(np.int8),\n",
    "      \"INT16\": np.dtype(np.int16),\n",
    "      \"INT32\": np.dtype(np.int32),\n",
    "      \"INT64\": np.dtype(np.int64),\n",
    "      \"FP16\": np.dtype(np.float16),\n",
    "      \"FP32\": np.dtype(np.float32),\n",
    "      \"FP64\": np.dtype(np.float64),\n",
    "      \"FP64\": np.dtype(np.double),\n",
    "      \"BYTES\": np.dtype(object)\n",
    "    }\n",
    "\n",
    "    client = grpcclient.InferenceServerClient(triton_uri)\n",
    "    model_meta = client.get_model_metadata(model_name)\n",
    "    \n",
    "    def predict(inputs):\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            # single ndarray input\n",
    "            request = [grpcclient.InferInput(model_meta.inputs[0].name, inputs.shape, model_meta.inputs[0].datatype)]\n",
    "            request[0].set_data_from_numpy(inputs.astype(np_types[model_meta.inputs[0].datatype]))\n",
    "        else:\n",
    "            # dict of multiple ndarray inputs\n",
    "            request = [grpcclient.InferInput(i.name, inputs[i.name].shape, i.datatype) for i in model_meta.inputs]\n",
    "            for i in request:\n",
    "                i.set_data_from_numpy(inputs[i.name()].astype(np_types[i.datatype()]))\n",
    "        \n",
    "        response = client.infer(model_name, inputs=request)\n",
    "        \n",
    "        if len(model_meta.outputs) > 1:\n",
    "            # return dictionary of numpy arrays\n",
    "            return {o.name: response.as_numpy(o.name) for o in model_meta.outputs}\n",
    "        else:\n",
    "            # return single numpy array\n",
    "            return response.as_numpy(model_meta.outputs[0].name)\n",
    "        \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9308bdd7-6f67-484d-8b51-dd1e1b2960ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = predict_batch_udf(partial(triton_fn, triton_uri=\"localhost:8001\", model_name=\"hf_generation\"),\n",
    "                             return_type=StringType(),\n",
    "                             input_tensor_shapes=[[1]],\n",
    "                             batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38484ffd-370d-492b-8ca4-9eff9f242a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1999686/3110230631.py:6: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 ms, sys: 14.1 ms, total: 26.1 ms\n",
      "Wall time: 2.62 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "preds = df1.withColumn(\"preds\", generate(struct(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebcb6699-3ac2-4529-ab0f-fab0a5e792da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 ms, sys: 6.19 ms, total: 18.8 ms\n",
      "Wall time: 1.93 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", generate(\"input\"))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2ed18ad-d00b-472c-b2c3-047932f2105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 6.23 ms, total: 18.2 ms\n",
      "Wall time: 1.93 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", generate(col(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cd64a1c-beb8-47d5-ac6f-e8525bb61176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                                                       input|                                                       preds|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|Translate English to German: A ridiculous movie, a terrib...|Ein lächerlicher Film, eine schreckliche Bearbeitung, sch...|\n",
      "|Translate English to German: Most of this film was okay, ...|Der größte Teil dieses Films war okay, für eine Fortsetzu...|\n",
      "|                        Translate English to German: I tried|                   Ich habe versucht, Englisch zu übersetzen|\n",
      "|Translate English to German: This movie attempted to make...|Dieser Film versuchte, das Leben von Stu Ungar interessan...|\n",
      "|Translate English to German: After I saw this I concluded...|Nach meiner Anzeige kam ich zu dem Schluss, dass es höchs...|\n",
      "|Translate English to German: Jeff Speakman never really m...|Jeff Speakman hat es nie wirklich über die niedrigsten Kl...|\n",
      "|Translate English to German: I haven't seen this movie in...|Ich habe diesen Film nicht in Jahren gesehen, das letzte ...|\n",
      "|Translate English to German: Now don't get me wrong I lov...|  Jetzt mache ich mir nicht recht, ich liebe schlechte Filme|\n",
      "|Translate English to German: There I am sitting at home i...|   Dort sitze ich morgens zu Hause, plötzlich dreht mein Bru|\n",
      "|Translate English to German: Yes, it was an awful movie, ...|              Ja, es war ein schrecklicher Film, aber es gab|\n",
      "|Translate English to German: This was the most uninterest...|Dies war der größte Horrorfilm, den ich bisher gesehen habe.|\n",
      "|Translate English to German: Another in the long line of ...|      Ein weiterer in der langen Linie von Conan-Wünstlingen|\n",
      "|Translate English to German: Oh, why did it have to end l...|        Laurel und Hardy's letzter Film, von dem grotesken C|\n",
      "|Translate English to German: Julie Andrews satirically pr...|  Julie Andrews sah in diesem überproduzierten musikalischen|\n",
      "|   Translate English to German: 'Leatherheads' tries so hard|                                'Leatherheads' tries so hard|\n",
      "|Translate English to German: If I wouldn't have had any e...|Wenn ich keine Erwartungen an diesen Film hätte, hätte er...|\n",
      "|Translate English to German: With several name actors (La...|        Warum hat Jeffery Combs mit mehreren Namenssachen (L|\n",
      "|          Translate English to German: Woa, talk about awful|                         Woa, sprechen Sie über schreckliche|\n",
      "|Translate English to German: The guy mentioned to sue for...|                    Der Name hat es gesagt, daß er für die 1|\n",
      "|  Translate English to German: (Warning: Some spoilers ahead|              (Warning: Einige Vorschläge für die Übersetzer|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af70fed8-0f2b-4ea7-841c-476afdf9b1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 17:41:32 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# only use first 100 rows, since generation takes a while\n",
    "df2 = df.withColumn(\"input\", preprocess(col(\"lines\"), \"Translate English to French: \")).select(\"input\").limit(100).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef075e10-e22c-4236-9e0b-cb47cf2d3d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                   input|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|Translate English to French: A ridiculous movie, a terrible editing job, worst screenplay, ridiculous acting, a story...|\n",
      "|                           Translate English to French: Most of this film was okay, for a sequel of a sequel of a sequel|\n",
      "|                                                                                    Translate English to French: I tried|\n",
      "|                Translate English to French: This movie attempted to make Stu Ungar's life interesting by being creative|\n",
      "|Translate English to French: After I saw this I concluded that it was most likely a chick flick; afterward I found ou...|\n",
      "|Translate English to French: Jeff Speakman never really made it beyond the lowest ranks of martial-artists-turned-act...|\n",
      "|Translate English to French: I haven't seen this movie in years, the last time i did i was really drunk after 5 pints...|\n",
      "|                                                   Translate English to French: Now don't get me wrong I love bad movies|\n",
      "|Translate English to French: There I am sitting at home in the morning, suddenly my brother flips on what appears to ...|\n",
      "|Translate English to French: Yes, it was an awful movie, but there was a song near the beginning of the movie, I thin...|\n",
      "|                           Translate English to French: This was the most uninteresting horror flick I have seen to date|\n",
      "|Translate English to French: Another in the long line of Conan wannabes that tired to cash in on that movie's success...|\n",
      "|Translate English to French: Oh, why did it have to end like this? Laurel and Hardy's last film, from the crudely cra...|\n",
      "|Translate English to French: Julie Andrews satirically prods her own goody-two-shoes image in this overproduced music...|\n",
      "|                                                               Translate English to French: 'Leatherheads' tries so hard|\n",
      "|      Translate English to French: If I wouldn't have had any expectations of this film, it might have received a 5 or 6|\n",
      "|Translate English to French: With several name actors (Lance Henrikson, David Warner, Joe Don Baker), why was Jeffery...|\n",
      "|                                                                      Translate English to French: Woa, talk about awful|\n",
      "|                                                         Translate English to French: The guy mentioned to sue for the 1|\n",
      "|                                                              Translate English to French: (Warning: Some spoilers ahead|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e7e4af8-b815-4375-b851-8368309ee8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1999686/3110230631.py:6: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 226 ms, sys: 86.9 ms, total: 313 ms\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df2.withColumn(\"preds\", generate(struct(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b0aefb0-a96b-4791-a23c-1ce9b24eb20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 9.74 ms, total: 21.2 ms\n",
      "Wall time: 2.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df2.withColumn(\"preds\", generate(\"input\"))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1214b75b-a373-4579-b4c6-0cb8627da776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 ms, sys: 5 ms, total: 19 ms\n",
      "Wall time: 2.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df2.withColumn(\"preds\", generate(col(\"input\")))\n",
    "results = preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9dbd21f-9e37-4221-b765-80ba8c80b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                                                       input|                                                       preds|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|Translate English to French: A ridiculous movie, a terrib...|Un film ridicule, un terrible travail de rédaction, le pi...|\n",
      "|Translate English to French: Most of this film was okay, ...|La plupart de ce film était en bonne et due forme, pour u...|\n",
      "|                        Translate English to French: I tried|                                                 J'ai essayé|\n",
      "|Translate English to French: This movie attempted to make...|Ce film tentait de rendre la vie de Stu Ungar intéressant...|\n",
      "|Translate English to French: After I saw this I concluded...|Après avoir vu ce film, j'ai conclu qu'il était très prob...|\n",
      "|Translate English to French: Jeff Speakman never really m...|Jeff Speakman n'a jamais vraiment franchi les rangs les p...|\n",
      "|Translate English to French: I haven't seen this movie in...|Je n'ai pas vu ce film depuis des années, la dernière foi...|\n",
      "|Translate English to French: Now don't get me wrong I lov...|     Maintenant, ne pas me grecher Je aime les mauvais films|\n",
      "|Translate English to French: There I am sitting at home i...|       l'intérieur, je me trouve à la maison le matin, sous-|\n",
      "|Translate English to French: Yes, it was an awful movie, ...|  Oui, c'était un film terrible, mais il y avait une chanson|\n",
      "|Translate English to French: This was the most uninterest...|         Ce fut le plus inquiétant et le plus inquiétant d'h|\n",
      "|Translate English to French: Another in the long line of ...|   Une autre en longue ligne de conan qui a eu la fatigue de|\n",
      "|Translate English to French: Oh, why did it have to end l...|            Laurel et Hardy s'en sont rendus dans le dernier|\n",
      "|Translate English to French: Julie Andrews satirically pr...|Julie Andrews s'enrôle satiriquement dans cette comédie m...|\n",
      "|   Translate English to French: 'Leatherheads' tries so hard|                                'Leatherheads' essaie si dur|\n",
      "|Translate English to French: If I wouldn't have had any e...|                       Si je n'aurais pas eu d'attentes à ce|\n",
      "|Translate English to French: With several name actors (La...|Avec plusieurs acteurs de nom (Lance Henrikson, David War...|\n",
      "|          Translate English to French: Woa, talk about awful|                                                            |\n",
      "|Translate English to French: The guy mentioned to sue for...|Le châssis mentionné pour intenter une action en justice ...|\n",
      "|  Translate English to French: (Warning: Some spoilers ahead|               (Avertissement : Quelques spoilers à l'avenir|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e3113-64dd-482a-9233-6607b3f63c1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stop Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "425d3b28-7705-45ba-8a18-ad34fc895219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>> stopping containers: ['5e892cc5bbfe']\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    print(\">>>> stopping containers: {}\".format([c.short_id for c in containers]))\n",
    "    if containers:\n",
    "        container=containers[0]\n",
    "        container.stop(timeout=120)\n",
    "\n",
    "    return [True]\n",
    "\n",
    "nodeRDD.barrier().mapPartitions(stop_triton).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2dec80ca-7a7c-46a9-97c0-7afb1572f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43118ab-fc0a-4f64-a126-4302e615654a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
