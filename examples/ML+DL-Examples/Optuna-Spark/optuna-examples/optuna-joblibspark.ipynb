{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright (c) 2024, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_efficientnet/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "# Distributed Hyperparameter Tuning: Optuna + JoblibSpark\n",
    "\n",
    "\n",
    "This demo demonstrates distributed hyperparameter tuning for XGBoost using the [JoblibSpark backend](https://github.com/joblib/joblib-spark), building on this [example from Databricks](https://docs.databricks.com/en/machine-learning/automl-hyperparam-tuning/optuna.html).  \n",
    "We implement best practices to precompute data and maximize computations on the GPU.  \n",
    "\n",
    "\n",
    "\n",
    "Reference: https://forecastegy.com/posts/xgboost-hyperparameter-tuning-with-optuna/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Before running, please make sure you've followed the relevant [setup instructions](../README.md) for your environment (standalone or databricks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import joblib\n",
    "from joblibspark import register_spark\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import xgboost as xgb\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import TaskContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "We'll use the [red wine quality dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv) to regress wine quality based on features such as acidity, sugar content, etc.  \n",
    "\n",
    "**Note**: This example uses a small dataset for demonstration purposes. The performance advantages of distributed training are best realized with large datasets and computational workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "os.mkdir(os.path.join(cwd, \"data\")) if not os.path.exists(os.path.join(cwd, \"data\")) else None\n",
    "filepath = os.path.join(cwd, \"data\", \"winequality-red.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"File downloaded and saved to {filepath}\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Running Optuna locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.metrics.regression import mean_squared_error\n",
    "from cuml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cudf.read_csv(filepath, delimiter=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the train/validation sets. Precompute the Quantile DMatrix, which is used by histogram-based tree methods to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data[\"quality\"].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Xy_train_qdm = xgb.QuantileDMatrix(X_train, y_train)  # Precompute Quantile DMatrix to avoid repeated quantization every trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "We define the objective and a hyperparameter search space to optimize via the `trial.suggest_` methods.  \n",
    "\n",
    "In each trial, new hyperparameters will be suggested based on previous results. See [optuna.trial.Trial](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html) API for a full list of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"verbosity\": 0,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        \"max_bins\": 64,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"device\": \"cuda\",\n",
    "    }\n",
    "\n",
    "    booster = xgb.train(params, dtrain=Xy_train_qdm, num_boost_round=1000)\n",
    "    predictions = booster.inplace_predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, predictions, squared=False).get()\n",
    "    \n",
    "    return rmse   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the study and optimize. By default, the study results will be stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 19:47:02,101] A new study created in memory with name: optuna-xgboost-local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 19:47:03,955] Trial 0 finished with value: 0.5448661551754318 and parameters: {'learning_rate': 0.005611516415334507, 'max_depth': 10, 'subsample': 0.7453942447208348, 'colsample_bytree': 0.6187255599871848, 'min_child_weight': 4}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:04,191] Trial 1 finished with value: 0.6762149201768457 and parameters: {'learning_rate': 0.002051110418843397, 'max_depth': 1, 'subsample': 0.8728673384861885, 'colsample_bytree': 0.6210592611560484, 'min_child_weight': 15}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:05,920] Trial 2 finished with value: 0.6871436852105118 and parameters: {'learning_rate': 0.0010994335574766201, 'max_depth': 10, 'subsample': 0.8408205087604007, 'colsample_bytree': 0.25172215514436236, 'min_child_weight': 4}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:06,505] Trial 3 finished with value: 0.6096809835807359 and parameters: {'learning_rate': 0.002327067708383781, 'max_depth': 4, 'subsample': 0.548518610050626, 'colsample_bytree': 0.46034776771000996, 'min_child_weight': 6}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:06,849] Trial 4 finished with value: 0.6060284135590066 and parameters: {'learning_rate': 0.01673808578875214, 'max_depth': 2, 'subsample': 0.3275374161084572, 'colsample_bytree': 0.3980437511290071, 'min_child_weight': 10}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:07,196] Trial 5 finished with value: 0.6041777615275192 and parameters: {'learning_rate': 0.037183641805732096, 'max_depth': 2, 'subsample': 0.538522716492931, 'colsample_bytree': 0.6127938404189404, 'min_child_weight': 1}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:07,554] Trial 6 finished with value: 0.6012987372324395 and parameters: {'learning_rate': 0.016409286730647923, 'max_depth': 2, 'subsample': 0.11179901333601554, 'colsample_bytree': 0.9514412603906666, 'min_child_weight': 20}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:08,117] Trial 7 finished with value: 0.5881042953989125 and parameters: {'learning_rate': 0.041380401125610165, 'max_depth': 4, 'subsample': 0.14278850830606465, 'colsample_bytree': 0.700021375186549, 'min_child_weight': 9}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:08,800] Trial 8 finished with value: 0.6243828841244896 and parameters: {'learning_rate': 0.0017541893487450805, 'max_depth': 5, 'subsample': 0.08266909505945748, 'colsample_bytree': 0.9138543819748429, 'min_child_weight': 6}. Best is trial 0 with value: 0.5448661551754318.\n",
      "[I 2024-12-06 19:47:09,376] Trial 9 finished with value: 0.5769714938263446 and parameters: {'learning_rate': 0.02113705944064573, 'max_depth': 4, 'subsample': 0.5440646201189203, 'colsample_bytree': 0.5693747653761156, 'min_child_weight': 4}. Best is trial 0 with value: 0.5448661551754318.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"optuna-xgboost-local\", sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE:  0.5448661551754318\n",
      "Best hyperparameters:  {'learning_rate': 0.005611516415334507, 'max_depth': 10, 'subsample': 0.7453942447208348, 'colsample_bytree': 0.6187255599871848, 'min_child_weight': 4}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print(\"Best RMSE: \", trial.value)\n",
    "print(\"Best hyperparameters: \", trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Distributed Optuna on Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Task\n",
    "\n",
    "Define the task UDF to distribute across the Spark cluster. In each task, the worker will:\n",
    "1. Read the dataset from the specified filepath.\n",
    "2. Load the study from the MySQL storage backend.\n",
    "3. Optimize over the objective for the assigned number of trials, sending results back to the database after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(num_trials: int, driver_ip: str, study_name: str, seed: int, filepath: str):\n",
    "    import cudf\n",
    "    from cuml.metrics.regression import mean_squared_error\n",
    "    from cuml.model_selection import train_test_split\n",
    "\n",
    "    def get_gpu_id(task_context: TaskContext) -> int:\n",
    "        if task_context is None:\n",
    "            raise RuntimeError(\"_get_gpu_id should not be invoked from driver side.\")\n",
    "        resources = task_context.resources()\n",
    "\n",
    "        if \"gpu\" not in resources:\n",
    "            raise RuntimeError(\n",
    "                \"Couldn't get the gpu id, Please check the GPU resource configuration\"\n",
    "            )\n",
    "        \n",
    "        return int(resources[\"gpu\"].addresses[0].strip())  # Return the first GPU ID for multi-GPU setups.\n",
    "\n",
    "    tc = TaskContext.get()\n",
    "    gpu_id = get_gpu_id(tc)\n",
    "\n",
    "    if filepath.startswith(\"/dbfs/\"):\n",
    "        # Check to ensure GPU direct storage is disabled for cuDF on databricks.\n",
    "        libcudf_policy = os.environ.get('LIBCUDF_CUFILE_POLICY')\n",
    "        if libcudf_policy != 'OFF':\n",
    "            raise RuntimeError(\"Set LIBCUDF_CUFILE_POLICY=OFF to read from DBFS with cuDF.\")\n",
    "    \n",
    "    data = cudf.read_csv(filepath, delimiter=\";\")\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data[\"quality\"].values\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    Xy_train_qdm = xgb.QuantileDMatrix(X_train, y_train)  # Precompute Quantile DMatrix to avoid repeated quantization every trial.\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"verbosity\": 0,\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "            \"max_bins\": trial.suggest_int(\"max_bins\", 32, 256),\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            \"device\": f\"cuda:{gpu_id}\",\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "\n",
    "        if \"max_bins\" in params:\n",
    "            # If tuning the max_bins param, we must recompute the QDM every trial.\n",
    "            if \"num_estimators\" not in params:\n",
    "                params[\"num_estimators\"] = 1000\n",
    "\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            booster = model.get_booster()\n",
    "        else:\n",
    "            # Train the model with xgb.train() API using the precomputed QDM.\n",
    "            booster = xgb.train(params, dtrain=Xy_train_qdm, num_boost_round=1000)\n",
    "            \n",
    "        # Perform in-place predictions on GPU using the booster.\n",
    "        predictions = booster.inplace_predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, predictions, squared=False).get()\n",
    "        \n",
    "        return rmse\n",
    "\n",
    "    study = optuna.load_study(\n",
    "        study_name=study_name,\n",
    "        storage=f\"mysql://optuna_user:optuna_password@{driver_ip}/optuna\",\n",
    "        sampler=TPESampler(seed=seed),\n",
    "    )\n",
    "\n",
    "    print(f\"Running {num_trials} trials on partition {tc.partitionId()}.\")\n",
    "    study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "    return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark\n",
    "\n",
    "For standalone users, we need to create the Spark session with the Spark-Rapids plugin. For Databricks users, the Spark session will be preconfigured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "def initialize_spark():\n",
    "    import socket\n",
    "    hostname = socket.gethostname()\n",
    "    conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster(f\"spark://{hostname}:7077\")  # Assuming master is on host and default port. \n",
    "    conf.set(\"spark.task.maxFailures\", \"1\")\n",
    "    conf.set(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    conf.set(\"spark.pyspark.python\", f\"{conda_env}/bin/python\")\n",
    "    conf.set(\"spark.pyspark.driver.python\", f\"{conda_env}/bin/python\")\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"optuna-joblibspark-xgboost\").config(conf=conf).getOrCreate()\n",
    "    return spark\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will register the Spark Session with the Joblib Spark backend.\n",
    "register_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and run the Optuna study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the driver IP for the MySQL database.  \n",
    "- For standalone users, make sure you've followed the [database setup instructions](../README.md#setup-database-for-optuna). The database should be on 'localhost'. \n",
    "- For databricks users, the database should already be setup on the driver node by the init script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we're running on databricks\n",
    "on_databricks = os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL database is hosted on localhost\n"
     ]
    }
   ],
   "source": [
    "if on_databricks:\n",
    "    driver_ip = spark.conf.get(\"spark.driver.host\")\n",
    "else:\n",
    "    driver_ip = \"localhost\"\n",
    "\n",
    "print(f\"MySQL database is hosted on {driver_ip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new study, referencing the MySQL database as the storage backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 19:47:11,323] A new study created in RDB with name: optuna-xgboost-joblibspark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<optuna.study.study.Study at 0x7f649a650940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_name = \"optuna-xgboost-joblibspark\"\n",
    "seed = 42\n",
    "\n",
    "try:\n",
    "    # Delete the study if it already exists\n",
    "    optuna.delete_study(\n",
    "        study_name=study_name, \n",
    "        storage=f\"mysql://optuna_user:optuna_password@{driver_ip}/optuna\"\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=f\"mysql://optuna_user:optuna_password@{driver_ip}/optuna\",\n",
    "    sampler=TPESampler(seed=seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of tasks, number of trials, and trials per task. \n",
    "\n",
    "**NOTE**: for standalone users running on a single worker, the 4 tasks will all be assigned to the same worker and executed sequentially in this demonstration. This can easily be scaled up by adding more workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_trials(total_trials: int, total_tasks: int) -> List[int]:\n",
    "    base_size = total_trials // total_tasks\n",
    "    extra = total_trials % total_tasks\n",
    "    partitions = [base_size] * total_tasks\n",
    "    for i in range(extra):\n",
    "        partitions[i] += 1\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials per task: [25, 25, 25, 25]\n"
     ]
    }
   ],
   "source": [
    "num_tasks = 4\n",
    "num_trials = 100\n",
    "trials_per_task = partition_trials(num_trials, num_tasks)\n",
    "print(f\"Trials per task: {trials_per_task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Databricks**: we must download the dataset to DBFS so that all workers can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_databricks:\n",
    "    dbutils.fs.mkdirs(\"/FileStore/optuna-data\")\n",
    "    filepath = \"/dbfs/FileStore/optuna-data/winequality-red.csv\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File downloaded and saved to {filepath}\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend(\"spark\", n_jobs=num_tasks):\n",
    "    results = joblib.Parallel()(\n",
    "        joblib.delayed(task)(num_trials, driver_ip, study_name, seed, filepath) for num_trials in trials_per_task\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.06337865756914637, 'max_depth': 10, 'subsample': 0.8315729907914456, 'colsample_bytree': 0.885700420269793, 'min_child_weight': 18, 'max_bins': 72}\n",
      "Best value: 0.5529384630337998\n"
     ]
    }
   ],
   "source": [
    "best_params = min(results, key=lambda x: x[1])[0]\n",
    "best_value = min(results, key=lambda x: x[1])[1]\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best value: {best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optuna-spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
