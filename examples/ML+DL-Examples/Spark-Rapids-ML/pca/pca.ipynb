{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "In this notebook, we will demonstrate the end-to-end workflow of Spark RAPIDS accelerated PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spark rapids jar\n",
      "File 'rapids-4-spark_2.12-24.08.1.jar' downloaded and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "### Download Spark Rapids jar ###\n",
    "\n",
    "SPARK_RAPIDS_VERSION = \"24.08.1\"\n",
    "rapids_jar = f\"rapids-4-spark_2.12-{SPARK_RAPIDS_VERSION}.jar\"\n",
    "\n",
    "if not os.path.exists(rapids_jar):\n",
    "    print(\"Downloading spark rapids jar\")\n",
    "    url = f\"https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/{SPARK_RAPIDS_VERSION}/{rapids_jar}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(rapids_jar, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File '{rapids_jar}' downloaded and saved successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure Spark Session ###\n",
    "conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(f\"spark://{hostname}:7077\") # Set to your hostname\n",
    "conf.set(\"spark.task.maxFailures\", \"1\")\n",
    "conf.set(\"spark.driver.memory\", \"10g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "conf.set(\"spark.rapids.ml.uvm.enabled\", \"true\")\n",
    "conf.set(\"spark.jars\", rapids_jar)\n",
    "conf.set(\"spark.executorEnv.PYTHONPATH\", rapids_jar)\n",
    "conf.set(\"spark.rapids.memory.gpu.minAllocFraction\", \"0.0001\")\n",
    "conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "conf.set(\"spark.locality.wait\", \"0s\")\n",
    "conf.set(\"spark.sql.cache.serializer\", \"com.nvidia.spark.ParquetCachedBatchSerializer\")\n",
    "conf.set(\"spark.rapids.memory.gpu.pooling.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.execution.sortBeforeRepartition\", \"false\")\n",
    "conf.set(\"spark.rapids.sql.format.parquet.reader.type\", \"MULTITHREADED\")\n",
    "conf.set(\"spark.rapids.sql.format.parquet.multiThreadedRead.maxNumFilesParallel\", \"20\")\n",
    "conf.set(\"spark.rapids.sql.multiThreadedRead.numThreads\", \"20\")\n",
    "conf.set(\"spark.rapids.sql.python.gpu.enabled\", \"true\")\n",
    "conf.set(\"spark.rapids.memory.pinnedPool.size\", \"2G\")\n",
    "conf.set(\"spark.python.daemon.module\", \"rapids.daemon\")\n",
    "conf.set(\"spark.rapids.sql.batchSizeBytes\", \"512m\")\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\n",
    "conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"1\")\n",
    "conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"20000\")\n",
    "conf.set(\"spark.rapids.sql.explain\", \"NONE\")\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"spark-rapids-ml-pca\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic dataset\n",
    "\n",
    "Here we generate a 100,000 x 2048 random dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 23:19:53 WARN TaskSetManager: Stage 0 contains a task of very large size (160085 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows = 100000\n",
    "dim = 2048\n",
    "dtype = 'float32'\n",
    "np.random.seed(42)\n",
    "\n",
    "data = np.random.rand(rows, dim).astype(dtype)\n",
    "pd_data = pd.DataFrame({\"features\": list(data)})\n",
    "prepare_df = spark.createDataFrame(pd_data)\n",
    "prepare_df.write.mode(\"overwrite\").parquet(\"PCA_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark-RAPIDS-ML accepts ArrayType input\n",
    "\n",
    "Note that in the original Spark-ML PCA, we must `Vectorize` the input column:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "    (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "    (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data,[\"features\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "...whereas the Spark-RAPIDS-ML version does not require extra Vectorization, and can accept an ArrayType column as the input column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = spark.read.parquet(\"PCA_data.parquet\")\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark-RAPIDS-ML PCA (GPU)\n",
    "\n",
    "Compared to the Spark-ML PCA training API:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=3, inputCol=\"features\")\n",
    "pca.setOutputCol(\"pca_features\")\n",
    "```\n",
    "\n",
    "We use a customized class which requires **no code change** from the user to enjoy GPU acceleration:\n",
    "\n",
    "```python\n",
    "from spark_rapids_ml.feature import PCA\n",
    "pca = PCA(k=3, inputCol=\"features\")\n",
    "pca.setOutputCol(\"pca_features\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_bcd33d128594"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spark_rapids_ml.feature import PCA\n",
    "\n",
    "gpu_pca = PCA(k=2, inputCol=\"features\")\n",
    "gpu_pca.setOutputCol(\"pca_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA estimator object can be persisted and reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_path = \"/tmp/pca_estimator\"\n",
    "gpu_pca.write().overwrite().save(estimator_path)\n",
    "gpu_pca_loaded = PCA.load(estimator_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 23:20:00,131 - spark_rapids_ml.feature.PCA - INFO - CUDA managed memory enabled.\n",
      "2024-10-03 23:20:00,239 - spark_rapids_ml.feature.PCA - INFO - Stage-level scheduling in spark-rapids-ml requires spark.executor.cores, spark.executor.resource.gpu.amount to be set.\n",
      "2024-10-03 23:20:00,241 - spark_rapids_ml.feature.PCA - INFO - Training spark-rapids-ml with 1 worker(s) ...\n",
      "2024-10-03 23:20:08,190 - spark_rapids_ml.feature.PCA - INFO - Finished training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU PCA fit took: 8.758668899536133 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gpu_pca_model = gpu_pca_loaded.fit(data_df)\n",
    "gpu_fit_time = time.time() - start_time\n",
    "print(f\"GPU PCA fit took: {gpu_fit_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|pca_features               |\n",
      "+---------------------------+\n",
      "|[-0.24846071, 0.33562037]  |\n",
      "|[0.5184792, 0.48330337]    |\n",
      "|[0.2507918, 0.3815673]     |\n",
      "|[0.39100257, 0.4842953]    |\n",
      "|[0.4037514, 0.70158374]    |\n",
      "|[0.30750397, 0.5324805]    |\n",
      "|[0.6082078, 0.5151396]     |\n",
      "|[0.21961018, 0.64743024]   |\n",
      "|[-0.1901558, 0.63220304]   |\n",
      "|[-0.61287963, 0.5951108]   |\n",
      "|[0.027350709, -0.03707385] |\n",
      "|[0.29946682, -0.05652547]  |\n",
      "|[0.54797435, -0.198609]    |\n",
      "|[0.6652416, 0.10023773]    |\n",
      "|[0.12782758, 0.46697623]   |\n",
      "|[0.43612525, -0.0074159503]|\n",
      "|[-0.62129164, 0.54278356]  |\n",
      "|[-0.048607834, 0.7038538]  |\n",
      "|[-0.6254531, 0.35484123]   |\n",
      "|[-0.16294907, 0.7283848]   |\n",
      "+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "GPU PCA transform took: 0.4192674160003662 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "embeddings = gpu_pca_model.transform(data_df).select(\"pca_features\").show(truncate=False)\n",
    "gpu_transform_time = time.time() - start_time\n",
    "print(f\"GPU PCA transform took: {gpu_transform_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark-ML PCA (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_791eff82b929"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "cpu_pca = PCA(k=2, inputCol=\"features\")\n",
    "cpu_pca.setOutputCol(\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "vector_df = data_df.select(array_to_vector(\"features\").alias(\"features\"))\n",
    "vector_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 23:20:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU PCA fit took: 64.01513171195984 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cpu_pca_model = cpu_pca.fit(vector_df)\n",
    "pca_fit_time = time.time() - start_time\n",
    "print(f\"CPU PCA fit took: {pca_fit_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|pca_features                               |\n",
      "+-------------------------------------------+\n",
      "|[0.24926765828229927,0.3425432972889563]   |\n",
      "|[-0.5175207040808384,0.48893065865444574]  |\n",
      "|[-0.2505049373829902,0.381272141155778]    |\n",
      "|[-0.39046980420292005,0.4870705091697811]  |\n",
      "|[-0.4024088726395023,0.707133448810984]    |\n",
      "|[-0.3061227832285992,0.5363554872099332]   |\n",
      "|[-0.6065136982526093,0.5205197626985932]   |\n",
      "|[-0.21870566838630084,0.6516598402789231]  |\n",
      "|[0.1910036552854184,0.6336513389989592]    |\n",
      "|[0.6139537641786907,0.6055187085018856]    |\n",
      "|[-0.026502904776425647,-0.0366087508156753]|\n",
      "|[-0.2989311781309336,-0.05136110567458389] |\n",
      "|[-0.5474468086054212,-0.18779964958125014] |\n",
      "|[-0.6644746232216499,0.10351178251944647]  |\n",
      "|[-0.12685301272617464,0.47394431583661295] |\n",
      "|[-0.4355221246718862,-0.00346289187881239] |\n",
      "|[0.6222719258951077,0.5488293416698503]    |\n",
      "|[0.04966907735703511,0.7138677407505005]   |\n",
      "|[0.6260486995906139,0.3553228450428632]    |\n",
      "|[0.16396683091519929,0.7382693234881972]   |\n",
      "+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU PCA transform took: 0.19607114791870117 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "embeddings = cpu_pca_model.transform(vector_df).select(\"pca_features\").show(truncate=False)\n",
    "pca_transform_time = time.time() - start_time\n",
    "print(f\"CPU PCA transform took: {pca_transform_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU runtime: (64.02s + 0.20s)\n",
      "GPU runtime: (8.76s + 0.42s)\n",
      "End-to-end speedup: CPU / GPU = 7.00x\n"
     ]
    }
   ],
   "source": [
    "speedup = (pca_fit_time + pca_transform_time) / (gpu_fit_time + gpu_transform_time)\n",
    "print(f\"CPU runtime: ({pca_fit_time:.2f}s + {pca_transform_time:.2f}s)\")\n",
    "print(f\"GPU runtime: ({gpu_fit_time:.2f}s + {gpu_transform_time:.2f}s)\")\n",
    "print(f\"End-to-end speedup: CPU / GPU = {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
