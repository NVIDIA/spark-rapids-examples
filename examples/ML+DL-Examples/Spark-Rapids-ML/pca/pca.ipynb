{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "In this notebook, we will demonstrate the end-to-end workflow of Spark RAPIDS accelerated PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "### Download Spark Rapids jar ###\n",
    "\n",
    "SPARK_RAPIDS_VERSION = \"24.08.1\"\n",
    "rapids_jar = f\"rapids-4-spark_2.12-{SPARK_RAPIDS_VERSION}.jar\"\n",
    "\n",
    "if not os.path.exists(rapids_jar):\n",
    "    print(\"Downloading spark rapids jar\")\n",
    "    url = f\"https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/{SPARK_RAPIDS_VERSION}/{rapids_jar}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(rapids_jar, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File '{rapids_jar}' downloaded and saved successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure Spark Session ###\n",
    "conda_env = os.environ.get(\"CONDA_PREFIX\")\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(f\"spark://{hostname}:7077\") # Set to your hostname\n",
    "conf.set(\"spark.task.maxFailures\", \"1\")\n",
    "conf.set(\"spark.driver.memory\", \"10g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.python.worker.reuse\", \"true\")\n",
    "conf.set(\"spark.rapids.ml.uvm.enabled\", \"true\")\n",
    "conf.set(\"spark.jars\", rapids_jar)\n",
    "conf.set(\"spark.executorEnv.PYTHONPATH\", rapids_jar)\n",
    "conf.set(\"spark.rapids.memory.gpu.minAllocFraction\", \"0.0001\")\n",
    "conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "conf.set(\"spark.locality.wait\", \"0s\")\n",
    "conf.set(\"spark.sql.cache.serializer\", \"com.nvidia.spark.ParquetCachedBatchSerializer\")\n",
    "conf.set(\"spark.rapids.memory.gpu.pooling.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.execution.sortBeforeRepartition\", \"false\")\n",
    "conf.set(\"spark.rapids.sql.format.parquet.reader.type\", \"MULTITHREADED\")\n",
    "conf.set(\"spark.rapids.sql.format.parquet.multiThreadedRead.maxNumFilesParallel\", \"20\")\n",
    "conf.set(\"spark.rapids.sql.multiThreadedRead.numThreads\", \"20\")\n",
    "conf.set(\"spark.rapids.sql.python.gpu.enabled\", \"true\")\n",
    "conf.set(\"spark.rapids.memory.pinnedPool.size\", \"2G\")\n",
    "conf.set(\"spark.python.daemon.module\", \"rapids.daemon\")\n",
    "conf.set(\"spark.rapids.sql.batchSizeBytes\", \"512m\")\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\n",
    "conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"1\")\n",
    "conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"20000\")\n",
    "conf.set(\"spark.rapids.sql.explain\", \"NONE\")\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"spark-rapids-ml-pca\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic dataset\n",
    "\n",
    "Here we generate a 100,000 x 2048 random dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 18:15:12 WARN TaskSetManager: Stage 0 contains a task of very large size (160085 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows = 100000\n",
    "dim = 2048\n",
    "dtype = 'float32'\n",
    "np.random.seed(42)\n",
    "\n",
    "data = np.random.rand(rows, dim).astype(dtype)\n",
    "pd_data = pd.DataFrame({\"features\": list(data)})\n",
    "prepare_df = spark.createDataFrame(pd_data)\n",
    "prepare_df.write.mode(\"overwrite\").parquet(\"PCA_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"PCA_data.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL: Mean-centering\n",
    "\n",
    "PCA expects mean-centered data as input so that the first principal component is not influenced by the distribution mean. We perform a simple mean centering on the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 18:15:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/10/03 18:15:21 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/03 18:15:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_values = df.select([\n",
    "    F.avg(F.col(\"features\")[i]).alias(f\"avg_{i}\") for i in range(dim)\n",
    "]).first()\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def mean_center_udf(features: pd.Series) -> pd.Series:\n",
    "    return features.apply(lambda row: [row[i] - avg_values[i] for i in range(dim)])\n",
    "\n",
    "mean_centered_df = df.withColumn(\"mean_centered_features\", mean_center_udf(F.col(\"features\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark-RAPIDS-ML accepts ArrayType input\n",
    "\n",
    "Note that in the original Spark-ML PCA, we must `Vectorize` the input column:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "    (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "    (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data,[\"features\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "...whereas the Spark-RAPIDS-ML version does not require extra Vectorization, and can accept an ArrayType column as the input column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = mean_centered_df.withColumn(\"features\", F.col(\"mean_centered_features\")).drop(\"mean_centered_features\")\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark-RAPIDS-ML PCA (GPU)\n",
    "\n",
    "Compared to the Spark-ML PCA training API:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=3, inputCol=\"features\")\n",
    "pca.setOutputCol(\"pca_features\")\n",
    "```\n",
    "\n",
    "We use a customized class which requires **no code change** from the user to enjoy GPU acceleration:\n",
    "\n",
    "```python\n",
    "from spark_rapids_ml.feature import PCA\n",
    "pca = PCA(k=3, inputCol=\"features\")\n",
    "pca.setOutputCol(\"pca_features\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_167b33961a13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spark_rapids_ml.feature import PCA\n",
    "\n",
    "gpu_pca = PCA(k=2, inputCol=\"features\")\n",
    "gpu_pca.setOutputCol(\"pca_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA estimator object can be persisted and reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_path = \"/tmp/pca_estimator\"\n",
    "gpu_pca.write().overwrite().save(estimator_path)\n",
    "gpu_pca_loaded = PCA.load(estimator_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 18:15:31,372 - spark_rapids_ml.feature.PCA - INFO - CUDA managed memory enabled.\n",
      "2024-10-03 18:15:31,423 - spark_rapids_ml.feature.PCA - INFO - Stage-level scheduling in spark-rapids-ml requires spark.executor.cores, spark.executor.resource.gpu.amount to be set.\n",
      "2024-10-03 18:15:31,425 - spark_rapids_ml.feature.PCA - INFO - Training spark-rapids-ml with 1 worker(s) ...\n",
      "2024-10-03 18:16:03,110 - spark_rapids_ml.feature.PCA - INFO - Finished training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU PCA fit took: 32.50137519836426 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gpu_pca_model = gpu_pca_loaded.fit(data_df)\n",
    "gpu_fit_time = time.time() - start_time\n",
    "print(f\"GPU PCA fit took: {gpu_fit_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|pca_features              |\n",
      "+--------------------------+\n",
      "|[-0.029416187, 0.14954807]|\n",
      "|[-0.114759326, 0.30470988]|\n",
      "|[0.24565856, -0.3830186]  |\n",
      "|[0.40122557, 0.0786071]   |\n",
      "|[0.33858502, -0.3383386]  |\n",
      "|[-0.4234191, 0.054718923] |\n",
      "|[0.31339574, -0.18767774] |\n",
      "|[0.48100916, -0.13139157] |\n",
      "|[0.24663548, 0.62084264]  |\n",
      "|[-0.7007258, 0.41795364]  |\n",
      "|[-0.3402629, 0.118103035] |\n",
      "|[0.050888825, -0.13529032]|\n",
      "|[0.22439958, -0.2205292]  |\n",
      "|[0.25716788, -0.03613429] |\n",
      "|[0.6055516, -0.44179356]  |\n",
      "|[-0.2515555, -0.1829353]  |\n",
      "|[-0.2190136, -0.48459405] |\n",
      "|[-0.28191802, 0.005161534]|\n",
      "|[-0.32060724, -0.52684677]|\n",
      "|[0.10207409, -0.07858773] |\n",
      "+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "GPU PCA transform took: 1.7161929607391357 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "embeddings = gpu_pca_model.transform(data_df).select(\"pca_features\").show(truncate=False)\n",
    "gpu_transform_time = time.time() - start_time\n",
    "print(f\"GPU PCA transform took: {gpu_transform_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark-ML PCA (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_cde1243ffb2d"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "cpu_pca = PCA(k=2, inputCol=\"features\")\n",
    "cpu_pca.setOutputCol(\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "vector_df = data_df.select(array_to_vector(\"features\").alias(\"features\"))\n",
    "vector_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 18:18:39 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU PCA fit took: 168.66824460029602 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cpu_pca_model = cpu_pca.fit(vector_df)\n",
    "pca_fit_time = time.time() - start_time\n",
    "print(f\"CPU PCA fit took: {pca_fit_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|pca_features                                |\n",
      "+--------------------------------------------+\n",
      "|[-0.028913990912549873,-0.14877800281975417]|\n",
      "|[-0.11377219195114611,-0.30286035088028784] |\n",
      "|[0.24481776731139782,0.38339521202540466]   |\n",
      "|[0.4012645935474968,-0.0774854638371508]    |\n",
      "|[0.3382728256029673,0.3389068302551277]     |\n",
      "|[-0.4228363394060016,-0.05490137931031454]  |\n",
      "|[0.3133387034642869,0.18913735472308166]    |\n",
      "|[0.4810119397199172,0.1325004050655491]     |\n",
      "|[0.24748029515381828,-0.6211263064522211]   |\n",
      "|[-0.6999917660681412,-0.420784987276893]    |\n",
      "|[-0.34046347604937044,-0.11978179300566327] |\n",
      "|[0.05074039845250796,0.13683028045753456]   |\n",
      "|[0.22282065118768452,0.22023244883076878]   |\n",
      "|[0.2562068262436395,0.03528786064789906]    |\n",
      "|[0.6045398358884778,0.44301892614623417]    |\n",
      "|[-0.25204946003221423,0.18266864414577164]  |\n",
      "|[-0.22000096004134898,0.4838697920777026]   |\n",
      "|[-0.28225973295047585,-0.006133424943195989]|\n",
      "|[-0.32151621576351685,0.5263178254530007]   |\n",
      "|[0.10169730752796506,0.07813949653526481]   |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU PCA transform took: 0.292694091796875 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "embeddings = cpu_pca_model.transform(vector_df).select(\"pca_features\").show(truncate=False)\n",
    "pca_transform_time = time.time() - start_time\n",
    "print(f\"CPU PCA transform took: {pca_transform_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "With our 100,000 x 2048 dataset, we achieved end-to-end speedup of  \n",
    "\n",
    "CPU: (173.7s + 0.50s)  \n",
    "GPU: (32.5s + 1.71s)  \n",
    "\n",
    "`CPU / GPU = 5.1x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
