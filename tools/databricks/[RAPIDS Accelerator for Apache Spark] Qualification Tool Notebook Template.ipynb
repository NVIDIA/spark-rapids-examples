{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"df33c614-2ecc-47a0-8600-bc891681997f\",\n",
    "     \"showTitle\": false,\n",
    "     \"title\": \"\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Welcome to the Qualification Tool for the RAPIDS Accelerator for Apache Spark\\n\",\n",
    "    \"\\n\",\n",
    "    \"To run the qualification tool, enter the log path that represents the DBFS location of your Spark GPU event logs. Then, select \\\"Run all\\\" to execute the notebook. Once the notebook completes, various output tables will appear below. For more options on running the profiling tool, please refer to the [Qualification Tool User Guide](https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/quickstart.html#running-the-tool).\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Note\\n\",\n",
    "    \"- Currently, local, S3 or DBFS event log paths are supported.\\n\",\n",
    "    \"- S3 path is only supported on Databricks AWS using [instance profiles](https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html).\\n\",\n",
    "    \"- Eventlog path must follow the formats `/dbfs/path/to/eventlog` or `dbfs:/path/to/eventlog` for logs stored in DBFS.\\n\",\n",
    "    \"- Use wildcards for nested lookup of eventlogs. \\n\",\n",
    "    \"   - For example: `/dbfs/path/to/clusterlogs/*/*`\\n\",\n",
    "    \"- Multiple event logs must be comma-separated. \\n\",\n",
    "    \"   - For example: `/dbfs/path/to/eventlog1,/dbfs/path/to/eventlog2`\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"5e9f5796-46ed-49ac-9d08-c8b98a87c39d\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Set Tools Version\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"DEFAULT_TOOLS_VER = \\\"24.12.4\\\"\\n\",\n",
    "    \"TOOLS_VER_ARG = dbutils.widgets.get(\\\"Tools Version\\\")\\n\",\n",
    "    \"TOOLS_VER = TOOLS_VER_ARG if TOOLS_VER_ARG else DEFAULT_TOOLS_VER\\n\",\n",
    "    \"print(f\\\"Using Tools Version: {TOOLS_VER}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"313ee58b-61b3-4010-9d60-d21eceea796c\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Install Package\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"%pip install spark-rapids-user-tools==$TOOLS_VER > /dev/null\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"acf401a3-12d3-4236-a6c5-8fe8990b153a\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Environment Setup\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def convert_dbfs_path(path):\\n\",\n",
    "    \"    return path.replace(\\\"dbfs:/\\\", \\\"/dbfs/\\\")\\n\",\n",
    "    \"  \\n\",\n",
    "    \"# Detect cloud provider from cluster usage tags\\n\",\n",
    "    \"valid_csps = [\\\"aws\\\", \\\"azure\\\"]\\n\",\n",
    "    \"CSP=spark.conf.get(\\\"spark.databricks.clusterUsageTags.cloudProvider\\\", \\\"\\\").lower()\\n\",\n",
    "    \"if CSP not in valid_csps:\\n\",\n",
    "    \"    print(f\\\"ERROR: Cannot detect cloud provider from cluster usage tags. Using '{valid_csps[0]}' as default. \\\")\\n\",\n",
    "    \"    CSP = valid_csps[0]\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Detected Cloud Provider from Spark Configs: '{CSP}'\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize variables from widgets\\n\",\n",
    "    \"dbutils.widgets.text(\\\"Eventlog Path\\\", \\\"/dbfs/user1/qualification_logs\\\")\\n\",\n",
    "    \"EVENTLOG_PATH=dbutils.widgets.get(\\\"Eventlog Path\\\")\\n\",\n",
    "    \"EVENTLOG_PATH=convert_dbfs_path(EVENTLOG_PATH)\\n\",\n",
    "    \"\\n\",\n",
    "    \"dbutils.widgets.text(\\\"Output Path\\\", \\\"/tmp\\\")\\n\",\n",
    "    \"OUTPUT_PATH=dbutils.widgets.get(\\\"Output Path\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"  \\n\",\n",
    "    \"# Setup environment variables\\n\",\n",
    "    \"os.environ[\\\"CSP\\\"] = CSP\\n\",\n",
    "    \"os.environ[\\\"EVENTLOG_PATH\\\"] = EVENTLOG_PATH\\n\",\n",
    "    \"os.environ[\\\"OUTPUT_PATH\\\"] = OUTPUT_PATH\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Setup console output file\\n\",\n",
    "    \"CONSOLE_OUTPUT_PATH = os.path.join(OUTPUT_PATH, 'console_output.log')\\n\",\n",
    "    \"CONSOLE_ERROR_PATH = os.path.join(OUTPUT_PATH, 'console_error.log')\\n\",\n",
    "    \"os.environ['CONSOLE_OUTPUT_PATH'] = CONSOLE_OUTPUT_PATH\\n\",\n",
    "    \"os.environ['CONSOLE_ERROR_PATH'] = CONSOLE_ERROR_PATH\\n\",\n",
    "    \"print(f'Console output will be stored at {CONSOLE_OUTPUT_PATH} and errors will be stored at {CONSOLE_ERROR_PATH}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"693b5ee0-7500-43f3-b3e2-717fd5468aa8\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Run Qualification Tool\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"%sh\\n\",\n",
    "    \"spark_rapids qualification --platform databricks-$CSP --eventlogs \\\"$EVENTLOG_PATH\\\" -o \\\"$OUTPUT_PATH\\\" --verbose > \\\"$CONSOLE_OUTPUT_PATH\\\" 2> \\\"$CONSOLE_ERROR_PATH\\\"\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"f83af6c8-5a79-4a46-965b-38a4cb621877\",\n",
    "     \"showTitle\": false,\n",
    "     \"title\": \"\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Console Output\\n\",\n",
    "    \"Console output shows the top candidates and their estimated GPU speedup.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"c61527b7-a21a-492c-bab8-77f83dc5cabf\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Show Console Output\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"%sh\\n\",\n",
    "    \"cat $CONSOLE_OUTPUT_PATH\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"f3c68b28-fc62-40ae-8528-799f3fc7507e\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Show Logs\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"%sh\\n\",\n",
    "    \"cat $CONSOLE_ERROR_PATH\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"05f96ca1-1b08-494c-a12b-7e6cc3dcc546\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Parse Output\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import re\\n\",\n",
    "    \"import shutil\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"def extract_file_info(console_output_path, output_base_path):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        with open(console_output_path, 'r') as file:\\n\",\n",
    "    \"            stdout_text = file.read()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Extract log file location\\n\",\n",
    "    \"        location_match = re.search(r\\\"Location: (.+)\\\", stdout_text)\\n\",\n",
    "    \"        if not location_match:\\n\",\n",
    "    \"            raise ValueError(\\\"Log file location not found in the provided text.\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        log_file_location = location_match.group(1)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Extract qualification output folder\\n\",\n",
    "    \"        qual_match = re.search(r\\\"qual_[^/]+(?=\\\\.log)\\\", log_file_location)\\n\",\n",
    "    \"        if not qual_match:\\n\",\n",
    "    \"            raise ValueError(\\\"Output folder not found in the log file location.\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        output_folder_name = qual_match.group(0)\\n\",\n",
    "    \"        output_folder = os.path.join(output_base_path, output_folder_name)\\n\",\n",
    "    \"        return output_folder, log_file_location\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        raise RuntimeError(f\\\"Cannot parse console output. Reason: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"def copy_logs(destination_folder, *log_files):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        log_folder = os.path.join(destination_folder, \\\"logs\\\")\\n\",\n",
    "    \"        os.makedirs(log_folder, exist_ok=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for log_file in log_files:\\n\",\n",
    "    \"            if os.path.exists(log_file):\\n\",\n",
    "    \"                shutil.copy2(log_file, log_folder)\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                print(f\\\"Log file not found: {log_file}\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        raise RuntimeError(f\\\"Cannot copy logs to output. Reason: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    output_folder, log_file_location = extract_file_info(CONSOLE_OUTPUT_PATH, OUTPUT_PATH)\\n\",\n",
    "    \"    jar_output_folder = os.path.join(output_folder, \\\"rapids_4_spark_qualification_output\\\")\\n\",\n",
    "    \"    print(f\\\"Output folder detected {output_folder}\\\")\\n\",\n",
    "    \"    copy_logs(output_folder, log_file_location, CONSOLE_OUTPUT_PATH, CONSOLE_ERROR_PATH)\\n\",\n",
    "    \"    print(f\\\"Logs successfully copied to {output_folder}\\\")\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(e)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"8c65adcd-a933-482e-a50b-d40fa8f50e16\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"Download Output\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import shutil\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"\\n\",\n",
    "    \"current_working_directory = os.getcwd()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def create_destination_folders(folder_name):\\n\",\n",
    "    \"    os.makedirs(folder_name, exist_ok=True)\\n\",\n",
    "    \"    base_download_folder_path = os.path.join(\\\"/dbfs/FileStore/\\\", folder_name)\\n\",\n",
    "    \"    os.makedirs(base_download_folder_path, exist_ok=True) \\n\",\n",
    "    \"    return base_download_folder_path\\n\",\n",
    "    \"\\n\",\n",
    "    \"def create_download_link(source_folder, destination_folder_name):\\n\",\n",
    "    \"    folder_to_compress = os.path.basename(source_folder)\\n\",\n",
    "    \"    zip_file_name = folder_to_compress + '.zip'\\n\",\n",
    "    \"    local_zip_file_path = os.path.join(current_working_directory, destination_folder_name, zip_file_name)\\n\",\n",
    "    \"    download_folder_path = os.path.join(destination_folder_name, zip_file_name)\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        base_download_folder_path = create_destination_folders(destination_folder_name)\\n\",\n",
    "    \"        shutil.make_archive(folder_to_compress, 'zip', source_folder)\\n\",\n",
    "    \"        shutil.copy2(zip_file_name, base_download_folder_path)\\n\",\n",
    "    \"        if os.path.exists(local_zip_file_path):\\n\",\n",
    "    \"            os.remove(local_zip_file_path)\\n\",\n",
    "    \"        shutil.move(zip_file_name, local_zip_file_path)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"        download_button_html = f\\\"\\\"\\\"\\n\",\n",
    "    \"        <style>\\n\",\n",
    "    \"            .download-btn {{\\n\",\n",
    "    \"                display: inline-block;\\n\",\n",
    "    \"                padding: 10px 20px;\\n\",\n",
    "    \"                font-size: 16px;\\n\",\n",
    "    \"                color: white;\\n\",\n",
    "    \"                background-color: #4CAF50;\\n\",\n",
    "    \"                text-align: center;\\n\",\n",
    "    \"                text-decoration: none;\\n\",\n",
    "    \"                border-radius: 5px;\\n\",\n",
    "    \"                border: none;\\n\",\n",
    "    \"                cursor: pointer;\\n\",\n",
    "    \"                margin: 15px auto;\\n\",\n",
    "    \"            }}\\n\",\n",
    "    \"            .download-btn:hover {{\\n\",\n",
    "    \"                background-color: #45a049;\\n\",\n",
    "    \"            }}\\n\",\n",
    "    \"            .button-container {{\\n\",\n",
    "    \"                display: flex;\\n\",\n",
    "    \"                justify-content: center;\\n\",\n",
    "    \"                align-items: center;\\n\",\n",
    "    \"            }}\\n\",\n",
    "    \"        </style>\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        <div style=\\\"color: #444; font-size: 14px; text-align: center; margin: 10px;\\\">\\n\",\n",
    "    \"            Zipped output file created at {local_zip_file_path}\\n\",\n",
    "    \"        </div>\\n\",\n",
    "    \"        <div class='button-container'>\\n\",\n",
    "    \"            <a href='/files/{download_folder_path}' class='download-btn'>Download Output</a>\\n\",\n",
    "    \"        </div>\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        displayHTML(download_button_html)\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        error_message_html = f\\\"\\\"\\\"\\n\",\n",
    "    \"        <div style=\\\"color: red; text-align: center; margin: 20px;\\\">\\n\",\n",
    "    \"            <strong>Error:</strong> Cannot create download link for {source_folder}. Reason: {e}\\n\",\n",
    "    \"        </div>\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        displayHTML(error_message_html)\\n\",\n",
    "    \"\\n\",\n",
    "    \"destination_folder_name = \\\"Tools_Output\\\"\\n\",\n",
    "    \"create_download_link(output_folder, destination_folder_name)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"bbe50fde-0bd6-4281-95fd-6a1ec6f17ab2\",\n",
    "     \"showTitle\": false,\n",
    "     \"title\": \"\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"## Summary Output\\n\",\n",
    "    \"\\n\",\n",
    "    \"The report provides a comprehensive overview of the entire application execution, estimated speedup, including unsupported operators and non-SQL operations. By default, the applications and queries are sorted in descending order based on the following fields:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Estimated GPU Speedup Category\\n\",\n",
    "    \"- Estimated GPU Speedup\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"b8bca4a6-16d8-4b60-ba7b-9aff64bdcaa1\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"qualification_summary.csv\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"summary_output=pd.read_csv(os.path.join(output_folder, \\\"qualification_summary.csv\\\"))\\n\",\n",
    "    \"summary_output=summary_output.drop(columns=[\\\"Unnamed: 0\\\"]).rename_axis('Index').reset_index()\\n\",\n",
    "    \"display(summary_output)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {},\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"73b5e0b0-3a96-4cc6-8e6c-840e4b0d9d43\",\n",
    "     \"showTitle\": false,\n",
    "     \"title\": \"\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"## Application Status\\n\",\n",
    "    \"\\n\",\n",
    "    \"The report show the status of each eventlog file that was provided\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"c9ffbfdb-dbb6-4736-b9cb-2ac457cc6714\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"rapids_4_spark_qualification_output_status.csv\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"status_output=pd.read_csv(os.path.join(jar_output_folder, \\\"rapids_4_spark_qualification_output_status.csv\\\"))\\n\",\n",
    "    \"display(status_output)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {},\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"09945d39-f9c2-4f4a-8afd-4f309f24f8e0\",\n",
    "     \"showTitle\": false,\n",
    "     \"title\": \"\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"## Metadata for Migration\\n\",\n",
    "    \"\\n\",\n",
    "    \"The report show the metadata of each app as:\\n\",\n",
    "    \"- Recommended GPU cluster\\n\",\n",
    "    \"- File location of full cluster config recommendations\\n\",\n",
    "    \"- File location of only Gpu specific config recommendations\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"133cf1bd-33b6-4a62-9ae2-5505717092d1\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"app_metadata.json\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"metadata_file = os.path.join(output_folder, \\\"app_metadata.json\\\")\\n\",\n",
    "    \"def camel_to_title(name):\\n\",\n",
    "    \"    return re.sub('([a-z])([A-Z])', r'\\\\1 \\\\2', name).title()\\n\",\n",
    "    \"  \\n\",\n",
    "    \"with open(metadata_file, 'r') as file:\\n\",\n",
    "    \"    json_data = json.load(file)\\n\",\n",
    "    \"\\n\",\n",
    "    \"df = pd.DataFrame(json_data)\\n\",\n",
    "    \"df['recommendedGpuCluster'] = df['clusterInfo'].apply(lambda x: x['recommendedCluster'])\\n\",\n",
    "    \"df['sourceCluster'] = df['clusterInfo'].apply(lambda x: x['sourceCluster'])\\n\",\n",
    "    \"df.drop(columns=['clusterInfo'], inplace=True)\\n\",\n",
    "    \"df = df[['appId', 'appName', 'estimatedGpuSpeedupCategory', 'recommendedGpuCluster', 'fullClusterConfigRecommendations', 'gpuConfigRecommendationBreakdown']]\\n\",\n",
    "    \"df.columns = [camel_to_title(col) for col in df.columns]\\n\",\n",
    "    \"display(df)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"6756159b-30ca-407a-ab6b-9c29ced01ea6\",\n",
    "     \"showTitle\": false,\n",
    "     \"title\": \"\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Stages Output\\n\",\n",
    "    \"\\n\",\n",
    "    \"For each stage used in SQL operations, the Qualification tool generates the following information:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. App ID\\n\",\n",
    "    \"2. Stage ID\\n\",\n",
    "    \"3. Average Speedup Factor: The average estimated speed-up of all the operators in the given stage.\\n\",\n",
    "    \"4. Stage Task Duration: The amount of time spent in tasks of SQL DataFrame operations for the given stage.\\n\",\n",
    "    \"5. Unsupported Task Duration: The sum of task durations for the unsupported operators. For more details, see [Supported Operators](https://nvidia.github.io/spark-rapids/docs/supported_ops.html).\\n\",\n",
    "    \"6. Stage Estimated: Indicates if the stage duration had to be estimated (True or False).\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"cdde6177-db5f-434a-995b-776678a64a3a\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"rapids_4_spark_qualification_output_stages.csv\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"stages_output=pd.read_csv(os.path.join(jar_output_folder, \\\"rapids_4_spark_qualification_output_stages.csv\\\"))\\n\",\n",
    "    \"display(stages_output)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"4d7ce219-ae75-4a0c-a78c-4e7f25b8cd6f\",\n",
    "     \"showTitle\": false,\n",
    "     \"title\": \"\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Execs Output\\n\",\n",
    "    \"\\n\",\n",
    "    \"The Qualification tool generates a report of the “Exec” in the “SparkPlan” or “Executor Nodes” along with the estimated acceleration on the GPU. Please refer to the [Supported Operators guide](https://nvidia.github.io/spark-rapids/docs/supported_ops.html) for more details on limitations on UDFs and unsupported operators.\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. App ID\\n\",\n",
    "    \"2. SQL ID\\n\",\n",
    "    \"3. Exec Name: Example: Filter, HashAggregate\\n\",\n",
    "    \"4. Expression Name\\n\",\n",
    "    \"5. Task Speedup Factor: The average acceleration of the operators based on the original CPU duration of the operator divided by the GPU duration. The tool uses historical queries and benchmarks to estimate a speed-up at an individual operator level to calculate how much a specific operator would accelerate on GPU.\\n\",\n",
    "    \"6. Exec Duration: Wall-clock time measured from when the operator starts until it is completed.\\n\",\n",
    "    \"7. SQL Node ID\\n\",\n",
    "    \"8. Exec Is Supported: Indicates whether the Exec is supported by RAPIDS. Refer to the Supported Operators section for details.\\n\",\n",
    "    \"9. Exec Stages: An array of stage IDs.\\n\",\n",
    "    \"10. Exec Children\\n\",\n",
    "    \"11. Exec Children Node IDs\\n\",\n",
    "    \"12. Exec Should Remove: Indicates whether the Op is removed from the migrated plan.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 0,\n",
    "   \"metadata\": {\n",
    "    \"application/vnd.databricks.v1+cell\": {\n",
    "     \"cellMetadata\": {\n",
    "      \"byteLimit\": 2048000,\n",
    "      \"rowLimit\": 10000\n",
    "     },\n",
    "     \"inputWidgets\": {},\n",
    "     \"nuid\": \"998b0c51-0cb6-408e-a01a-d1f5b1a61e1f\",\n",
    "     \"showTitle\": true,\n",
    "     \"title\": \"rapids_4_spark_qualification_output_execs.csv\"\n",
    "    },\n",
    "    \"jupyter\": {\n",
    "     \"source_hidden\": true\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"execs_output=pd.read_csv(os.path.join(jar_output_folder, \\\"rapids_4_spark_qualification_output_execs.csv\\\"))\\n\",\n",
    "    \"display(execs_output)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"application/vnd.databricks.v1+notebook\": {\n",
    "   \"dashboards\": [\n",
    "    {\n",
    "     \"elements\": [],\n",
    "     \"globalVars\": {},\n",
    "     \"guid\": \"\",\n",
    "     \"layoutOption\": {\n",
    "      \"grid\": true,\n",
    "      \"stack\": true\n",
    "     },\n",
    "     \"nuid\": \"91c1bfb2-695a-4e5c-8a25-848a433108dc\",\n",
    "     \"origId\": 2173122769183715,\n",
    "     \"title\": \"Executive View\",\n",
    "     \"version\": \"DashboardViewV1\",\n",
    "     \"width\": 1600\n",
    "    },\n",
    "    {\n",
    "     \"elements\": [],\n",
    "     \"globalVars\": {},\n",
    "     \"guid\": \"\",\n",
    "     \"layoutOption\": {\n",
    "      \"grid\": true,\n",
    "      \"stack\": true\n",
    "     },\n",
    "     \"nuid\": \"62243296-4562-4f06-90ac-d7a609f19c16\",\n",
    "     \"origId\": 2173122769183716,\n",
    "     \"title\": \"App View\",\n",
    "     \"version\": \"DashboardViewV1\",\n",
    "     \"width\": 1920\n",
    "    },\n",
    "    {\n",
    "     \"elements\": [],\n",
    "     \"globalVars\": {},\n",
    "     \"guid\": \"\",\n",
    "     \"layoutOption\": {\n",
    "      \"grid\": true,\n",
    "      \"stack\": true\n",
    "     },\n",
    "     \"nuid\": \"854f9c75-5977-42aa-b3dd-c680b8331f19\",\n",
    "     \"origId\": 2173122769183722,\n",
    "     \"title\": \"Untitled\",\n",
    "     \"version\": \"DashboardViewV1\",\n",
    "     \"width\": 1024\n",
    "    }\n",
    "   ],\n",
    "   \"environmentMetadata\": null,\n",
    "   \"language\": \"python\",\n",
    "   \"notebookMetadata\": {\n",
    "    \"mostRecentlyExecutedCommandWithImplicitDF\": {\n",
    "     \"commandId\": 2173122769183704,\n",
    "     \"dataframes\": [\n",
    "      \"_sqldf\"\n",
    "     ]\n",
    "    },\n",
    "    \"pythonIndentUnit\": 2,\n",
    "    \"widgetLayout\": [\n",
    "     {\n",
    "      \"breakBefore\": false,\n",
    "      \"name\": \"Eventlog Path\",\n",
    "      \"width\": 778\n",
    "     },\n",
    "     {\n",
    "      \"breakBefore\": false,\n",
    "      \"name\": \"Output Path\",\n",
    "      \"width\": 302\n",
    "     }\n",
    "    ]\n",
    "   },\n",
    "   \"notebookName\": \"[RAPIDS Accelerator for Apache Spark] Qualification Tool Notebook Template\",\n",
    "   \"widgets\": {\n",
    "    \"Eventlog Path\": {\n",
    "     \"currentValue\": \"/dbfs/user1/qualification_logs\",\n",
    "     \"nuid\": \"1272501d-5ad9-42be-ab62-35768b2fc384\",\n",
    "     \"typedWidgetInfo\": null,\n",
    "     \"widgetInfo\": {\n",
    "      \"defaultValue\": \"/dbfs/user1/qualification_logs\",\n",
    "      \"label\": \"\",\n",
    "      \"name\": \"Eventlog Path\",\n",
    "      \"options\": {\n",
    "       \"autoCreated\": false,\n",
    "       \"validationRegex\": null,\n",
    "       \"widgetType\": \"text\"\n",
    "      },\n",
    "      \"widgetType\": \"text\"\n",
    "     }\n",
    "    },\n",
    "    \"Output Path\": {\n",
    "     \"currentValue\": \"/tmp\",\n",
    "     \"nuid\": \"ab7e082c-1ef9-4912-8fd7-51bf985eb9c1\",\n",
    "     \"typedWidgetInfo\": null,\n",
    "     \"widgetInfo\": {\n",
    "      \"defaultValue\": \"/tmp\",\n",
    "      \"label\": null,\n",
    "      \"name\": \"Output Path\",\n",
    "      \"options\": {\n",
    "       \"autoCreated\": null,\n",
    "       \"validationRegex\": null,\n",
    "       \"widgetType\": \"text\"\n",
    "      },\n",
    "      \"widgetType\": \"text\"\n",
    "     }\n",
    "    }\n",
    "   }\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}\n"
   ],
   "id": "4ba18da2c217d2f1"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
